你好，我是黄申。

今天，我们来聊另一种降维的方法，**SVD奇异值分解**（Singular Value Decomposition）。它的核心思路和PCA不同。PCA是通过分析不同维度特征之间的协方差，找到包含最多信息量的特征向量，从而实现降维。而SVD这种方法试图通过样本矩阵本身的分解，找到一些“潜在的因素”，然后通过把原始的特征维度映射到较少的潜在因素之上，达到降维的目的。

这个方法的思想和步骤有些复杂，它的核心是矩阵分解，首先，让我们从方阵的矩阵分解开始。

## 方阵的特征分解

在解释方阵的分解时，我们会用到两个你可能不太熟悉的概念：方阵和酉矩阵。为了让你更顺畅的理解整个分解的过程，我先给你解释下这两个概念。

**方阵**（Square Matrix）是一种特殊的矩阵，它的行数和列数相等。如果一个矩阵的行数和列数都是n，那么我们把它称作n阶方阵。

如果一个矩阵和其转置矩阵相乘得到的是单位矩阵，那么它就是一个**酉矩阵**（Unitary Matrix）。

$X’X=I$

其中X’表示X的转置，I表示单位矩阵。换句话说，矩阵X为酉矩阵的充分必要条件是X的转置矩阵和X的逆矩阵相等。

$X’=X^{-1}$
<div><strong>精选留言（17）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/16/f5/e0/76822dd9.jpg" width="30px"><span>南边</span> 👍（2） 💬（3）<div>把 V 的这 n 个特征向量进行标准化处理，那么对于每个特征向量 Vi​，就有 ∣∣Vi​∣∣2​=1，而这表示 V’i​Vi​=1，此时 V 的 n 个特征向量为标准正交基，满足 V’V=I ， 也就是说 V 为酉矩阵

对于V是酉矩阵这个推导还是不太理解</div>2020-01-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/18/d0/49b06424.jpg" width="30px"><span>qinggeouye</span> 👍（2） 💬（2）<div>import numpy as np
from numpy import linalg as la

# 文档集合 文档和词条关系矩阵 行表示文档 列表示词条
x = np.mat([[1, 1, 1, 0, 0], [2, 2, 2, 0, 0],
            [1, 1, 1, 0, 0], [5, 5, 5, 0, 5],
            [0, 0, 0, 2, 2], [0, 0, 0, 3, 3],
            [0, 0, 0, 1, 1]])

U, sigma, VT = la.svd(x)
print(U, &quot;\n&quot;)
print(sigma, &quot;\n&quot;)
print(VT, &quot;\n&quot;)

S = np.zeros((7, 5))  # 奇异矩阵
for i in range(len(sigma)):
    S[i, i] = sigma[i]

print(&quot; 与矩阵 x 一致？ \n&quot;, U.dot(S).dot(VT.transpose()))

这里计算出的左奇异矩阵、奇异值矩阵、右奇异值矩阵，以及它们的点乘，与本文中的都不太一样，不知哪里出问题了？</div>2019-04-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/f5/73/f7d3a996.jpg" width="30px"><span>！null</span> 👍（1） 💬（1）<div>U 是一个 m×m 维的矩阵，V 是一个 n×n 维的矩阵。而 Σ 是一个 m×n 维的矩阵

上边m×m和n×n意思是m阶和n阶方阵吗？如果是，为什么lsa的例子里，就是计算机文档和医学文档的例子里，U和V都不是方阵，而 Σ 是个方阵？</div>2021-08-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/f5/73/f7d3a996.jpg" width="30px"><span>！null</span> 👍（1） 💬（1）<div>σ[i]=(X*v[i])&#47;u[i]
这个公式后边怎么算没看懂。X应该是最开始的矩阵，v[i]和u[i]是特征向量吗？那X*u[i]结果应该也是向量。最后就是两个向量之间做除法。向量间除法应该怎么算？</div>2021-08-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/f5/73/f7d3a996.jpg" width="30px"><span>！null</span> 👍（1） 💬（1）<div>其中 U 是一个 m×m 维的矩阵，V 是一个 n×n 维的矩阵。而 Σ 是一个 m×n 维的矩阵，对于 Σ 来说，只有主对角线之上的元素可以为非 0，其他元素都是 0，而主对角线上的每个元素就称为奇异值。
后边“计算机文档和医学文档”奇异值分解之后，没觉得U是 m×m 维的矩阵，V 是一个 n×n 维的矩阵，Σ 是一个 m×n 维的矩阵。如果Σ有对角线的话，是不是Σ 应该是个方阵？</div>2021-08-23</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（1） 💬（2）<div>文档例子和svd分解有差别，svd是左右为方阵，中间为非方阵。文档例子是中间为方阵，左右为方阵，我感觉这里缺了一步，是不是文档的例子已经是降维后的结果了？</div>2019-10-11</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/kT185qC7s1afo3w6mFUXPpagtZ0JRguoBF2GSLcoN0ib6L8pB7ZUicuC87JU6LEqtCRNsJfaGzQw5hTA6fEuHNqg/132" width="30px"><span>marcus1877</span> 👍（0） 💬（1）<div>“第三步，对 SVD 分解后的矩阵进行降维，这个操作和 PCA 主成分分析的降维操作是类似的。”
是对SVD分解后的U,V&#39;,∑ 降维吗？</div>2021-03-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg" width="30px"><span>建强</span> 👍（0） 💬（1）<div>思考题：请教一下老师，SVD中的奇异值矩阵，能否用U&#39;XV来计算，因为UU&#39;=I,
所以对于等式X = UΣV&#39;，分别用U&#39;和V对等式两边进行左乘和右乘，就得到Σ = U&#39;XV，不知这样推导是否正确？
用Python简单写了一个SVD分解的程序，源代码如下：

def SVD_Solve(X):
    # 计算U矩阵 = XX&#39;的特征矩阵
    U = X.dot(X.T)
    U_feature, U_vector = LA.eig(U)

    # 计算V矩阵 = X&#39;X的特征矩阵
    V = X.T.dot(X)
    V_feature, V_vector = LA.eig(V)

    # 计算西格码对角矩阵 = U&#39;XV
    XGM = (U_vector.T.dot(X)).dot(V_vector)
    
    return U_vector, XGM, V_vector.T

#测试
X = mat([[1,1,1,0,0]
        ,[2,2,2,0,0]
        ,[1,1,1,0,0]
        ,[5,5,5,0,0]
        ,[0,0,0,2,2]
        ,[0,0,0,3,3]
        ,[0,0,0,1,1]
        ]
       )

U,XGM,V = SVD_Solve(X)
print(&quot;左奇异向量\n&quot;,&quot;=&quot;*10, &quot;\n&quot;, U)
print(&quot;奇异值矩阵\n&quot;,&quot;=&quot;*10, &quot;\n&quot;, XGM)
print(&quot;右奇异向量\n&quot;,&quot;=&quot;*10, &quot;\n&quot;, V)
</div>2020-11-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/36/7d/96f0457e.jpg" width="30px"><span>不接地气的马三岁</span> 👍（0） 💬（3）<div>算方差的时候，不应该还有个1&#47;N吗，N是元素个数，(x1-0)^2+...+(xn-0)^2为什么不等于N。
</div>2020-07-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/7e/a6/4e331ef4.jpg" width="30px"><span>骑行的掌柜J</span> 👍（0） 💬（1）<div>又get到一个新的知识点 奇异值SVD😁虽然有疑惑这里“就有 ∣∣Vi​∣∣2​=1，而这表示 V’i​Vi​=1，此时 V 的 n 个特征向量为标准正交基” 不过已经有好几个朋友提问出来 老师也解答了 剩下就是熟练这个过程 谢谢黄老师
PS: 如果有SVD奇异值分解项目案例就更好了 </div>2020-07-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/42/df/a034455d.jpg" width="30px"><span>罗耀龙@坐忘</span> 👍（0） 💬（1）<div>茶艺师学编程

到这里，总算看到了在大学时算的死去活来的求逆矩阵、矩阵相乘、矩阵变换的一个应用……

</div>2020-05-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/ad/50/3cb818e8.jpg" width="30px"><span>灰太狼</span> 👍（0） 💬（1）<div>如果我们会把 V 的这 n 个特征向量进行标准化处理，那么对于每个特征向量 V_i，就有 ||V_i||_2=1，而这表示 V’_iV_i=1，此时 V 的 n 个特征向量为标准正交基，满足 V’V=I ， 也就是说 V 为酉矩阵，有 V’=V^{-1} 。这样一来，我们就可以把特征分解表达式写作 X=VΣV’。
--------------------------
黄老师，接着上一个问题，我还是有点疑问，这个地方特征向量进行了标准化，这个标准化是在原始矩阵上对每个列向量进行的标准化吗？还是在哪儿做的标准化？如果原始矩阵没做标准化，求出特征向量后又进行标准化那应该不再是原始矩阵的特征向量了吧？</div>2020-04-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/ad/50/3cb818e8.jpg" width="30px"><span>灰太狼</span> 👍（0） 💬（1）<div>如果我们会把 V 的这 n 个特征向量进行标准化处理，那么对于每个特征向量 V_i，就有 ||V_i||_2=1，而这表示 V’_iV_i=1，此时 V 的 n 个特征向量为标准正交基，满足 V’V=I ，
--------------
黄老师，这个地方有点没太看懂，请问这儿的标准化是在什么时候做的？如果不做标准化等式是不是就不成立了？</div>2020-04-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/14/30/55a006ae.jpg" width="30px"><span>wick</span> 👍（0） 💬（1）<div>老师好，文中奇异值 σ的求解，由XV=UΣ推出Xvi​=σi​ui​再到σi​=ui​Xvi​​没懂</div>2020-03-13</li><br/><li><img src="" width="30px"><span>013923</span> 👍（1） 💬（0）<div>学习了新知识，谢谢！</div>2022-09-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/98/2c/cff47039.jpg" width="30px"><span>米饭</span> 👍（0） 💬（0）<div>“把 V 的这 n 个特征向量进行标准化处理，那么对于每个特征向量 Vi​，就有 ∣∣Vi​∣∣2​=1，而这表示 V’i​Vi​=1，此时 V 的 n 个特征向量为标准正交基，满足 V’V=I ， 也就是说 V 为酉矩阵”

这里其实是先标准化，再归一化
V为特征矩阵，V_i为特征向量，标准化后V_i向量成正态分布，均值为0，标准差为1，方差也为1，此时|| V_i ||_2 = n（即标准化后的向量模为n）。
再将标准化后的向量归一化为标准化向量(即单位向量，模为1)，求一个向量的标准化向量，本质是让这个向量与自身的模相除，所以再除以n
最后得到标准化向量 || V_i ||_2 = 1
所以 (|| v_i ||_2)的平方 = 1，即 V&#39;_iV&#39; = 1
所以V的每一列都是单位向量，才满足标准正交基的定义
参考: https:&#47;&#47;www.cnblogs.com&#47;shine-lee&#47;p&#47;11779514.html</div>2020-08-31</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（0） 💬（0）<div>方阵分解成正交阵 x 对角阵 x 正交阵转置
非方阵也可以做类似分解
这里的对角阵参数的大小反映了重组后分量的信息量</div>2019-10-11</li><br/>
</ul>