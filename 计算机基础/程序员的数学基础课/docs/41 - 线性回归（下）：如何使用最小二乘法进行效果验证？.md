你好，我是黄申。

上一节我们已经解释了最小二乘法的核心思想和具体推导过程。今天我们就用实际的数据操练一下，这样你的印象就会更加深刻。我会使用几个具体的例子，演示一下如何使用最小二乘法的结论，通过观测到的自变量和因变量值，来推算系数，并使用这个系数来进行新的预测。

## 基于最小二乘法的求解

假想我们手头上有一个数据集，里面有3条数据记录。每条数据记录有2维特征，也就是2个自变量，和1个因变量。

![](https://static001.geekbang.org/resource/image/94/b4/9427dc10b0745cb5680e911e0d0d15b4.png?wh=1270%2A300)

如果我们假设这些自变量和因变量都是线性的关系，那么我们就可以使用如下这种线性方程，来表示数据集中的样本：

$b\_1·0+b\_2·1=1.5$  
$b\_1·1-b\_2·1=-0.5$  
$b\_1·2+b\_2·8=14$

也就是说，我们通过观察数据已知了自变量$x\_1$、$x\_2$和因变量$y$的值，而要求解的是$b\_1$和$b\_2$这两个系数。如果我们能求出$b\_1$和$b\_2$，那么在处理新数据的时候，就能根据新的自变量$x\_1$和$x\_2$的取值，来预测$y$的值。
<div><strong>精选留言（12）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/14/6a/8e/7b6ea886.jpg" width="30px"><span>Joe</span> 👍（4） 💬（1）<div>回答与疑问：
1. 非线性关系的数据拟合，可以先将自变量转为非线性。如转化为多项式（sklearn的PolynomialFeatures）。再用线性回归的方法去拟合。
2. 请问老师对于求解逆矩阵有没有什么高效的方法？
附上以前写的polyfit方法，请老师指点。谢谢
    def oneDPolynomiaTransform(self, x_origin):
        &#39;&#39;&#39;
        @description: generate polynomial for 1D input data. rule: [x0, x0^2, x0^3,...,x0^degreee]
        @param {type} x_origin- data before transformed[nX1]
        @return: x_transformed- data after transformed[nXdegree]
        &#39;&#39;&#39;
        len_features = len(x_origin)
        # polynomial feature data after transformed.
        x_transformed = np.array([])
        for i in range(len_features):
            for j in range(self.degree+1):
                x_transformed = np.append(
                    x_transformed, [(x_origin[i])**(j)])
        x_transformed = x_transformed.reshape(-1, self.degree+1)
        return x_transformed</div>2019-03-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/7e/a6/4e331ef4.jpg" width="30px"><span>骑行的掌柜J</span> 👍（3） 💬（1）<div>感谢黄老师 之前一直只知道建模，拟合，预测，但是背后的数学原理一直没有很理解！！！现在有种恍然大悟的感觉！原来是这样，当因变量和自变量呈现线性关系，建模这一系列动作都是基于上面线代的原理！😁终于get到了它在求什么了！！
PS：刚刚评论有个朋友也提问了如何判断一个数据集可以用线性模型？可以用R2，也就是regression.score这个方法，
所以请问黄老师：是只有先去用线性模型拟合数据集，再来判断模型是否合适吗？
还是我可以从拿到数据集一开始，直接查看每个自变量和因变量的关系是否呈线性再来决定线性模型是否合适？谢谢</div>2020-07-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/c1/1f/cc77944d.jpg" width="30px"><span>叮当猫</span> 👍（2） 💬（1）<div>文中有提到，如何判断一个数据集是否可以用线性模型来表示，可以使用决定系数R2，随着自变量个数不断增加，R2将不断增大，这时需要用Rc2，而其中R2就是regression.score，那请问Rc2是库里面的什么呢？</div>2019-04-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/de/ba/f01dc860.jpg" width="30px"><span>Cest la vie</span> 👍（2） 💬（1）<div>老师好! 后面可以来一节PLS偏最小二乘的原理讲解和应用么</div>2019-03-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/42/df/a034455d.jpg" width="30px"><span>罗耀龙@坐忘</span> 👍（4） 💬（0）<div>茶艺师学编程

因为还不会编程，我只能用手算了……

B=[1.463916
      -0.403059
      -0.616992]

然后再算了误差，23.907，嗯，我有98％的信心相信我这次算错了。

而且我还不知道在哪里找错。

我想到如果是我在编写程序时出现这状况，很有可能会比这更惨……</div>2020-05-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/4e/c7/8c2d0a3d.jpg" width="30px"><span>余泽锋</span> 👍（4） 💬（0）<div>import numpy as np
X = np.mat([[1, 3, -7], [2, 5, 4], [-3, -7, -2], [1, 4, -12]])
Y = np.mat([[-7.5], [5.2], [-7.5], [-15]])
B1 = X.transpose().dot(X).I
B2 = B1.dot(X.transpose())
B = B2.dot(Y)
&#39;&#39;&#39;
matrix([[12.01208791],
        [-4.35934066],
        [ 0.82527473]])
&#39;&#39;&#39;</div>2019-04-24</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（1） 💬（0）<div>高斯消元求得是精确解。
线性回归求得是最好近似解，覆盖高斯消元能处理的情况，也能在没有精确解的时候找到近似解，还提供测量近似参数，是处理线性关系的利器。</div>2019-10-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/18/d0/49b06424.jpg" width="30px"><span>qinggeouye</span> 👍（1） 💬（0）<div>&quot;&quot;&quot;
思考题同理
&quot;&quot;&quot;
x = np.mat([[1, 3, -7], [2, 5, 4], [-3, -7, -2], [1, 4, -12]])
y = np.mat([[-7.5], [5.2], [-7.5], [-15]])
print(&quot;\n 系数矩阵 B: \n&quot;, (x.transpose().dot(x)).I.dot(x.transpose()).dot(y))

 系数矩阵 B: 
 [[12.01208791]
 [-4.35934066]
 [ 0.82527473]]</div>2019-03-30</li><br/><li><img src="" width="30px"><span>013923</span> 👍（0） 💬（0）<div>学习了！</div>2022-09-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg" width="30px"><span>建强</span> 👍（0） 💬（0）<div>根据最小二乘法系数计算模型，用了一个简单的Python程序计算系数矩阵，程序代码：
import numpy as np
# 矩阵X
X = np.array([[1, 3, -7]
            ,[2, 5, 4]
            ,[-3, -7, -2]
            ,[1, 4, -12]
             ]
            )
# X的转置X&#39;
TRANS_X = X.T

# 目标值向量Y
Y = np.array([[-7.5, 5.2, -7.5, -15]]).T

# X&#39;X
B = TRANS_X.dot(X)

# (X&#39;X)^-1
B = np.matrix(B).I

# [(X&#39;X)^-1]X&#39;
B = B.dot(TRANS_X)

# [(XX&#39;)^-1]X&#39;Y
B = B.dot(Y)

print(&#39;系数矩阵：&#39;)
print(B)

print(&#39;验证：&#39;)
print(X.dot(B))

================程序输出结果==================
系数矩阵：
[[12.01208791]
 [-4.35934066]
 [ 0.82527473]]
验证：
[[ -6.84285714]
 [  5.52857143]
 [ -7.17142857]
 [-15.32857143]]</div>2020-10-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/6d/5b/03763d38.jpg" width="30px"><span>观望者</span> 👍（0） 💬（0）<div>#用sklearn的库
df = pd.read_csv(&#39;data.csv&#39;)
regr = linear_model.LinearRegression()#产生线性拟合的对象
X = df.loc[:,[&#39;x1&#39;,&#39;x2&#39;,&#39;x3&#39;]]
Y = df[&#39;y&#39;]
regr.fit(X, Y)#传入拟合的数据集
Y_pred = regr.predict(X)##预测值
print(&#39;Coefficients: \n&#39;, regr.coef_)

#如果用公式来计算需要在数据中加入 截距（也就是x=1的列），计算如下：
inter = pd.Series([1 for i in range(4)], name=&#39;X0&#39;)
X_inter = pd.concat([inter,X], axis=1)
B = min2mul(X_inter,Y)
print(&#39;Coefficients B: \n&#39;, B)
def min2mul(X,Y):
    X_REV = np.transpose(X)##矩阵转置
    X2 = np.linalg.inv(np.dot(X_REV,X))## 计算(X&#39;X)^(-1)
    B = np.dot(np.dot(X2,X_REV),Y)
    return B</div>2020-02-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/b5/98/ffaf2aca.jpg" width="30px"><span>Ronnyz</span> 👍（0） 💬（0）<div>from numpy import *
x=mat([[1,3,-7],[2,5,4],[-3,-7,-2],[1,4,-12]])
y=mat([[-7.5],[5.2],[-7.5],[-15]])
print(&quot;\n系数矩阵B:\n&quot;,(x.transpose().dot(x)).I.dot(x.transpose()).dot(y))

系数矩阵B:
 [[12.01208791]
 [-4.35934066]
 [ 0.82527473]]</div>2019-10-22</li><br/>
</ul>