你好，我是黄申。

之前和你聊了概率在朴素贝叶斯分类算法中的应用。其实，概率在很多像信息论这样的应用数学领域都有广泛的应用。信息论最初就是运用概率和统计的方法，来研究信息传递的。最近几十年，人们逐步开始使用信息论的概念和思想，来描述机器学习领域中的概率分布，并衡量概率分布之间的相似性。随之而来的是，人们发明了不少相关的机器学习算法。所以接下来的几节，我来介绍一些基于信息论知识的内容。

信息论的概念比较枯燥，为了让你更轻松地学习，让我从一个生动的案例开始。最近我在朋友圈看到一个小游戏，叫“测一测你是金庸笔下的哪个人物？”。玩这个游戏的步骤是，先做几道题，然后根据你的答案，生成对应的结果。下面是我几位朋友答题之后得到的结果。

![](https://static001.geekbang.org/resource/image/55/f1/55ea8faffde5fd7450518903b9d3f3f1.jpeg?wh=1920%2A2349)

这种测试挺好玩的，而且好像有很多类似的，比如测星座啊、测运势啊等等。那你知道这种心理或者性格测试的题目是怎么设计的吗？

通常，这种心理测试会有一个题库，包含了许多小题目，也就是从不同的方面，来测试人的性格。不过，针对特定的测试目标，我们可能没必要让被测者回答所有的问题。那么，问卷设计者应该如何选择合适的题目，才能在读者回答尽量少的问题的同时，相对准确地测出自己是什么“性格”呢？这里，我们就需要引入基于概率分布的信息熵的概念，来解决这个问题。
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/10/9c/1d/f0f10198.jpg" width="30px"><span>蒋宏伟</span> 👍（28） 💬（5）<div>信息熵是衡量信息简单、纯净或复杂、混乱的标尺。人类必须将事务抽象为信息才能进行理解。事物的信息熵越小越容易理解，越大越难理解。
写好代码的本质，就是降低程序信息熵
。作用域、模块、组件、微服务、文档、注释是在不同的纬度，降低信息熵的工具。</div>2019-02-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/18/d0/49b06424.jpg" width="30px"><span>qinggeouye</span> 👍（23） 💬（3）<div>1、事件发生的概率 P(x) 越小，包含的信息量 H(x) 越大；
2、两个不相关的事件 x 、y，同时发生的信息量 H(x,y) 等于这两个事件分别发生时的信息量 H(x) 、H(y) 之和；
3、信息熵 Entropy(x) 是信息量 H(x) 的加权平均，即信息量的期望；
4、信息增益等于集合元素划分前的信息熵减去划分后的信息熵；划分后的信息熵等于各个分组的信息熵的加权平均；

思考题：64*(-1)*(1&#47;64)*log(1&#47;64) = 6 , (对数底数取 2)。</div>2019-03-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/73/af/4bb834c1.jpg" width="30px"><span>刘杰</span> 👍（13） 💬（1）<div>这个是我读过最好的信息论概念的解释！</div>2019-02-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/40/52/7266ee09.jpg" width="30px"><span>哈</span> 👍（12） 💬（2）<div>事情发生的概率越大，产生的信息量越小；事情发生的概率越小，产生的信息量越大。
这个应该怎么理解呢</div>2019-08-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/69/4d/81c44f45.jpg" width="30px"><span>拉欧</span> 👍（7） 💬（1）<div>2的6次方是64，所以是6</div>2019-02-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1b/7a/82/2890b534.jpg" width="30px"><span>jay</span> 👍（4） 💬（1）<div>黄老师，您好！您讲的真的好，虽然之前接触过这些概念，但理解的不透彻，从之前的贝叶斯到今天的信息熵，我现在完全搞明白了，相见恨晚啊。（希望您能开个ML和DL的专栏，将来想从事这方面的研究，谢谢）</div>2020-01-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/36/3f/95a9a40a.jpg" width="30px"><span>张九州</span> 👍（4） 💬（1）<div>总信息量减少 为什么叫做增益呢？不太理解</div>2019-09-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg" width="30px"><span>建强</span> 👍（3） 💬（3）<div>思考题：
包含64个元素的集合信息熵 Entroy(P) = 64 * (-1 * 1&#47;64 * log(2, 1&#47;64)) = 2^6 * (-1) * (2^(-6)) * (-6) = 6 = log(2,64)
我个人理解：信息熵其实就是用二进制来表达某个数时所需要的二进制位数</div>2020-06-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/f5/ea/faf489e4.jpg" width="30px"><span>Dale</span> 👍（2） 💬（1）<div>6位二进制数刚好可以表示0~63共64个不同的状态，也就是说在概率相同的情况下，熵就等于存储所有状态的比特数(没有取整)</div>2021-02-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/91/00/2007d2f3.jpg" width="30px"><span>zhengfan</span> 👍（2） 💬（1）<div>黄老师：
请问对一个几个不断地做划分，信息熵是否是个单调递减过程？
我试着推导了一下可以得出，对于一个完全无分类集合，所有有效划分（不会导致空子集产生的划分）都必然带来大于0的信息增益，也就是信息熵必然减小。
对于已经存在分类的集合，我直觉上认为是成立的，思考了几个例子也支持。请问能严格证明吗？</div>2020-05-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1b/65/20/cf47a9e1.jpg" width="30px"><span>Geek_80dbb5</span> 👍（2） 💬（1）<div>其实，古人的“钻木取火”，就是一种能量转换，即机械能向热能转换；并且在这个转换过程中，“熵”便产生了。</div>2020-04-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/8c/30/d642e01a.jpg" width="30px"><span>zhengnachuan</span> 👍（2） 💬（1）<div>如果只是为了增加增益，其实可以细分到最小，但是实际上应该是要考虑其他维度的吧，例如分组的次数，即在固定次数下的最大增益。
另外，有点疑惑，假设为了获得最大增益，n个元素分为n组，是不是表示就需要有n个条件能一次进行区分。以开始的人物区分为例，这个条件应该怎么给呢，是不是要重新设计独有的特征。</div>2019-03-23</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eo9Xajp9qOGPQMwzvGPKXzb1TptIZsAaJavfU6a3n1qDANplTmVAjkickhddL1lrhqNVX1BneOabNQ/132" width="30px"><span>201201904</span> 👍（1） 💬（1）<div>事情发生的概率越大，产生的信息量越小；事情发生的概率越小，产生的信息量越大。
怎么理解？</div>2021-07-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/03/5c/8733ec5a.jpg" width="30px"><span>Geek_b636f6</span> 👍（1） 💬（1）<div>看来老师对学生的学习情况进行区分，通过考试问卷给学生打分，是一个信息增益的过程。老师的劳动创造了信息。</div>2021-02-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/03/5c/8733ec5a.jpg" width="30px"><span>Geek_b636f6</span> 👍（1） 💬（1）<div>熵增一般被认为是混乱度增加，信息熵增是指系统的不确定性增加，都是人不喜欢的。人的本性是追求秩序的，追求确定性的，所以生命和文明都是熵减过程。
老师，请问个体层面有意识的熵减活动过程，是否从宏观层面看还是是熵增，比如“大众创业，万众创新”？</div>2021-02-21</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoxfvYvvMkjsjwA6rnpIGY2DSZ6Ww5yxP6brbzkicNdXc6ftPoAC8J94SoicneVDmH1Wdta194oFribQ/132" width="30px"><span>吴关俊</span> 👍（1） 💬（1）<div>就是计算总体熵的时候，为啥用单个武侠人物的平均概率计算呢？ 根据公式 不应该是武侠人物每个特征就是一个集合计算熵然后相加得到总体熵嘛，而你直接用1&#47; 10 = 0.1是何道理？</div>2020-10-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/f5/73/f7d3a996.jpg" width="30px"><span>！null</span> 👍（1） 💬（2）<div>真实项目中遇到了一个问题。还是类比大侠吧，有个特征是擅长的兵器，杨过的用剑，谢逊用刀，郭靖用手，张无忌会太极拳和太极剑，石破天啥都用，就是有的人有的特征他不关心。因为实际项目中的问题，有的关心A特征，不关心B特征，有的关心B特征，A特征如何取值无所谓，这个熵和增益应该如何计算？P(用刀|石破天）=P（用剑|石破天）=P(用手|石破天)=P(用刀）=P（用剑）=P（用手），那相当于集合中石破天的概率是其他人的3倍？算熵的时候的概率就相当与3倍的其他人？分组后就相当于3个石破天分到三个组中？</div>2020-08-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/f5/73/f7d3a996.jpg" width="30px"><span>！null</span> 👍（1） 💬（1）<div>当我们观察到某个随机变量的具体值时，接收到了多少信息。接受到的信息指的什么，能不能举个例子？</div>2020-07-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1e/da/94/8acab546.jpg" width="30px"><span>良知犹存</span> 👍（1） 💬（1）<div>信息熵用来表示每个分组在整体中的混乱情况。熵增意味着更加混乱，熵减意味着分组的独立
思考题：
由于各自独立，所以最终分为64组，所以每组的出现的概率为1&#47;64
整体的熵计算为：64*（-1*1&#47;64 *log （1&#47;64，2 ））= 6</div>2020-06-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/7e/a6/4e331ef4.jpg" width="30px"><span>骑行的掌柜J</span> 👍（1） 💬（1）<div>之前在学过一点信息熵的知识 但是理解不是很透彻 这里重学了一遍 瞬间把之前的迷糊点弄懂了 谢谢黄老师

PS 看评论也可以学到很多😁</div>2020-06-08</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJman25D8Jlr6P6AIhumWr2CNqZPvXl8JJLc3yOvvTlWFDVuKbYpNXgKib6y1Sa0HApwvz1xM6MBjw/132" width="30px"><span>大秦岭</span> 👍（1） 💬（1）<div>经过各学者多年的探究和各种语言的统计，得出一个结果，汉语是世界上信息熵最大的语言。那么这个信息熵是 什么？信息熵指的就是可能发生的所有事情中包含的信息期望值，比如鸟不能生活在水中，违背自然常理，那么信息熵为0.
</div>2019-06-19</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoxfvYvvMkjsjwA6rnpIGY2DSZ6Ww5yxP6brbzkicNdXc6ftPoAC8J94SoicneVDmH1Wdta194oFribQ/132" width="30px"><span>吴关俊</span> 👍（0） 💬（1）<div>老师有个位置不懂啊
根据之前单个集合的熵计算，A 和 B 组元素所组成的小集合，它的熵是 1。而 C 组没有和其他组混合，所形成的小集合其熵为 0
C组熵咋会是0呢？ AB组形成一个集合 是大集合中的小集合 有熵能理解，C 也是大集合中的小集合熵怎么会是0呢？ 很费解</div>2020-10-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/84/50/b108704b.jpg" width="30px"><span>我要换个名字</span> 👍（0） 💬（1）<div>通俗易懂，很有用</div>2020-03-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/d4/44/0ec958f4.jpg" width="30px"><span>Eleven</span> 👍（0） 💬（1）<div>按照公式计算为：
-1*64*1&#47;64*log(1&#47;64) = -1*1*(log1 - log64) = -1(0 - 6) = 6</div>2020-01-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/b5/98/ffaf2aca.jpg" width="30px"><span>Ronnyz</span> 👍（0） 💬（1）<div>64*-1*（1&#47;64）*log(1&#47;64,2)=6 
由于是64等分，相当于2^6=64</div>2019-10-11</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（0） 💬（1）<div>老师，我推导了一下独立事件熵的公式和文中不一样，不知道哪一步有问题？多谢！
H(x,y) = -P(x,y)lgP(x,y) = -P(x)P(y)lg(P(x)P(y)) =H(x)P(y) + H(y)P(x)</div>2019-09-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg" width="30px"><span>mickey</span> 👍（0） 💬（2）<div>信息熵的公式是H(x)=−log(P(x),2)
文中熵为 -100%*log(100%, 2) = 0
请问第一个 100% 怎么来的？少了一个 P（x）吧。</div>2019-02-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/42/df/a034455d.jpg" width="30px"><span>罗耀龙@坐忘</span> 👍（7） 💬（2）<div>茶艺师学编程

思考题：计算一个包含了 64 个元素，而每个元素的分类都互不相同的集合的信息熵。仔细观察一下结果，和二进制有没有什么联系？

假设这个集合就有64种分类，那么它的信息熵就是64*[-1&#47;64*log（1&#47;64,2）]=6
假设这个集合就有63种分类，那么它的信息熵就是63*[-1&#47;63*log（1&#47;63,2）]≈5.977
假设这个集合就有62种分类，那么它的信息熵就是62*[-1&#47;62*log（1&#47;62,2）]≈5.954
假设这个集合就有61种分类，那么它的信息熵就是61*[-1&#47;61*log（1&#47;61,2）]≈5.931
假设这个集合就有60种分类，那么它的信息熵就是60*[-1&#47;60*log（1&#47;60,2）]≈5.907
······
假设这个集合就有4种分类，那么它的信息熵就是4*[-1&#47;4*log（1&#47;4,2）]=2
假设这个集合就有3种分类，那么它的信息熵就是3*[-1&#47;3*log（1&#47;3,2）]≈1.585
假设这个集合就有2种分类，那么它的信息熵就是2*[-1&#47;2*log（1&#47;2,2）]=1
假设这个集合就有1种分类，那么它的信息熵就是1*[-1&#47;1*log（1&#47;1,2）]=0

因为这里用到的log2，而在信息论中，描述“有0和1两种状态，出现的可能性都是50%，那么是0还是1？”这就是二进制，这样的信息量就是1比特。换句话说在二进制（信息论）的视角里，一个有64个元素的集合分类的信息熵，最多就是6比特的事情。</div>2020-04-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/a4/5f/6892585a.jpg" width="30px"><span>LiuHDme</span> 👍（2） 💬（0）<div>这一讲非常不错👍</div>2022-02-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/9a/89/babe8b52.jpg" width="30px"><span>A君</span> 👍（1） 💬（0）<div>信息量跟信息发生的概率成反比，信息熵是随机变量所有具体值的信息量期望，信息增益是指集合划分后信息熵减少的数值。在选取特征来划分集合时，要选信息增益最大的那个，它让集合减少的信息熵就是它增加的信息增益。
决策树模型就是根据信息增益理论设计的。</div>2020-07-11</li><br/>
</ul>