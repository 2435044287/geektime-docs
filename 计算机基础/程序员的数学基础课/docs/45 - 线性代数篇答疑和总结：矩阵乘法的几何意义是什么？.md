你好，我是黄申。今天是线性代数的答疑和总结。

在这个模块中，我们讲了不少向量、矩阵、线性方程相关的内容。看到大家在留言区的问题，今天我重点说说矩阵乘法的几何意义，以及为什么SVD中$X’X$的特征向量组成了$V$矩阵，而$XX’$的特征向量组成了$U$矩阵。最后，我会对整个线性代数的模块做一个总结。

## 矩阵乘法的几何意义

首先，我们来说说矩阵乘法所代表的几何意义。

在阐述PCA主成分分析的时候，我们聊过为什么这个方法要研究协方差矩阵的特征值和特征向量。其中，我提到对某个向量左乘一个矩阵，实际上是对这个向量进行了一次变换。某个矩阵的特征向量表示了这个矩阵在空间中的变换方向，这些方向都是正交或者趋于正交的，而特征值表示每个方向上伸缩的比例。今天，我会继续深入这个话题，结合实例，给出更详细地解释。

多维的向量空间很难理解，所以我们还是从最简单的二维空间开始。首先，我们需要明白什么是二维空间中的正交向量。正交向量的定义非常简单，只要两个向量的点乘结果为0，那么它们就是正交的。在酉矩阵之中，矩阵和矩阵的转置相乘为单位矩阵，只有向量自己点乘自己值为1，而不同向量之间点乘值为0，所以不同的向量之间是正交的。
<div><strong>精选留言（13）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/14/3d/77/45e5e06d.jpg" width="30px"><span>胡鹏</span> 👍（10） 💬（2）<div>没法实践，听不懂，却在坚持，这样有益处嘛？</div>2019-03-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/18/d0/49b06424.jpg" width="30px"><span>qinggeouye</span> 👍（4） 💬（1）<div>1、矩阵 X 左乘另一个向量 a，向量 a 分别沿着矩阵 X 的特征向量进行伸缩，伸缩大小为特征向量对应的特征值；

2、不太理解，Σ 为什么会是对角矩阵？矩阵 X 是 m x n 维的，U 是 m x m 维左奇异矩阵，V 是 n x n 维的右奇异矩阵，按道理 Σ 是 m x n 维奇异矩阵(对角线上是奇异值，其它元素为0) ；当 m = n 时，Σ 才是对角矩阵吧？这样的话才是 $Σ&#39;Σ = Σ^2$ 。又或者是对 奇异值矩阵 Σ 中的奇异值做了某种取舍(降维)，U 未必是 m x m 维，V 也未必是 n x n 维？</div>2019-04-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/01/9c/1a750bc7.jpg" width="30px"><span>l c</span> 👍（2） 💬（1）<div>https:&#47;&#47;www.bilibili.com&#47;video&#47;BV1ys411472E
3Blue1Brown的线性代数的本质，本科线代没好好学一塌糊涂，后来研究生的时候发现很多东西都需要依靠线代思考，所以找到了这个复习，豁然开朗。</div>2020-08-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/11/d0/aefbaf52.jpg" width="30px"><span>我是小白007</span> 👍（2） 💬（1）<div>工作中暂时用不到。但是拓展了思路，以前对线代的阴影也少了许多。谢谢黄老师</div>2019-10-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/f8/ed/60fb8fba.jpg" width="30px"><span>陈小渣👻</span> 👍（2） 💬（1）<div>太开心啦👏，整个线代模块下来，学到了很多大学里面没联系起来的知识~黄老师的表述好清晰~</div>2019-08-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/f5/e0/76822dd9.jpg" width="30px"><span>南边</span> 👍（1） 💬（1）<div>这是一门基础课程，第一遍看完，虽然大部分知识看懂了，但是还是很生疏，之前看的，现在又忘记了，主要是因为没有应用起来，还不熟悉，多看几遍，自己多写点demo，自然会熟悉起来</div>2020-01-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/a1/23/2b527dc4.jpg" width="30px"><span>逐风随想</span> 👍（1） 💬（1）<div>@胡鹏：一样，人们都说笨鸟先飞。刚开始的一腔热血。慢慢的到后面发现越来越艰难了。也越来越痛苦。这个已经超过了能力范围。有的时候智力真的是跨越不了的坎。</div>2019-03-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg" width="30px"><span>建强</span> 👍（0） 💬（1）<div>心得：
1.向量和矩阵的运算是一种可以理解为是机器学习中的基本工具和手段，适用于机器学习领域中的许多算法。
2.利用矩阵运算，可以让很多算法模型的表达更加简洁，比如PageRank算法、最小二乘法等。
3.机器学习中，把样本和样本的特征看成是一个多维矩阵，运用矩阵运算，可以对样本进行各种分析，比如主成份分析、相似度分析等。
4.向量和矩阵运算只适用于对象之间线性关系的运算和分析，不知道这样理解对不对</div>2020-12-06</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（0） 💬（1）<div>最大的收获就是矩阵乘法的几何解释，这次学习线性代数终于把矩阵的特征值和特征向量，矢量的旋转和伸缩窜起来了。总结起来就是:
列向量左乘一个方阵，相当于把这个方阵分解成特征向量和特征值，列向量在特征向量上的每一个分量都作相应特征值的伸缩，伸缩后的分量合成一个新的列向量就是左乘矩阵的结果。
矩阵本身就可以分解成列向量的集合，左乘方阵就相当于对每个列向量作相应的几何变换。
但是这只限于方阵，请问黄老师对于一般的矩阵乘法，也有几何解释吗？</div>2019-10-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/eb/c9/b1b233cf.jpg" width="30px"><span>小伟</span> 👍（0） 💬（1）<div>之前学过线性代数，但是没有跟实际的应用场景结合一起，多谢黄老师的课程；
知道了线性代数可以求文档相关性和去重，了解了LSM推到过程，了解了PCA和奇异值分解的过程。
但是这些在我的工作中都没有应用到，所以，后面会更多的去应用场景，用好这些工具。</div>2019-08-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/ef/1a/e7ebdfa8.jpg" width="30px"><span>道可</span> 👍（8） 💬（3）<div>打好基础：http:&#47;&#47;open.163.com&#47;special&#47;opencourse&#47;daishu.html</div>2019-08-08</li><br/><li><img src="" width="30px"><span>013923</span> 👍（1） 💬（0）<div>学习了新知识！</div>2022-09-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/b0/02/76f42fe7.jpg" width="30px"><span>lk</span> 👍（1） 💬（0）<div>基础的知识掌握的七七八八了，但是线性代数的数学思维还没建立起来，在现实项目中能想到并运用线性代数的能力还需要大大提高。</div>2022-06-19</li><br/>
</ul>