你好，我是黄申。

上一节，我们介绍了基于概率的语言模型。概率语言模型的研究对象其实是一个词的序列，以及这个词序列出现的概率有多大。那语言模型是不是也可以用于估算其他序列出现的概率呢？答案是肯定的。

通过上一节我们知道，语言模型中有个重点：马尔科夫假设及对应的多元文法模型。如果我们把这一点进一步泛化，就能引出马尔科夫模型。也就是说，只要序列的每个状态之间存在转移的概率，那么我们就可以使用马尔科夫模型。有时候情况会更复杂，不仅每个状态之间的转移是按照一定概率进行的，就连每个状态本身也是按照一定概率分布出现的，那么还需要用到隐马尔科夫模型。

今天这一节，我们就来学习马尔科夫模型、隐马尔科夫模型，以及它们在PageRank和语音识别中的应用。

## 马尔科夫模型

在介绍语言模型的时候，我们提到了马尔科夫假设，这个假设是说，每个词出现的概率和之前的一个或若干个词有关。我们换个角度思考就是，**每个词按照一定的概率转移到下一个词**。怎么个转移呢？我来解释一下。

如果把词抽象为一个状态，那么我们就可以认为，状态到状态之间是有关联的。前一个状态有一定的概率可以转移到到下一个状态。如果多个状态之间的随机转移满足马尔科夫假设，那么这类随机过程就是一个马尔科夫随机过程。而刻画这类随机过程的统计模型，就是**马尔科夫模型**（Markov Model）。
<div><strong>精选留言（29）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/13/18/d0/49b06424.jpg" width="30px"><span>qinggeouye</span> 👍（23） 💬（4）<div>机器翻译，比如一个中文句子翻译为英文，中文句子可拆分为多个词，每个中文词可能会匹配多个英文单词，一句中文翻译为英文，可能会有多种翻译结果。那么：
隐藏状态层 -&gt; 多种翻译结果可能性中的一种
输出层 -&gt; 每个中文词可能匹配多个英文单词

这样理解不知是否正确？</div>2019-03-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/a2/aa/bf65e8be.jpg" width="30px"><span>Thinking</span> 👍（18） 💬（1）<div>不理解一个地方，由读音推测汉字的过程为什么要算P(xiang(4)mu(4)|项目）概率而不是P(项目|xiang(4)mu(4)）概率？隐马尔科夫解决由输出层找到产生输出的隐藏状态层，为什么不换个角度说成由看得到的输入层找到隐藏状态层呢？</div>2019-02-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/6a/8e/7b6ea886.jpg" width="30px"><span>Joe</span> 👍（11） 💬（1）<div>隐马尔科夫模型在语音中的应用，流程是：
1，根据拼音去找到单个对应的词语，不考虑声调的概率。
2，再根据词语之间转移的概率，词语对应目标音高的概率，进而求出整个句子输出的概率。概率越大，可能性越高。
因此第一个词可以是xiangmu 对应语料库的所有词，不一定是四声，可以是香木之类的词语。
不知道这样理解对不对？
</div>2019-02-21</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/eyKgpIVFSDQBia7SJRVUKFh5qgwc3ohzEPSKvchLf9ZvwIO9CrS470ER7OhNzWTs0svECHCBiarQTa41BO3Hf0DA/132" width="30px"><span>Temme</span> 👍（9） 💬（1）<div>学习了之后，又看了几篇隐马尔可夫的一些收获：
隐马尔科夫适用于，只知道表象（观测层或者输出层），而内部的状态是隐藏的，未知的，可能有无数种的。语音识别分为3步
1.表象推测出隐藏，比如xiangmu的拼音读音可以推出隐藏的的实际字是项目，也可以是香木，各自有一定的概率，也可以是木香，只不过概率为0。推出概率在一定值以上的，做为隐藏状态。
2.各自推出隐藏的状态后，这些状态可以组合成各种状态链，比如橡木凯发世间这种，可以想象会有各种诡异的词组，这时候就用到马尔科夫状态转移的方法，筛选出靠谱的词组，得出结论，比如项目转开发转时间，这个状态转移的概率很高。概率在一定值以上的，挑出来作为状态序列。
3.语音识别只需要一个解。也是隐马尔科夫的关键，隐藏层推出表象层的概率（有点反人类认知，但是贝叶斯告诉我们这个无非是个数学的转换），大概就是，p(隐藏)*p(表象|隐藏)。好像p(表象|隐藏)，就是隐藏推表象的概率，其实不然，这里的推出还要考虑到隐藏层本身是个状态转移。</div>2019-07-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/01/b9/73435279.jpg" width="30px"><span>学习学个屁</span> 👍（2） 💬（1）<div>输出层 是输出的文字（被正确翻译的句子）  隐藏层句子中每个字 单词 背后的意思 ， 连在一起组成的顺通句子。  可以这么理解吧。</div>2020-01-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/ed/4e/ef406442.jpg" width="30px"><span>唯她命</span> 👍（2） 💬（1）<div>老师你好  ， “隐藏状态层”产生“输出层”的概率   这句话是什么意思。
还有  “隐藏状态层”产生“输出层”的概率 那个公式是怎么推导出来的。</div>2019-03-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/00/4e/be2b206b.jpg" width="30px"><span>吴小智</span> 👍（1） 💬（1）<div>希望老师可以说一下这些模型在实际的工程项目中，主要的难点在哪里？比如说，在运用语言模型来解决 PageRank 的实践中，在那个步骤会是难点。</div>2020-01-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/5c/02/e7af1750.jpg" width="30px"><span>teddytyy</span> 👍（1） 💬（1）<div>输出层是目标语言的分词，状态层是源语言的分词，这么建模对吗？</div>2019-12-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/ed/4e/ef406442.jpg" width="30px"><span>唯她命</span> 👍（1） 💬（1）<div>老师  语音识别的例子 
隐藏层  ：项目开发时间  或者 橡木开发事件
输出层  ： xiang(四声)mu(四声) kai(一声)fa(一声) shi(四声)jian(四声)
               xiang(一声)mu(-声) kai(一声)fa(一声)  shi(四声)jian(四声)
               xiang(四声)mu(-声) kai(一声)fa(一声)  shi(二声)jian(四声)
               等等所有可能的音调
不知道这么理解对不对？

另外还望老师解答下课后思考题</div>2019-03-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/50/99/44378317.jpg" width="30px"><span>李皮皮皮皮皮</span> 👍（1） 💬（1）<div>讲马尔可夫链的第一张图中，就是ABC的那种图，链上的概率是如何得出的？
我的理解是：根据语境中出现的AB，BC这些组合的概率，比如说总共有10个二元组，AB出现了一次，所以A到B的链上概率是0.1。如果按这么理解的话，那么所以的概率之和应该等于1吧。但是我算了一下，图中概率和等于1.1。不知道我理解的对不对，望老师解答😢</div>2019-03-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/69/4d/81c44f45.jpg" width="30px"><span>拉欧</span> 👍（1） 💬（1）<div>输出层：被翻译语言，隐藏状态层：翻译语言
比如要翻译 get busy living ,or get busy dying
输出层为 get busy living ,or get busy dying
隐藏层可能为：
1、要么忙于生存，要么赶着去死
2、忙于活，或忙于死
。。。
然后按照隐马尔科夫概率乘积选择概率最大的
不知道这么理解对不对？
</div>2019-02-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/bc/a0/97c7679b.jpg" width="30px"><span></span> 👍（1） 💬（1）<div>硬着头皮看完。比如英文翻译目标语言，我认为要翻译的文本的直接看出来的特征（单词包括的意思），单词之间的语法规则和词性，时态是隐藏层。
数学差看起来就是费劲o(╥﹏╥)o，英文差看不懂最新论文。不清楚大家啥水平，我建议老师多给几个例子好理解些。深刻体会到没多一个公式，少一分看下去的兴趣。</div>2019-02-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/61/f2/ca989d6f.jpg" width="30px"><span>Leon Wong</span> 👍（0） 💬（1）<div>老师，我还想请教下，HMM模型是否只有二元文法模型？我看你举例都是以二元作为讲解的基础</div>2022-12-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/61/f2/ca989d6f.jpg" width="30px"><span>Leon Wong</span> 👍（0） 💬（1）<div>请教下老师，语音识别的真实环境下，是怎么通过人说话的语音转化成可以被统计观测的拼音？</div>2022-12-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/60/76/be584def.jpg" width="30px"><span>张祈璟</span> 👍（0） 💬（1）<div>关于马尔可夫猜想，是不是因为大多数人的大脑一般也就是一二阶的样子，所以跟人相关的一系列活动基本上都是一二阶，跟人无关的或者因为阶数太多人无法理解的就无解了</div>2021-06-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/9a/89/babe8b52.jpg" width="30px"><span>A君</span> 👍（0） 💬（2）<div>在机器翻译里用隐马尔科夫建模，得到的不就是RNN么？隐藏状态层表示的是模型从一个词那提取到的信息，这些信息又会传到下一个step来帮助提取下一个词的信息。输出层是二元文法的预测结果，根据前一个词来预测后一个词。状态与状态之间，状态和结果之间，它们的概率用权重表示，在代码里就是用线性层、全连接层表示。</div>2020-07-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg" width="30px"><span>建强</span> 👍（0） 💬（1）<div>思考题：机器翻译中，使用隐马尔科夫模型，隐藏层表示的是目标语言组成的句子，在输出层表示的是用源语言表达的待翻译句子的各种组合形式</div>2020-06-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/53/3c/c86e3052.jpg" width="30px"><span>猛仔</span> 👍（0） 💬（1）<div>老师可不可以说马尔科夫模型是语言模型的进阶？
</div>2020-04-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/75/be/6f3ab95e.jpg" width="30px"><span>拉普达</span> 👍（0） 💬（1）<div>“对 Dijkstra 算法稍加修改，来找出权重乘积最大的最优路径，提升查找的效率。”这句话没太理解。构造出来的图，所有的输出层节点是没有出度的。如果进行最优路径计算，起点和终点分别是什么？是不是增加一个起点和终点，起点转移到所有第一个音节的权重都是0，所有最后一个音节转移到终点的权重也是0。然后计算起点到终点经过隐层节点的的最长路径？
</div>2020-03-29</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（0） 💬（1）<div>隐式马尔可夫的公式是不是计算y1 y2 y3 出现的联合概率，如果是的话，感觉少乘了两个p(x 1)，不知道是不是这样.</div>2019-09-06</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（0） 💬（1）<div>Pagerank公式可以分成两部分，第一部分,根据入链接，计算出每个入链接对该页面的PageRank贡献。第二部分，是把这个值和1&#47;N，用α和1-α加权。第一部分体现的是其他指向该网页贡献。但是第二部分，我看不懂，老师能否解释一下，为什么要和1&#47;N加权，多谢！</div>2019-09-06</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLBemRWsNxJt0WmF3rswPMnsO2ibRKok3OiakicIsFTe8xEudicnytT7zv07E26LfMBG4ic0XsZ87WNPlA/132" width="30px"><span>凝神寂照</span> 👍（0） 💬（1）<div>老师您好，对于PageRank的web拓扑图，网页A链接网页B，网页B链接网页C,一个网页只和前一个网页有关，这是一个一阶的马尔科夫链，但实际中网页A也可能链接C吧，为什么PageRank不用高阶的马尔科夫链呢？</div>2019-07-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/42/df/a034455d.jpg" width="30px"><span>罗耀龙@坐忘</span> 👍（3） 💬（0）<div>茶艺师学编程

思考题：如果机器翻译中使用以马尔科夫进行建模，那么隐藏状态层表示是什么？输出层表示的又是什么？

就拿中文和日语互相翻译来说。

如果说是中文翻译成日文，隐藏状态层则表示“翻译成日语的可能结果之一”，输出层则表示“需要翻译的中文”。

为什么选中日语来举例子呢？因为日语有这么一个特点：日语中的一类说法等于中文的很多种说法，换句话说日语x能对应到中文的y1、y2、y3、y4……这样隐马尔科夫看起来也像一个“梯形”。

关于马尔科夫模型：

马尔可夫过程要求满足四个条件 ——

第一，系统中有有限多个状态。

第二，状态之间切换的概率是固定的。

第三，系统要具有遍历性，也就是从任何一个状态出发，都能找到一条路线，切换到任何一个其他的状态。

第四，其中没有循环的情况，不能说几个状态形成闭环，把其他状态排斥在外。

而数学定理说，只要是马尔可夫过程，不管你的初始值如何，也不管你在这个过程中有什么一次性的干预，它终究会演化到一个统计的*平衡态*：其中每个状态所占的比例是不变的。</div>2020-04-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/64/9b/d1ab239e.jpg" width="30px"><span>J.Smile</span> 👍（2） 💬（0）<div>要点：
隐马尔科夫模型需要回答的最主要问题是：给定一个模型和某个特定的输出序列，如何找到最可能产生这个输出的状态序列。</div>2020-02-27</li><br/><li><img src="" width="30px"><span>013923</span> 👍（0） 💬（0）<div>学习了，谢谢！</div>2022-07-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/9a/89/babe8b52.jpg" width="30px"><span>A君</span> 👍（0） 💬（0）<div>根据马尔科夫假设和二元文法得出一阶马尔科夫模型，即当前状态是由前一个状态根据一定概率转换而成，这种状态的随机转换过程称为马尔科夫过程。如果这些状态并不确定，而是呈现某种概率分布，就需要用到隐马尔科夫模型。隐马尔科夫模型是二阶马尔科夫模型，隐藏状态层的状态只表示确定状态的部分信息，例如某个单词的隐藏状态表示的只是这个单词的词性。隐马尔科夫模型的第二层是输出层，表示隐藏状态经某概率转换后得到的确定状态。状态与状态之间，状态与输出之间，会根据某种概率进行转换，要计算这个句子出现的概率，可以计算第一个单词出现的概率乘以所有状态与状态，状态与输出之间转换🉐概率。</div>2020-07-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/a0/3f/06b690ba.jpg" width="30px"><span>刘桢</span> 👍（0） 💬（0）<div>等一个茶艺师</div>2020-04-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/f5/e0/76822dd9.jpg" width="30px"><span>南边</span> 👍（0） 💬（0）<div>尝试推导一下音频解析的公式：</div>2019-12-13</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（0） 💬（0）<div>思考题
输出层是原语言的句子，隐藏状态层是目标语言的单词和句子，我们要求解的是找到隐藏层中的句子，使得输出层（原语句，也就是被翻译的句子）出现概率最大。</div>2019-09-06</li><br/>
</ul>