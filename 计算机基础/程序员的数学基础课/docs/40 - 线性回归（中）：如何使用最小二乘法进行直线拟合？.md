你好，我是黄申。

上一节，我提到了，求解线性回归和普通的线性方程组最大的不同在于误差ε。在求解线性方程组的时候，我们并不考虑误差的存在，因此存在无解的可能。而线性回归允许误差ε的存在，我们要做的就是尽量把ε最小化，并控制在一定范围之内。这样我们就可以求方程的近似解。而这种近似解对于海量的大数据分析来说是非常重要的。

但是现实中的数据一定存在由于各种各样原因所导致的误差，因此即使自变量和因变量之间存在线性关系，也基本上不可能完美符合这种线性关系。总的来说，线性回归分析并不一定需要100%精确，而误差ε的存在可以帮助我们降低对精度的要求。通常，多元线性回归会写作：

$y=b\_0+b\_1·x\_1+b\_2·x\_2+…+$  
$b\_{n-1}·x\_{n-1}+b\_n·x\_n+ε$

这里的$x\_1，x\_2，…，x\_n$是自变量，$y$是因变量，$b\_0$是截距，$b\_1$，$b\_2$，…，$b\_n$是自变量的系数，$ε$是随机误差。

在线性回归中，为了实现最小化$ε$的目标，我们...
<div><strong>精选留言（20）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/10/e8/87/89561ed0.jpg" width="30px"><span>Peng</span> 👍（6） 💬（1）<div>老师，线性代数这部分开始很难看懂了，是不是需要先复习一遍线代？请老师指点，明确课程前置条件。</div>2019-03-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/42/df/a034455d.jpg" width="30px"><span>罗耀龙@坐忘</span> 👍（3） 💬（1）<div>茶艺师学编程

刚打开课文，“哇，全是公式，AWSL。”

但真的学下来，脑子居然蹦出来说：“啊，我能跟上！”（手斜眼看着脑）

这节课我是这么消化的：

最小二乘（平方）法，就是在考虑误差的情况下，求解一组方程组的系数矩阵B，并且确保误差最小。按照这样的要求，系数矩阵B=（X’X）^(-1)X’Y。

这节课就是*花大力气*在推导B是怎么来的。

在推导过程中，矩阵操作是主线，tr（）对角线加和、矩阵点乘的转置&#47;逆。而在确保误差最小时，线性函数的求导操作起大作用，一阶导数为0表示函数为“极值”，二阶导数&gt;0表示该“极值”为“最小值”（反之为“最大值”）。


感觉回到了大学，但在大学没能现在理解如此清晰···············不愧是黄老师！</div>2020-05-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/3f/2b/966c348b.jpg" width="30px"><span>zzz</span> 👍（3） 💬（1）<div>不太明白为什么要取tr()</div>2019-04-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/09/8e/2ff2823e.jpg" width="30px"><span>howhigh</span> 👍（3） 💬（1）<div>黄老师，关于矩阵求导的步骤我依然没有看懂，有没有矩阵求导的资料推荐？</div>2019-03-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/98/2c/cff47039.jpg" width="30px"><span>米饭</span> 👍（2） 💬（1）<div>分享一下国外的网站“数学乐“，里面对于一些数学概念，运算法则讲的很形象具体
就比如这节课的重点就在如何求ε的最小值，这里用到了导数的概念，可以参考用导数求极大值极小值(https:&#47;&#47;www.shuxuele.com&#47;calculus&#47;maxima-minima.html)
对于我这种数学渣也能快速理解</div>2020-08-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/18/d0/49b06424.jpg" width="30px"><span>qinggeouye</span> 👍（2） 💬（2）<div>补充证明的步骤 d ，tr(B&#39;X&#39;XB) 对 矩阵 B 的求导，拆成了两部分 tr(B&#39;(X&#39;XB)) 、tr((B&#39;X&#39;X)B)  分别对 B 求导，求和；意思是 B&#39; 和 B 实际上都与矩阵B相关，第一部分求导相当于固定 B 再对 B 求导，第二部分固定 B‘ 后对 B 求导，而后两部分利用了补充证明步骤 c 的结论得出证明结果？</div>2019-03-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/7e/a6/4e331ef4.jpg" width="30px"><span>骑行的掌柜J</span> 👍（1） 💬（1）<div>感觉零基础的我看懂了七七八八。。。但是在求导那里有点懵 还是下来好好补一下线代 
思考题这个“计算 train.csv 中所有样本因变量预测值和真实值之间的误差。”是指的MSE吗？黄老师
如果是 那么我用traIn_test_split 把训练集和测试集 按默认的测试集占比33%划分数据集后
最后拟合得到的MSE 是 23.4644515655
但是按8： 2划分后 MSE 是 23.4148125597
发现多运行几次 会有不一样的MSE 。。。这是怎么回事呢 黄老师 谢谢</div>2020-07-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/a3/66/5e339b4c.jpg" width="30px"><span>liying</span> 👍（1） 💬（3）<div>老师，课程中好像缺步骤b的证明公式</div>2020-04-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/a1/23/2b527dc4.jpg" width="30px"><span>逐风随想</span> 👍（1） 💬（1）<div>黄老师，越到后面越吃力，怎么办。当年家里条件不好，读到初二就辍学出去工作了。是否需要从新把初中到大学的数学知识补一补呢？</div>2019-03-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/69/4d/81c44f45.jpg" width="30px"><span>拉欧</span> 👍（1） 💬（1）<div>今天的数学推导看的有点吃力，不过还好</div>2019-03-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/98/2c/cff47039.jpg" width="30px"><span>米饭</span> 👍（0） 💬（1）<div>补充一下步骤c的同理证明
d(ΣΣ(y&#39;x)_j,i * b_i,j)&#47;db_i,j
= d(ΣΣ(y&#39;x)&#39;_i,j * b_i,j)&#47;db_i,j
=(y&#39;x)&#39;_i,j
=(Y&#39;X)&#39;
=X&#39;Y
老师对吗？</div>2020-08-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/53/3c/c86e3052.jpg" width="30px"><span>猛仔</span> 👍（0） 💬（1）<div>谢谢老师，之前对最小二乘法的理解一直是记公式，看了老师的讲解终于明白了</div>2020-04-01</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（0） 💬（1）<div>最小二乘法是最小化误差作为目标求出系数矩阵。
虽然以前学过微积分和线性代数，但是矩阵函数的求导还是不太熟悉，老师在这方面有什么文章书籍推荐吗，多谢！</div>2019-10-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/4f/de/72f2f849.jpg" width="30px"><span>Minis-Shu</span> 👍（0） 💬（1）<div>老师，您好！我想问一下，我通过最小二乘求出的系数矩阵是非正交矩阵，有没有什么办法，使得结果是正交阵呢？</div>2019-06-18</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（2） 💬（0）<div>tr(B&#39;X&#39;XB) 对 矩阵 B 的求导，拆成了两部分 tr(B&#39;(X&#39;XB)) 、tr((B&#39;X&#39;X)B) 分别对 B, 这一步没有看懂，主要是因为tr函数，这个函数只有在矩阵中出现而不在其他单因变量的函数中出现，我的疑问是求导的乘法法则可以直接应用在tr函数的里面而不用管tr函数对求导的影响吗?</div>2019-10-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg" width="30px"><span>建强</span> 👍（1） 💬（0）<div>用python代码计算了一下因变量预测值和真实值之间的误差，误差的平均值和方差都很小，输出结果：
误差平均值= 8.05648566542149e-17
误差的方差= 0.2594756827028548

python程序代码如下：

# 用线性回归模型预测房价
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

df = pd.read_csv(&quot;G:&#47;开发实验区&#47;实验数据&#47;房价预测样本数据&#47;boston_housing_data.csv&quot;)       #读取Boston Housing中的train.csv
#删除目标值是nan的样本
nanindex = list(df[df[&#39;MEDV&#39;].isna()].index)
for row in nanindex:
    df.drop(index=row, inplace=True)

df_features = df.drop([&#39;MEDV&#39;], axis=1)     #Dataframe中除了最后一列，其余列都是特征，或者说自变量
df_targets = df[&#39;MEDV&#39;]                    #Dataframe最后一列是目标变量，或者说因变量

# 标准化处理后，再进行回归分析
standardScaler = StandardScaler()    #基于Z分数的标准化
standardScaler.fit(df)
df_standardized = standardScaler.transform(df)  #对原始数据进行标准化，包括特征值和目标变量
df_features_standardized = df_standardized[:, 0:-1] #获取标准化之后的特征值
df_targets_standardized = df_standardized[:, -1]    #获取标准化之后的目标值

# 线性回归
regression_standardized = LinearRegression().fit(df_features_standardized, df_targets_standardized)

# 计算预测误测

# 设置系数矩阵B
B=regression_standardized.coef_.reshape(len(regression_standardized.coef_),1)

# 设置特征矩阵X
X = df_features_standardized

# 设置目标矩阵Y
Y = df_targets_standardized.reshape(len(df_targets_standardized),1)

# 计算预测误差绝对值
E = np.dot(X,B) - Y

# 输出预测误差的平均值和方差
print(&#39;误差平均值=&#39;, np.mean(E))
print(&#39;误差的方差=&#39;, np.var(E))</div>2020-10-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg" width="30px"><span>那时刻</span> 👍（1） 💬（0）<div>重温下久违的线性代数</div>2019-03-18</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/PBMtVAjHwoMWWrqzTPia9lJd2TKb9TlAmRJ5Zxib6TDcia4nAhDvqVauPh3hHEvySraFaOpfUMMDa9ZHGxWtV8alg/132" width="30px"><span>Geek_62f62c</span> 👍（0） 💬（0）<div>老师，关于文稿中的证明我有一个疑问，文稿中
(X&#39;XB)+ (B&#39;X&#39;X)&#39;
= X&#39;XB+X&#39;XB
=2X&#39;XB

我的理解是否应该是
(X&#39;XB)+ (B&#39;X&#39;X)&#39;
= X&#39;XB+(X&#39;X)&#39;B
=X&#39;XB+XX&#39;B

请问XX‘B和X&#39;XB相等吗</div>2025-02-03</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/eyKgpIVFSDQBia7SJRVUKFh5qgwc3ohzEPSKvchLf9ZvwIO9CrS470ER7OhNzWTs0svECHCBiarQTa41BO3Hf0DA/132" width="30px"><span>Temme</span> 👍（0） 💬（0）<div>有个疑问，tr()是取对角线，但这里实际tr当中只是一个一行一列的矩阵，感觉这里应该会有拓展，实际应用中有没有可能tr当中是多行多列的矩阵，比如多个自变量对应多个因变量需要去拟合？用最小二乘也应该适用？</div>2019-10-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/4f/de/72f2f849.jpg" width="30px"><span>Minis-Shu</span> 👍（0） 💬（0）<div>老师 我说的非正交矩阵就是行列式不为1的矩阵，我其实是用您讲的最小二乘法求解刚体变换来着，把矩阵看成空间坐标系的一组基向量，矩阵非正交的话，基向量之间的外积就不为0，而利用非正交矩阵进行刚体变换时，向量变换之后，其长度及夹角会发生变化。我就想着，如何将非正交矩阵正交化呢？</div>2019-06-19</li><br/>
</ul>