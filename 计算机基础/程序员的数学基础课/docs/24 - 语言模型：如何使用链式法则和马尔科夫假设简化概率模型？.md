你好，我是黄申。

之前我给你介绍了用于分类的朴素贝叶斯算法。我们讲了，朴素贝叶斯算法可以利用贝叶斯定理和变量之间的独立性，预测一篇文章属于某个分类的概率。除了朴素贝叶斯分类，概率的知识还广泛地运用在其他机器学习算法中，例如语言模型、马尔科夫模型、决策树等等。

今天我就来说说，基于概率和统计的语言模型。语言模型在不同的领域、不同的学派都有不同的定义和实现，因此为了避免歧义，我这里先说明一下，我们谈到的语言模型，都是指基于概率和统计的模型。

## 语言模型是什么？

在解释语言模型之前，我们先来看两个重要的概念。第一个是链式法则，第二个是马尔科夫假设及其对应的多元文法模型。为什么要先说这两个概念呢？这是因为链式法则可以把联合概率转化为条件概率，而马尔科夫假设通过变量间的独立性来减少条件概率中的随机变量，两者结合就可以大幅简化计算的复杂度。

### 1.链式法则

链式法则是概率论中一个常用法则。它使用一系列条件概率和边缘概率，来推导联合概率，我用一个公式来给你看看它的具体表现形式。

![](https://static001.geekbang.org/resource/image/de/0c/de5d37d9392a18225b4f9d522b4f180c.png?wh=554%2A42)

其中，$x\_{1}$到$x\_{n}$表示了n个随机变量。
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/12/4e/ef/2ad3effd.jpg" width="30px"><span>枫林火山</span> 👍（22） 💬（3）<div>黄老师，一直没想明白多元文法里的前面N个词的是否有顺序。例如: 大家好， 家大好 。 这2种情况都符合三元文法中的P(xn|xn-2,xn-1)的统计条件吗？
推广下 P(x1,x2,x3,x4….xn) 等于 P(xn,xn-1,xn-2,….,x4,x3,x2,x1) 吗？
百度-联合概率是指在多元的概率分布中多个随机变量分别满足各自条件的概率。我的理解联合概率的条件是可以交换顺序的。
所以从联合概率定义的角度理解是等于的， 但是从语法模型的角度理解，语法是有顺序的，那用联合概率表示对不对。
本节讲的语法模型是把一句话当作有序队列去对待的还是无序集合对待的？ 我听您的讲解是感觉是有序的，但是我理解公式从联合概率定义又感觉公式是在说一个无序的一组条件。 没法把这两者联系起来。
以下两组公式，我只能知道当使用一元文法时，二者时相等的。二元以上有点懵！老师能不能讲解下
P(x1,x2,x3,x4….xn) = P(x1)*P(x2|x1)*P(x3|x1,x2)*…….*P(xn|x1,x2,x3,x4….xn-1) 
P(xn,xn-1,xn-2,….,x4,x3,x2,x1) = P(xn)*P(xn-1|xn)*P(xn-2|xn,xn-1)*…….*P(x1|xn,xn-1,……,x2) </div>2019-04-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/bc/a0/97c7679b.jpg" width="30px"><span></span> 👍（8） 💬（1）<div>文本分类器，对给定文本进行判断。用特征词代表该文本。应该和上篇文章分类的计算有类似之处。计算每个特征词出现在该类文章的概率。然后根据权重分类？或者根据每个词的词频。
（我也很迷糊）那中文中有时词的顺序错乱也能表达一个意思。
比如，密码是123和321是密码；蹦迪坟头和坟头蹦迪。
比如（相互和互相；代替和替代）比
如，纳税可以是一个专有名词，也可以是人名，姓纳名税。还有那遇到多音字咋办？</div>2019-02-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/18/d0/49b06424.jpg" width="30px"><span>qinggeouye</span> 👍（6） 💬（1）<div>思考题：

首先，利用语言模型进行中文分词，计算句子 S=使用纯净水源浇灌的大米，属于哪种分词结果 W（“使用|纯净|水源|浇灌|的|大米”、“使用|纯净水|源|浇灌|的|大米”）的概率最大？

然后，回到上节的文本分类，再计算 分词结果W 属于哪种分类（“大米”、“纯净水”）的概率比较大？</div>2019-03-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/ed/4e/ef406442.jpg" width="30px"><span>唯她命</span> 👍（5） 💬（1）<div>已经求得p(q|d) = p(k1,k2,k3...,kn|d) = p(k1|d) * p(k2|k1,d) * p(k3|k2,k1,d) ....
那么我们怎么求得 p(k2|k1,d) 和  p(k3|k2,k1,d)呢</div>2019-02-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/d2/66/811970de.jpg" width="30px"><span>OP_未央</span> 👍（4） 💬（2）<div>思考题：
可以增加类别的先验概率，P(w1,w2...wn|C)*P(C)；或者已知大米广告的条件下，通过得到的不同分词计算所属类别的概率，选择属于大米概率大的那种分词？</div>2019-04-04</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/SsJajvXghPMDicSuOcx54mV6L9zv4KSKM2bKY0gsUdAH3oGCWzfRv9Q9HRljic2IvHzYFpRECp8SXGWhiaqGWFTKg/132" width="30px"><span>seleven</span> 👍（3） 💬（1）<div>换句话说：其实我是想问，如何能更好的利用全文或者说全部训练集的语义信息？</div>2019-02-16</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/SsJajvXghPMDicSuOcx54mV6L9zv4KSKM2bKY0gsUdAH3oGCWzfRv9Q9HRljic2IvHzYFpRECp8SXGWhiaqGWFTKg/132" width="30px"><span>seleven</span> 👍（3） 💬（1）<div>这篇文章和上篇文章中介绍的分类基本都是在假定文章中的词语相互独立的情况下进行的，虽然马尔科夫模型用到前面词的概率，这可以说是结合了部分上下文之间的语义关系，只使用了和前面词的语义关系，对文本分类其实效果已经很好，尤其是在语料充足的情况下可以使用机器学习的方法让分类模型自迭代优化，目前很多机器翻译就是这么干的。但是我想问下老师，针对语料不是很充足的文本集，如何使用全篇文章的语义（不只是前面词的上下文关系）来设计一个文本分类器？</div>2019-02-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg" width="30px"><span>建强</span> 👍（2） 💬（1）<div>思考题：
考虑分类信息的分词模式，只要把分词概率计算公式中的文档集合D，改为某一分类的文档集合T，即公式改为：arg maxP(Wi|T)，在某个文档类别下，取分词概率最大的那种切分方式，具体计算时，应在同类别下的所有文档中去计算每个分词出现的概率(即分词出现的频度)。</div>2020-05-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/ed/4e/ef406442.jpg" width="30px"><span>唯她命</span> 👍（2） 💬（1）<div>老师你好，p(K2 | k1,d) 指的是K2 | k1  和   d的联合概率
还是指的是 在满足k1和d的条件下，出现k2的概率？</div>2019-02-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/9a/89/babe8b52.jpg" width="30px"><span>A君</span> 👍（1） 💬（1）<div>链式法则将联合概率转成了一系列条件概率相乘，而马尔科夫假设简化了多条件概率的计算复杂度，两者结合，就可以计算一个查询、一条文本在数据集&#47;文档中出现的概率，不仅可以计算查询和文档的匹配度，还可以用于进行中文分词，根据数据集特性来对文本进行分词。</div>2021-01-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/42/df/a034455d.jpg" width="30px"><span>罗耀龙@坐忘</span> 👍（1） 💬（1）<div>茶艺师学编程

思考题：如何对语言模型加以改进，把分类信息也包含进去？

1、用另一纬度的信息来帮忙减少不确定性……我想到的是用另外一组信息表达式为y2，原来的为y1，求y1＊y2=0（正交），从而敲定正确分法。

2、链式法则公式中的文档D，直接替换成“全局函数”，即本段文字的语境，接下来就是计算在本语境下合适的分法是哪个?</div>2020-04-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/64/9b/d1ab239e.jpg" width="30px"><span>J.Smile</span> 👍（1） 💬（1）<div>思考题：朴素贝叶斯算法，计算哪种分词方法分类为大米或者纯净水的概率是最大的，比如分词方法A分为大米的概率最大，分词方法B分为纯净水的概率最大，那么根据实际文章分类情况选择不同的分词方法。</div>2020-02-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/ed/4e/ef406442.jpg" width="30px"><span>唯她命</span> 👍（1） 💬（1）<div>老师，$P(w_{1}, w_{2}, …, w_{n})$ 要在集合中找到一模一样的句子，基本是不可能的

不一定要找到一模一样的句子吧   例如abc   难道cab 这种打乱顺序的不行吗？</div>2019-02-26</li><br/><li><img src="" width="30px"><span>013923</span> 👍（0） 💬（1）<div>学习了！谢谢！</div>2022-07-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/e4/73/74dce191.jpg" width="30px"><span>鼠里鼠气</span> 👍（0） 💬（1）<div>“二元文法表示，某个单词出现的概率只和它前面的 1 个单词有关。”  这里的“某个单词出现的概率”为什么是条件概率呐？怎么看的?老师</div>2020-11-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/61/b8/7b23f8cb.jpg" width="30px"><span>本来是亚</span> 👍（0） 💬（1）<div>如果将分类的信息加入，则整个过程可以看作两个阶段：
1、中文分词。通过语料库计算各种分词序列的概率
2、分词后，计算词频，从而计算使得对应类最大概率的分词方式。</div>2020-04-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/64/9b/d1ab239e.jpg" width="30px"><span>J.Smile</span> 👍（0） 💬（1）<div>要点总结：
利用马尔科夫假设近似和多元文法-N-Gram模型原理进行两个工程中的应用：
①信息检索 ②中文分词</div>2020-02-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/bc/96/c679bb3d.jpg" width="30px"><span>总统老唐</span> 👍（0） 💬（1）<div>黄老师，关于信息检索，若令 p(d | q) 表示，接受到输入 q 这个查询时，用户期望检索的是文章 d 的概率。当我们针对某个具体查询，求出文章库中每一篇文章的概率，所有概率中最大值对应的文章就是用户最想检索的文章，若将 d 称为 q 的 “最相关文章”。那 p(q | d)的含义是否该这样描述，q1，q2，q3这三个查询的最相关文章都是d，现在已知针对某个查询，输出结果为d，那这个查询是q1的概率为 p(q1 | d)</div>2019-12-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/5c/02/e7af1750.jpg" width="30px"><span>teddytyy</span> 👍（0） 💬（1）<div>“对于同一个查询，其出现概率 P(q) 都是相同的，同一个文档 d 的出现概率 P(d) 也是固定的。因此它们可以忽略。“对于这个描述，具体的计算时如何忽略呢？</div>2019-12-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/c2/21/a8ef82ac.jpg" width="30px"><span>炎发灼眼</span> 👍（0） 💬（2）<div>老师你好，文中P(q|d)通过链式法则推导的公式没看懂，联合概率的推导可以看懂，这种带了d的条件概率的推导没有看懂，望老师能给予解释。
</div>2019-10-22</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（0） 💬（1）<div>信息检索是建立文章和查询的条件概率来判断查询和文章的紧密关系，这里查询当中一句句子来处理，然后用近似方法计算其在文中的概率，请问老师，查询里每个关键词的前后顺序和相邻都没什么关系，但是马尔可夫假设用到了前后和相邻关系，这样会不会造成较大误差？</div>2019-09-05</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（0） 💬（1）<div>链式法则和马尔可夫假设可以大大简化计算联合概率的复杂度。链式法则在数学上是等价。马尔可夫假设是不是基于这样的事实，文章中的单词和该单词前n个单词的关系较为紧密，但是和其他单词没什么关系，从另一个角度看，一个单词和紧接着的单词也有紧密关系，但这一层关系在马尔可夫假设里由后面的单词处理，请问老师是不是这样，多谢。</div>2019-09-05</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLBemRWsNxJt0WmF3rswPMnsO2ibRKok3OiakicIsFTe8xEudicnytT7zv07E26LfMBG4ic0XsZ87WNPlA/132" width="30px"><span>凝神寂照</span> 👍（0） 💬（1）<div>老师您好，对于信息检索部分，计算 P(d|q)=P(q|d)P(d)&#47;P(q)来对不同文档进行排序，P(q)是固定的我能理解，但是对于不同的文档P（d）应该是不同的吧，您为什么说P（d）是固定的呢？</div>2019-07-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/5f/09/80484e2e.jpg" width="30px"><span>李斌</span> 👍（0） 💬（1）<div>查询文档相关性的部分，elasticsearch 里常用 TF-IDF 以及最新的 BM25，两者都用到了 IDF 信息，但是感觉基于概率的方法没有用到 IDF 的信息啊</div>2019-07-14</li><br/><li><img src="" width="30px"><span>jennbetty</span> 👍（0） 💬（1）<div>老师我不明白中文分词求出的三元文法P(s|D)是如何求出argmax(P(Wi|D))的呢？比如说P(w2|D), P(w3|D)是怎么求出来的呢</div>2019-06-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/4e/ef/2ad3effd.jpg" width="30px"><span>枫林火山</span> 👍（0） 💬（1）<div>明白这个顺序在哪里体现了，谢谢老师的耐心讲解👍🏻</div>2019-04-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/ed/4e/ef406442.jpg" width="30px"><span>唯她命</span> 👍（0） 💬（1）<div>老师啊，中文分词，同一个句子，我们是不是把每种可能得分词 的ps都算出来啊，然后所有的ps求最大值，也就是这种分词的概率最大，然后我们就选择这种分词方法</div>2019-02-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/9a/89/babe8b52.jpg" width="30px"><span>A君</span> 👍（1） 💬（0）<div>👍🏻让我茅塞顿开。通过贝叶斯公式的链式法则，将由n个词组成的联合概率展开成一系列条件概率，再通过马尔科夫假设Ngram来简化条件概率的复杂性，从而计算出一篇文章中一个句子出现的概率，或者一个句子的所有分词的可能性，亦或者是在文库中输入一个关键字，最有可能检索到的文章。</div>2020-07-03</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（1） 💬（0）<div>思考题
原来中文分词的是一个句子在所有语料条件下成句的最大概率分词方法。如果语料足够多，可以计数在特定文章分类下的条件概率，然后取最大条件概率的分词方法。</div>2019-09-05</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（1） 💬（0）<div>中文分词就是选用不同的分词，然后计算每个分词成句的概率大小来对分词作优劣判断。</div>2019-09-05</li><br/>
</ul>