你好，我是黄申，今天我们继续来聊PCA主成分分析的下半部分。

上一节，我们讲解了一种特征降维的方法：PCA主成分分析。这个方法主要是利用不同维度特征之间的协方差，构造一个协方差矩阵，然后获取这个矩阵的特征值和特征向量。根据特征值的大小，我们可以选取那些更为重要的特征向量，或者说主成分。最终，根据这些主成分，我们就可以对原始的数据矩阵进行降维。

PCA方法的操作步骤有些繁琐，并且背后的理论支持也不是很直观，因此对于初学者来说并不好理解。考虑到这些，我今天会使用一个具体的矩阵示例，详细讲解每一步操作的过程和结果，并辅以基于Python的核心代码进行分析验证。除此之外，我还会从多个角度出发，分析PCA方法背后的理论，帮助你进一步的理解和记忆。

## 基于Python的案例分析

这么说可能有一些抽象，让我使用一个具体的案例来帮你理解。假设我们有一个样本集合，包含了3个样本，每个样本有3维特征$x\_1$，$x\_2$和$x\_3$。

![](https://static001.geekbang.org/resource/image/96/9f/962b0abb078974d1d964627e43081f9f.png?wh=332%2A208)

在标准化的时候，需要注意的是，我们的分母都使用m而不是m-1，这是为了和之后Python中sklearn库的默认实现保持一致。
<div><strong>精选留言（20）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/14/77/da/54c663f3.jpg" width="30px"><span>Wing·三金</span> 👍（21） 💬（1）<div>思考题：
基于分类的特征选择
优点：可以根据标签，有针对性地选择特征，可解释性强；
缺点：若是已有的特征未能反映数据的本质关系，可能会起不到精简特征的作用，因为每个特征几乎同等地重要。

基于 PCA 的特征降维：
优点：可以组合相关性强的特征维度，挖掘数据的本质特征；
缺点：因为失去了原始特征的描述，导致新的特征可解释性很差。</div>2019-04-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/1c/8a/90c9a8ea.jpg" width="30px"><span>半城柳色 / 半声笛</span> 👍（13） 💬（1）<div>这也是为什么，我们需要使用原始的数据去左乘这个特征向量，来获取降维后的新数据。

对于这句话还是不太理解，这个特征向量是协方差矩阵的特征向量，如果是协方差矩阵乘上特征向量，就能使得特征向量伸缩，也就是获得了协方差矩阵的主要方向（在特征值最大情况下），但是原始数据乘上协方差矩阵的特征向量，为啥得到的也是它的主成分？</div>2019-07-24</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKia6PQiaF3N9KvzbloVvicY9fQz3vs8C82ykfOTgNeMqpRAJxCICQgpIMFFTtQ2DrHej7IeFlcG9tdQ/132" width="30px"><span>Geek_a50e46</span> 👍（10） 💬（1）<div>老师您好，我这边还有点疑惑，就是这里的降维，究竟降的或者说去掉的是那几维特征。或者换个问法，计算出的特征值分别对应哪个列的特征呢？谢谢</div>2020-02-16</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（5） 💬（1）<div>我个人的理解，基于分类标签的选择主要选择那些和预测结果相关度高的特征，这类似于从结果出发反向选择那些有紧密联系的特征。这种方法精度较高，而且特征是原始的，容易理解。但是需要大量有标记的数据。
而PCA主成分分析法是从特征出发，分析特征之间的相关性和特征本身的信息量，最终重新合成特征，这些新的特征最大程度包含原特征的信息量。这种方法的好处是不需要标记数据。缺点是只是对特征分析，现实世界预测列和特征列的关系可能并不是特征列之间的相关性就能表达，而且新的特征是合成的，不好理解。</div>2019-10-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/48/30/77c6c4ec.jpg" width="30px"><span>动摇的小指南针</span> 👍（4） 💬（1）<div>老师，PCA的原理实际上就想使目标矩阵X的列向量组x1,x2,x3..方差最大，同时协方差为0。而协方差矩阵刚好满足这样的特征，只需要找到可以让协方差矩阵对角化的矩阵P，就可以使x1,x2,x3的彼此协方差化为0（即正交），而保留方差的特征，而对角化的矩阵P也就是X矩阵的特征向量集合。不知道我说的对不对</div>2019-07-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg" width="30px"><span>建强</span> 👍（2） 💬（1）<div>基于分类标签的特征选择，通过特征对类别划分的增益大小，即特征对样本类别的影响程度，来决定该特征是否对类别的判断是否有价值，主要用于有明确目标的监督式学习中。
优点：计算信息量较小，公式可解释性强，适用于监督式学习，特征可以是非数值型，缺点：易受噪声干扰影响精度。

基于PCA的特征分析，是分析样本特征本身所含的信息量大小，来决定特征是否有价值，并通过保留信息量大的特征，去除信息量小的特征，来达到特征降维的目的。
优点：精度较高，不受噪声干扰，适用于无监督式学习，缺点：计算量大，公式不直观，难理解，且特征值必须是数值型。

这是我个人的一点肤浅理解，请老师指正。
</div>2020-11-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/07/8c/0d886dcc.jpg" width="30px"><span>蚂蚁内推+v</span> 👍（2） 💬（1）<div>老师你好，问一下离散型的纬度是不是不适合参与这种方式降纬啊？如果有纬度是离散的，那应该怎么处理呢？</div>2019-03-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/a5/e1/6c300c33.jpg" width="30px"><span>hp++</span> 👍（1） 💬（1）<div>请问：
如果左乘矩阵对某一个向量或某些向量只发生伸缩变换，不对这些向量产生旋转的效果，那么这些向量就称为这个矩阵的特征向量，而伸缩的比例就是特征值。
旋转后就不是特征向量了么</div>2020-09-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/98/2c/cff47039.jpg" width="30px"><span>米饭</span> 👍（1） 💬（1）<div>我觉得本章的难点在与为什么协方差矩阵对角化可以降维，以下是我的理解：
协方差矩阵度量的就是维度和维度之间的关系。协方差矩阵主对角线上的元素就是各个维度上的方差，非主对角线上元素是各维度之间的相关性（协方差）。
既然我们的目的是为了降维，那就是让不同维度间的相关性尽可能的小，所以就可以转化为如何让协方差矩阵非对角线元素尽可能为零，要达到这个目的，就要进行矩阵对角化
不知道我的理解对不对</div>2020-08-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/4e/c7/8c2d0a3d.jpg" width="30px"><span>余泽锋</span> 👍（1） 💬（1）<div>关于选择协方差作为PCA分析的对象，有些困惑。
本质上，皮尔森系数和数据标准化的协方差是一致的，那么皮尔森系数是不是跟协方差一样可以拿来衡量信息量的大小和不同维度之间的相关性。
那么选择协方差或者皮尔森系数作为PCA分析的对象岂不是差不多？</div>2019-04-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/69/4d/81c44f45.jpg" width="30px"><span>拉欧</span> 👍（1） 💬（1）<div>基于分类标签的特征选择是监督式的，基于PCA的主成分分析是非监督式的，前者需要测试数据进行学习，后者不需要，可以这么理解么？</div>2019-03-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg" width="30px"><span>斯盖丸</span> 👍（0） 💬（1）<div>老师，我看到别的地方有用矩阵转置乘以矩阵的积，对这个对称阵的积再求特征值和特征向量的。用这个方法和用协方差矩阵求特征值和特征向量比，有什么区别，哪个更好呢？</div>2020-10-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/a5/e1/6c300c33.jpg" width="30px"><span>hp++</span> 👍（0） 💬（1）<div>请问：上面讲到的矩阵尽可能的对角化 是不是通过PCA求出正交矩阵R  R做成原始数据矩阵  达到数据矩阵列方差大 列和列之间基本正交的目的？</div>2020-09-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/7e/a6/4e331ef4.jpg" width="30px"><span>骑行的掌柜J</span> 👍（0） 💬（1）<div>大致懂了PCA主成分分析 谢谢黄老师 
还需要再反复咀嚼 后面打算做个这门课程的思维导图 方便把散的知识点串联起来📖✏</div>2020-07-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1b/7a/82/2890b534.jpg" width="30px"><span>jay</span> 👍（0） 💬（1）<div>PCA终于懂了，蟹蟹黄老师</div>2020-01-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/bb/7d/26340713.jpg" width="30px"><span>黄振宇</span> 👍（0） 💬（1）<div>刚刚还在上一篇有疑问，在这一节就得到解惑了。
“”需要注意的是，这个新的方向，往往不代表原始的特征，而是多个原始特征的组合和缩放。“”</div>2019-11-29</li><br/><li><img src="" width="30px"><span>013923</span> 👍（1） 💬（0）<div>复习大学论文的内容</div>2022-09-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/4e/c7/8c2d0a3d.jpg" width="30px"><span>余泽锋</span> 👍（1） 💬（0）<div>
思考题：
分类的特征选择
基于人工主观意识

 PCA 的特征降维：
基于统计学</div>2019-04-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/18/d0/49b06424.jpg" width="30px"><span>qinggeouye</span> 👍（1） 💬（0）<div>「可对角化的矩阵，对角化后对角线上的元素就是特征值」
https:&#47;&#47;blog.csdn.net&#47;danieljianfeng&#47;article&#47;details&#47;22171581</div>2019-03-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/42/df/a034455d.jpg" width="30px"><span>罗耀龙@坐忘</span> 👍（0） 💬（0）<div>茶艺师学编程

思考题：在监督学习中基于分类标签的特征选择，以及基于特征协方差矩阵的PCA主成分析，说说它们各自的优缺点。

分类标签的特征选择，这有点类似“眼前的兄弟是来自哪里？”，怎么判断？听他的口音，所用的特别词语，甚至是看他的身高面貌，大致上能判断他是＊来自＊哪里。
要能做到这一点，自己至少得知道其他地区的人是什么样子（总体大致的情况）。如果在这方面缺乏常识，这就很大有可能做错，或者做不出来。

基于特征的斜方差距阵PCA主成分分析，这更倾向于如何用“最少的信息来确定眼前这个人是谁”。比如说警察盘查行人，都会先要求对方拿出身份证——能90%确定一个人的身份，再不济就加上指纹 。
这不用先处理整体，直接聚焦个体就好。但如果说该个体本身的数据还不足以说明问题，那还得需要寻求新的数据，而且这个数据在能说明事件的同时最好还不能与原数据有太大的关系。
</div>2020-05-12</li><br/>
</ul>