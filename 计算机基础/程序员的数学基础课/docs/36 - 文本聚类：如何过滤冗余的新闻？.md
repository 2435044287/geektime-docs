你好，我是黄申。

前两节，我讲了向量空间模型，以及如何在信息检索领域中运用向量空间模型。向量空间模型提供了衡量向量之间的距离或者相似度的机制，而这种机制可以衡量查询和被查询数据之间的相似程度，而对于文本检索来说，查询和文档之间的相似程度可作为文档的相关性。

实际上，除了文档的相关性，距离或者相似度还可以用在机器学习的算法中。今天，我们就来聊聊如何在聚类算法中使用向量空间模型，并最终实现过滤重复文章。

## 聚类算法

在概率统计模块中，我们介绍了分类（Classification/Categorization）和回归（Regression）这两种监督式学习（Supervised Learning）。监督式学习通过训练资料学习并建立一个模型，并依此模型对新的实例进行预测。

不过，在实际场景中，我们常常会遇到另一种更为复杂的情况。这时候不存在任何关于样本的先验知识，而是需要机器在没人指导的情形下，去将很多东西进行归类。由于缺乏训练样本，这种学习被称为“非监督学习”（Unsupervised Learning），也就是我们通常所说的聚类（Clustering）。在这种学习体系中，系统必须通过一种有效的方法发现样本的内在相似性，并把数据对象以群组（Cluster）的形式进行划分。
<div><strong>精选留言（13）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg" width="30px"><span>mickey</span> 👍（7） 💬（2）<div># encoding=utf-8
from sklearn.feature_extraction.text import CountVectorizer

#模拟文档集合
corpus = [&#39;I like great basketball game&#39;,
          &#39;This video game is the best action game I have ever played&#39;,
          &#39;I really really like basketball&#39;,
          &#39;How about this movie? Is the plot great?&#39;,
          &#39;Do you like RPG game?&#39;,
          &#39;You can try this FPS game&#39;,
          &#39;The movie is really great, so great! I enjoy the plot&#39;]

#把文本中的词语转换为词典和相应的向量
vectorizer = CountVectorizer()
vectors = vectorizer.fit_transform(corpus)

from sklearn.feature_extraction.text import TfidfTransformer

#构建tfidf的值
transformer = TfidfTransformer()
tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))

# 输出每个文档的向量
tfidf_array = tfidf.toarray()
words = vectorizer.get_feature_names()

from Bio.Cluster import kcluster

#进行聚类，使用向量间的夹角余弦作为相似度的度量
clusterid, error, nfound  = kcluster(tfidf_array, nclusters=3, dist=&#39;u&#39;)
print(clusterid)
&#39;&#39;&#39;
Kmeans 的欧式距离:[0 2 0 1 2 2 1]
Bio 的夹角余弦:[2 0 0 2 0 2 1]
&#39;&#39;&#39;</div>2019-03-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/4e/c7/8c2d0a3d.jpg" width="30px"><span>余泽锋</span> 👍（6） 💬（2）<div>请问一下老师，余弦夹角本质上可以说是归一化的欧式距离么？</div>2019-04-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/91/00/2007d2f3.jpg" width="30px"><span>zhengfan</span> 👍（3） 💬（1）<div>黄老师您好。
您在k均值算法介绍后附的图示，似乎呈现出“恰巧把初选质心点选在了不同群组中”的效果。而作为k均值算法的第一步，最初质心的选择是完全随机的。也就是很有可能这些初选质心是在同一个群组中。请问这种情况如何处理？</div>2020-06-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/6a/8e/7b6ea886.jpg" width="30px"><span>Joe</span> 👍（3） 💬（2）<div>老师，现在很多机器学习的算法都有现成的库了，如sklearn等。但是直接调库，总觉得停留在表面。请问有没有必要自己手动去实现一些经典的机器学习算法呢？</div>2019-03-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/ba/c5/bf9b71a9.jpg" width="30px"><span>Bindy🌲</span> 👍（2） 💬（1）<div>老师，这里如果是中文，是不是要做分词呢</div>2019-04-10</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（1） 💬（1）<div>kmeans方法用的是利用几何信息将空间点归类的方法，最终的结果是归好类的点集质心之间的距离远，但是集合内点之间距离近的效果，可以类比于软件开发中的高内聚低耦合原则来记忆。</div>2019-09-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg" width="30px"><span>建强</span> 👍（0） 💬（2）<div>我自己写了一个用向量夹角的余弦做聚类的算法，但不论怎么调整，聚类结果和欧氏距离算法都不一样，是不是和初始质心选择有关，还是程序中的算法有问题，需要请教老师，篇幅有限，部分程序代码如下：

    # 用样本拟合聚类算法模型
    def fit(self, sample_matrix):
        #检查分类个数是否大于待分类的样本数
        if self.n_clusters &gt; len(sample_matrix):
            raise ValueError(&#39;cluster numbers must less than or equal to sample numbers&#39;)
        
        # 初始化质心，取前n个样本的向量作为质心
        self.cluster_centers_ = np.array([sample_matrix[0]])
        for j in range(1, self.n_clusters):
            self.cluster_centers_ = np.r_[self.cluster_centers_, [sample_matrix[j]]]
        # 迭代计算各样本与质心的夹角，并更新分组，直到新质心与旧质心之间的差距小于指定精度
        k = 0
        cluster = {}
        while True:
            cluster.clear()
            for i in range(len(sample_matrix)):
                max_cos = 0
                nearest_label = None
                for j in range(self.n_clusters):
                    cos = self.__cos(sample_matrix[i],self.cluster_centers_[j])
                    if max_cos &lt; cos:
                        max_cos = cos
                        nearest_label = j
                cluster.setdefault(nearest_label,[])
                cluster[nearest_label].append(i)
            
            #保存老的质心，计算新的质心
            old_cluster_centers = self.cluster_centers_.copy()
            for j in range(self.n_clusters):
                self.cluster_centers_[j] = 0
                for i in cluster[j]:
                    self.cluster_centers_[j] += sample_matrix[i]
                self.cluster_centers_[j] = self.cluster_centers_[j] &#47; len(cluster[j])
            # 用方差比较新老质心之间的差距
            if self.__variance(self.cluster_centers_, old_cluster_centers) &lt;= self.precision:
                break;
            k+=1
        # 根据计算结果给样本归类
        self.labels_ = list(&#39;0&#39; * len(sample_matrix))
        for j in range(self.n_clusters):
            for i in cluster[j]:
                self.labels_[i] = j
        self.labels_ = np.array(self.labels_)
 =================聚类结果==========
每个文档所属的分组
[0 1 2 1 0 1 0]</div>2020-09-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/d4/44/0ec958f4.jpg" width="30px"><span>Eleven</span> 👍（0） 💬（2）<div>看到这里就有点吃力了，工作中没有涉及到这块东西...</div>2020-06-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/7e/a6/4e331ef4.jpg" width="30px"><span>骑行的掌柜J</span> 👍（0） 💬（1）<div>我这里向量间使用欧式距离得到的是：[1 0 1 2 1 0 2]
而向量间用夹角余弦得到的是：[2 1 2 1 0 0 1]
scikit-learn没有相应的夹角余弦包，所以要用后面这个Biopython。
建议大家可以去Biopython官网看看相关用法，用anaconda安装Biopython的指令是 ：
conda install -c conda-forge biopython 
这是中文文档地址：https:&#47;&#47;biopython-cn.readthedocs.io&#47;zh_CN&#47;latest&#47;cn&#47;chr15.html
这是英文的：https:&#47;&#47;biopython.org&#47;wiki&#47;Download</div>2020-06-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/6a/8e/7b6ea886.jpg" width="30px"><span>Joe</span> 👍（8） 💬（0）<div>def customKmeans(vec_array, n_clusters=3, epochs=50):
    &#39;&#39;&#39;
    @description: 手动实现kmeans，以向量间的夹角余弦作为相似度
                 根据上述tf-idf得到的7条文本向量tfidf_array，进行聚类算法
    @param {type} vec_array- 向量组, n_clusters-聚类数目, epochs- 训练次数
    @return: cluster_labels- 分类标签
    &#39;&#39;&#39;

    # 初始化质心的位置
    cluster_centers = []
    cluster_centers_indx = []
    while (len(cluster_centers_indx) &lt; n_clusters):
        indx = random.randint(0, len(vec_array)-1)
        if indx not in cluster_centers_indx:
            cluster_centers_indx.append(indx)
            cluster_centers.append(vec_array[indx])

    # 初始化向量类别
    cluster_labels = [0] * len(vec_array)
    max_similarity = [-1] * len(vec_array)
    epoch = 0
    while(epoch &lt; epochs):
        # 计算每个向量与质心的相似性，并将其归纳到最近的质心集群中
        for i in range(0, len(vec_array)):
            max_similarity[i] = computeCosine(vec_array[i], cluster_centers[0])
            for j in range(1, n_clusters):
                temp = computeCosine(vec_array[i], cluster_centers[j])
                if (temp &gt; max_similarity[i]):
                    max_similarity[i] = temp
                    cluster_labels[i] = j

        # 更新质心位置
        for i in range(0, n_clusters):
            # 找到集群对应原向量的下标
            indxs = [indx for indx, cluster in enumerate(
                cluster_labels) if cluster == i]
            # 根据集群向量的平均值更新质点位置
            cluster_centers[i] = np.average(
                [vec_array[indx] for indx in indxs], axis=0)

        # 当满足迭代次数或平均最大相似性时退出算法
        epoch += 1
        if (np.average(max_similarity) &gt; 0.9):
            break
    return cluster_labels


def computeCosine(vec1, vec2):
    # 计算向量间的余弦值
    vecCosine = np.dot(vec1, vec2) &#47; \
        (np.linalg.norm(vec1) * np.linalg.norm(vec2))
    return vecCosine


# 应用customkmeans
labels = customKmeans(tfidf_array, n_clusters=3, epochs=1000)
print(labels)

输出结果：
[0 2 0 1 2 2 1]</div>2019-03-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/42/df/a034455d.jpg" width="30px"><span>罗耀龙@坐忘</span> 👍（2） 💬（2）<div>茶艺师学编程

K—Means算法，给我的感觉，就像是在铺满铁屑的纸上放几个磁铁，附近的铁屑会吸过来。

向量空间模型则是换上更强力的“磁铁”。

……

“什么还有没有吸过来的铁屑，那就不用管了”</div>2020-04-28</li><br/><li><img src="" width="30px"><span>013923</span> 👍（1） 💬（0）<div>Keep fit；Study well，and Work hard！</div>2022-09-14</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/PBMtVAjHwoMWWrqzTPia9lJd2TKb9TlAmRJ5Zxib6TDcia4nAhDvqVauPh3hHEvySraFaOpfUMMDa9ZHGxWtV8alg/132" width="30px"><span>Geek_62f62c</span> 👍（0） 💬（0）<div>请问老师 arg(j=1,n) avg (xij)公式中的arg代码什么含义？
</div>2025-01-31</li><br/>
</ul>