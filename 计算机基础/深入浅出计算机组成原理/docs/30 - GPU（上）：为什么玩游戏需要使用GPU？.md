讲完了CPU，我带你一起来看一看计算机里的另外一个处理器，也就是被称之为GPU的图形处理器。过去几年里，因为深度学习的大发展，GPU一下子火起来了，似乎GPU成了一个专为深度学习而设计的处理器。那GPU的架构究竟是怎么回事儿呢？它最早是用来做什么而被设计出来的呢？

想要理解GPU的设计，我们就要从GPU的老本行图形处理说起。因为图形处理才是GPU设计用来做的事情。只有了解了图形处理的流程，我们才能搞明白，为什么GPU要设计成现在这样；为什么在深度学习上，GPU比起CPU有那么大的优势。

## GPU的历史进程

GPU是随着我们开始在计算机里面需要渲染三维图形的出现，而发展起来的设备。图形渲染和设备的先驱，第一个要算是SGI（Silicon Graphics Inc.）这家公司。SGI的名字翻译成中文就是“硅谷图形公司”。这家公司从80年代起就开发了很多基于Unix操作系统的工作站。它的创始人Jim Clark是斯坦福的教授，也是图形学的专家。

后来，他也是网景公司（Netscape）的创始人之一。而Netscape，就是那个曾经和IE大战300回合的浏览器公司，虽然最终败在微软的Windows免费捆绑IE的策略下，但是也留下了Firefox这个完全由开源基金会管理的浏览器。不过这个都是后话了。
<div><strong>精选留言（24）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/1b/65/2b/446ef7b6.jpg" width="30px"><span>许先森</span> 👍（5） 💬（1）<div>可以对2D加速，各个流水线步骤都一样，Z轴维度都看作0即可。</div>2020-01-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/db/26/54f2c164.jpg" width="30px"><span>靠人品去赢</span> 👍（18） 💬（1）<div>我觉得可以对2D加速，刚才有人提过2D理解为深度为0的的3D，少处理一个维度。而且现在很多2D游戏，如果不对2D加速，那CPU真的够苦逼的。</div>2019-07-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/e7/2e/1522a7d6.jpg" width="30px"><span>活的潇洒</span> 👍（11） 💬（0）<div>眼过千遍不如手过一遍

day30天笔记:https:&#47;&#47;www.cnblogs.com&#47;luoahong&#47;p&#47;11413746.ht</div>2019-08-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/cf/96/251c0cee.jpg" width="30px"><span>xindoo</span> 👍（9） 💬（2）<div>很好奇cpu和gpu之间是如何交互的</div>2019-07-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/16/5b/83a35681.jpg" width="30px"><span>Monday</span> 👍（6） 💬（1）<div>gpu阉割版cpu😂</div>2020-06-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/4f/89/65ca6878.jpg" width="30px"><span>missingmaria</span> 👍（5） 💬（0）<div>应该可以对2D加速，2D其实就是z=0的3D</div>2019-07-04</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoib6BjEV4KPEaibP0MSA3f9VA21H2h4P8uIaatDqW7krBqJF6YH136TCNKGSbERHscOIEp0TlYlcbw/132" width="30px"><span>若失</span> 👍（3） 💬（0）<div>终于讲到GPU，希望老师多讲一些这方面的内容，对于游戏开发者来说深入了解底层硬件知识还是很重要的！</div>2019-07-03</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqXNhbTULKiakib8lYXrvGF2zPwfedooBzC2EtSv1nt1MwV1KUvTkcJrvCBFvcdwJicnr3OEXnk9GUCg/132" width="30px"><span>WENMURAN</span> 👍（2） 💬（0）<div>GPU:图形处理器
我们电脑显示的3D图形是通过多边形组合出来的。图形的移动和变化都是计算机根据图形学通过实时计算渲染出来的。渲染过程的步骤:1顶点处理，把三维的点转换成二维的。2，图元处理，把顶点转换之后的其他点连接成多边形。3，栅格化，把连接成的多边形进行栅格处理（像素）。 4，片段处理，计算每一个像素的颜色透明度信息，给像素点上色。5，像素操作，把不同像素点混合在一起。
计算量大，CPU计算能力不够，占用资源太多，只用硬件来进行图形渲染。只有顶点处理用CPU计算，后续都由显卡完成。</div>2020-04-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/15/69/187b9968.jpg" width="30px"><span>南山</span> 👍（2） 💬（0）<div>电脑集显，玩古老的传奇都会卡顿。以此推断出可以对2D进行加速～</div>2019-07-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/ec/3e/885ec1d2.jpg" width="30px"><span>宋不肥</span> 👍（2） 💬（0）<div>GPU主要靠硬件并行来加速图形处理，不管2D还是3D图像处理，本质上都是大规模的矩阵运算，应该都可以加速</div>2019-07-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/23/a8/1e/a04cbc6c.jpg" width="30px"><span>Kitty🐱婷🎉</span> 👍（1） 💬（0）<div>2D里由于z = 0，所以就不需要图元处理里的 剔除和剪裁 这个步骤了，其他对图形渲染的步骤与3D一致。 不知道我的理解对不对</div>2022-02-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/ae/19/5f2d96ce.jpg" width="30px"><span>Bruce</span> 👍（1） 💬（1）<div>3D模型会有一个基于多边形建模的三维图形的渲染过程。那手机拍照的时候需不需要这个渲染过程的，就是说静止的图片，要这个3D图形渲染过程吗？感觉手机咔嚓一下就拍照好了。</div>2021-01-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/0b/5d/c05da54e.jpg" width="30px"><span>水能载舟</span> 👍（1） 💬（0）<div>老师请问，OpenGL 的运行是否要求计算机一定要有图形卡模块，哪怕是集成显卡，是不是没有显卡模块就无法运行</div>2019-08-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/81/e9/d131dd81.jpg" width="30px"><span>Mamba</span> 👍（0） 💬（0）<div>虽然显卡可以加速2D图形处理，但并不是所有的2D图形操作都会自动由显卡加速。开发者需要确保他们的应用程序或系统驱动程序正确地利用了显卡的加速功能。在某些情况下，如果应用程序没有针对GPU加速进行优化，那么2D图形处理仍然会依赖于CPU。</div>2024-08-29</li><br/><li><img src="" width="30px"><span>Geek_88604f</span> 👍（0） 💬（0）<div>1.三维图形渲染过程，顶点处理、图元处理、栅格化、片段处理以及像素操作这 5 个步骤。
    2.这些步骤是固定的，数据之间没有依赖，可以并行执行。因此可以用专门的硬件来完成</div>2023-11-14</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKLkFXq7dy9GUuWibibss2PxKtDN3HSDsQW5MwFkH9guwictzkBt50LmSPjaCtJMMxTo5sia8UjX7pLoQ/132" width="30px"><span>Geek_23e324</span> 👍（0） 💬（1）<div>5400万为啥会等于54M</div>2022-03-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/fe/2d/e23fc6ee.jpg" width="30px"><span>深水蓝</span> 👍（0） 💬（1）<div>老师好，我有个疑惑，为什么顶点处理不能也交给GPU完成呢？顶点之间也是互相独立的，顶点的坐标系转换其实也是固定的矩阵运算，应该也很适合GPU的特性啊？</div>2020-02-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/25/00/3afbab43.jpg" width="30px"><span>88591</span> 👍（0） 💬（0）<div>GPU 计算 互相之间没有依赖，可以并行独立计算。GPU 减少了CPU 通用计算的一些设计，这样就使得GPU 更便宜和性能更高效（只做固定的事情）。</div>2019-12-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/f5/d4/5a0a2f8d.jpg" width="30px"><span>火腿</span> 👍（0） 💬（0）<div>贴图没讲， 其实在现代GPU中非常关键。</div>2019-11-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/47/10/2d673601.jpg" width="30px"><span>好饿早知道送外卖了</span> 👍（0） 💬（0）<div>安卓手机不是有个强制GPU2D加速的选项么</div>2019-10-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/f5/b8/9f165f4b.jpg" width="30px"><span>mfist</span> 👍（0） 💬（0）<div>应该可以加速2d图形，但是相对于3d图形计算少了很多，所以传统2d一般不开启gpu渲染</div>2019-08-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/3a/93/d7be8a1a.jpg" width="30px"><span>晓小东</span> 👍（0） 💬（0）<div>想知道像渲染引擎像openGL, 是如何利用GPU能力， 以及openGL的跨平台， 在GPU层面到底跨的是什么</div>2019-07-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/f8/23/937a1bb7.jpg" width="30px"><span>周</span> 👍（0） 💬（0）<div>2D图形也能就行加速。
2D的渲染也需要顶点处理，栅格化，片段处理，像素处理，等这些步骤，可能具体细节会不一样。 同样的，这些步骤中也存在独立可并行的计算。所以可以加速。</div>2019-07-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/79/4b/740f91ca.jpg" width="30px"><span>-W.LI-</span> 👍（0） 💬（0）<div>我觉得不能对2D加速。GPU算法是死的，里面的存储只能处理3D。存猜</div>2019-07-04</li><br/>
</ul>