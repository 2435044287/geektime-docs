搜索引擎的热门搜索排行榜功能你用过吗？你知道这个功能是如何实现的吗？实际上，它的实现并不复杂。搜索引擎每天会接收大量的用户搜索请求，它会把这些用户输入的搜索关键词记录下来，然后再离线地统计分析，得到最热门的Top 10搜索关键词。

那请你思考下，**假设现在我们有一个包含10亿个搜索关键词的日志文件，如何能快速获取到热门榜Top 10的搜索关键词呢？**

这个问题就可以用堆来解决，这也是堆这种数据结构一个非常典型的应用。上一节我们讲了堆和堆排序的一些理论知识，今天我们就来讲一讲，堆这种数据结构几个非常重要的应用：优先级队列、求Top K和求中位数。

## 堆的应用一：优先级队列

首先，我们来看第一个应用场景：优先级队列。

优先级队列，顾名思义，它首先应该是一个队列。我们前面讲过，队列最大的特性就是先进先出。不过，在优先级队列中，数据的出队顺序不是先进先出，而是按照优先级来，优先级最高的，最先出队。

如何实现一个优先级队列呢？方法有很多，但是用堆来实现是最直接、最高效的。这是因为，堆和优先级队列非常相似。一个堆就可以看作一个优先级队列。很多时候，它们只是概念上的区分而已。往优先级队列中插入一个元素，就相当于往堆中插入一个元素；从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/8e/2e/abb7bfe3.jpg" width="30px"><span>oatlmy</span> 👍（130） 💬（2）<div>老师，请问为什么评价算法性能是根据时间和空间复杂度，而不是别的参数？是因为计算机结构是冯诺依曼体系，除了输入输出设备和控制器，就剩下运算器和存储器了吗？</div>2018-11-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/ee/4f/b9ebc543.jpg" width="30px"><span>Miletos</span> 👍（107） 💬（5）<div>“如果新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆；如果新加入的数据大于等于小顶堆的堆顶元素，我们就将这个新数据插入到小顶堆。”

1.  这里不太对劲，前文中说到，小顶堆的堆顶大于大顶堆的堆顶。

如果新进元素在小顶堆堆顶和大顶堆堆顶元素值之间，没有规定插入哪个堆。

我觉得，是不是只要判断一次就可以了。新进元素值大于等于小顶堆堆顶元素的，插入小顶堆，否则插入大顶堆。
当某一个堆数据过多时再重新移动堆顶元素。

2.  求中位数的源数据中，是否允许重复数据？</div>2018-11-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/48/f6/aeb93f8a.jpg" width="30px"><span>豪华</span> 👍（35） 💬（6）<div>老师，分片求取前十是不是有bug，如果有一个关键词在每一组分片中都是前第十一位，在整个十亿中个数总和是第一位，是不是用分片求出了错误的结果呢？</div>2018-11-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/61/c1/93031a2a.jpg" width="30px"><span>Aaaaaaaaaaayou</span> 👍（31） 💬（2）<div>topK 是不是应该先要填满堆，后面插入的时候再做删除操作</div>2019-02-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/9b/a8/6a391c66.jpg" width="30px"><span>geraltlaush</span> 👍（28） 💬（10）<div>之前遇到qq的一个面试题，一个用户的登录了qq，如果五分钟内用户没有检测到心跳包，就需要用户重新登录，登录后重新计时五分钟，是不是也用高等计时器来管理海量用户的计时登录问题，把qq号和时间五分钟当成堆节点，有心跳来就调整堆，把五分钟规定时间剩下最少的用户放在堆顶，后台线程每次扫描的时候取堆顶元素，然后计算剩余的等待时间，等规定时间到了，就取堆顶元素，判断是否为0，如果为0，就需要重新登录，一直取堆顶，直到取到不为0的元素，然后计算剩余等待时间，线程休眠，等到了规定时间再来，老师你看这个方案可行不</div>2019-07-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/da/7f/8069035d.jpg" width="30px"><span>ZX</span> 👍（25） 💬（4）<div>看了这一章，发现堆删除任意元素这个方法毫无意义啊。只有删除堆顶元素才有意义</div>2018-12-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/07/8c/0d886dcc.jpg" width="30px"><span>蚂蚁内推+v</span> 👍（21） 💬（6）<div>数据是动态的，为什么不能用数组呢？我们维护一个size为k的有序数组（用快排，复杂度是klogk，插入法建堆也是klogk），然后每来一个元素，就把数组的第一个元素比较，如果大就插入，如果小就舍弃。插入使用二分，也是logn。没感觉堆有什么好处啊？老师</div>2019-08-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/08/c7/6b0cb046.jpg" width="30px"><span>小花小黑的铲屎官</span> 👍（14） 💬（1）<div>我们遍历这 10 亿个关键词，并且通过某个哈希算法对其求哈希值，然后哈希值同 10 取模，得到的结果就是这个搜索关键词应该被分到的文件编号。
这样并不能保证每个文件都是一亿条数据吧？可能多也可能少吧？</div>2018-12-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/f2/15/c5be3083.jpg" width="30px"><span>Allen</span> 👍（13） 💬（1）<div>高性能定时器，使用堆数据结构不一定是最优解，“环形队列”也许更好一点</div>2018-12-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/14/ee/d72a8222.jpg" width="30px"><span>攻城拔寨</span> 👍（11） 💬（1）<div>如果我要1%到99%响应时间，这样建的堆就有点多了</div>2018-12-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/69/10/275ae749.jpg" width="30px"><span>懒猫</span> 👍（5） 💬（2）<div>”查找前K大数据呢？我们可以维护一个大小为K的小顶堆，顺序遍历数组，从数组中取出取数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理，继续遍历数组。这样等数组中的数据都遍历完之后，堆中的数据就是前K大数据了。“，没人觉得这里有问题么，【如果比堆顶元素大，我们就把堆顶元素删除】，应该是如果比堆顶元素大，且堆已满的情况下才删除堆顶元素吧，否则只是比堆顶大，但堆未满，还是得插入。例如求top3，数组元素是5,6,2,4,1，按老师讲的就不是top3了</div>2019-03-08</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJGOSxM1GIHX9Y2JIe7vGQ87rK8xpo5F03KmiaGyXeKnozZsicHeSZrbSlzUVhTOdDlXCkTrcYNIVJg/132" width="30px"><span>ferry</span> 👍（5） 💬（5）<div>看完老师给出的三个应用后，我在想快速排序和堆排序时间复杂度相同，那么用快速排序替换堆排序可以吗？是考虑到堆排序时间复杂度的稳定性所以选择堆排序，还是因为别的原因呢？</div>2019-02-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/63/14/06eff9a4.jpg" width="30px"><span>Jerry银银</span> 👍（4） 💬（1）<div>早起的鸟儿读算法。

文章中『解答开篇』部分，说是扫描1亿个热门关键词，这应该是错别字吧，应该是10亿个吧。看了好几遍，我应该没理解错吧😄

老师说使用散列表统计10亿个搜索关键词的频率，但是这里的约束条件是10亿个关键词中确实有很多重复，而且去重之后的数据，内存中是能够放得下的。如果单机内存放不下，应该就不能这么做了

---------------------------------------------------------

以上是我早上本来要留言的，但是并没有一字不漏的看完文章。我回头一想不对，文章中肯定会考虑到这个情况。当我看完，我就把以上留言删了。

唉，阅读时，犯了一个低级错误，记录在此，提醒自己</div>2018-11-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/26/87/31c785a3.jpg" width="30px"><span>鱼星草</span> 👍（3） 💬（1）<div>求topK这块有问题，维护一个大小为k的小顶堆，但是你的描述中连第一个堆顶元素是什么都没有描述清楚，初始的K个元素到底是那几个？怎么计算？</div>2019-03-09</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIbas4S4X5W15njMeoEPPSyBZRX37nrTXbMFFeHghXl4Slk6WXE7oq5yxoNnukYfcOQs00RAvUmEA/132" width="30px"><span>Geek_5258f8</span> 👍（3） 💬（2）<div>topk用快排与小堆哪个更快？</div>2018-12-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/e0/75/1763638a.jpg" width="30px"><span>ECHO</span> 👍（3） 💬（1）<div>为了解决内存不够，而采用的“10 亿条搜索关键词先通过哈希算法分片到 10 个文件中” 这个思想是否类似 桶排序 的思想呢？ </div>2018-12-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/07/8c/0d886dcc.jpg" width="30px"><span>蚂蚁内推+v</span> 👍（3） 💬（4）<div>王老师  第一点合并有序小文件 为什么要用到优先级队列 和 堆还是不理解。两个比最小取出合并，只要两个数组是有序就可以了，快排成有序，从小到大比较合并，不可以吗，为什么要用到优先级队列，方便老师解答下吗</div>2018-11-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/f4/c5/39f2acfd.jpg" width="30px"><span>被吹落的风</span> 👍（2） 💬（1）<div>老师，我有一事不明，请赐教：“对这 10 亿个关键词分片之后，每个文件都只有 1 亿的关键词，去除掉重复的，可能就只有 1000 万个”，这里为啥要会去掉重复的呢，文件里确实真实存在着重复数据啊，每个文件大于1G。</div>2019-01-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/95/fd/5aa00179.jpg" width="30px"><span>王多鱼</span> 👍（1） 💬（1）<div>操作系统的定时器是不是也是使用堆来实现的？</div>2019-06-15</li><br/><li><img src="" width="30px"><span>Geek_596356</span> 👍（1） 💬（1）<div>如果使用散列表，出现散列冲突后，怎么办？再借助链表和红黑树吗？</div>2019-05-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/dc/b9/946b181d.jpg" width="30px"><span>好运连连</span> 👍（1） 💬（1）<div>老师，看得有点迷糊，topk问题，为什么不用快排呢？是因为动态数据这一原因吗？根本原因是什么呢？</div>2019-03-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/38/69/864569a4.jpg" width="30px"><span>devil</span> 👍（1） 💬（1）<div>求中位数这个，动态数据，数据会越来多。两个堆无限增长会把内存吃完OOM，有没有什么优化办法，一直没想到。</div>2019-03-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/e3/65/f3553a48.jpg" width="30px"><span>邵靖隆</span> 👍（1） 💬（2）<div>老师，我问一个关于排序的问题
如果在归并排序中，当切分出的子列长度小于10时不再对其继续递归归并排序，而改用插入排序。
这样的组合算法，其时间复杂度如何计算？</div>2018-11-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/4b/be/f8768be4.jpg" width="30px"><span>夏天</span> 👍（1） 💬（1）<div>感觉用堆能解决的问题,换成其他数据结构也能解决</div>2018-11-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/0c/c0/1abdd0ca.jpg" width="30px"><span>无悔</span> 👍（0） 💬（1）<div>老师您的github地址是多少，想看看您的实现</div>2019-11-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/5b/45/5dc5437e.jpg" width="30px"><span>Joiner</span> 👍（0） 💬（1）<div>老师，利用堆求 Top K 这个案例中，尽管文章中已经说明动态数据是动态的加入数据，但我还是想知道如果动态的删除数据，是不是还得检查删除的数据是否在堆中，那么有两点：
1.最坏情况需要把堆中数据都遍历一遍。
2.如果在，需要删除该数据后基于当前数据重新生成堆。
这两点我的理解有问题吗？</div>2019-10-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/b0/45/f0a63850.jpg" width="30px"><span>Pyer</span> 👍（0） 💬（1）<div>我有个问题，对这 10 亿个关键词分片之后，每个文件都只有 1 亿的关键词,去除掉重复的，可能就只有 1000 万个。
这个去重怎么去重?比如你一个文件有1亿个词，那么对这个文件去重话，会不会很耗内存，如果内存不够的话??</div>2019-10-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/a5/c6/5ead3889.jpg" width="30px"><span>李</span> 👍（0） 💬（1）<div>对这 10 亿个关键词分片之后，每个文件都只有 1 亿的关键词，去除掉重复的，可能就只有 1000 万个，每个关键词平均 50 个字节，所以总的大小就是 500MB。1GB 的内存完全可以放得下。
每个500m，一共10个，一共还是5g啊？好像1g也不够。是不是我哪里理解有误啊</div>2019-10-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/7b/41/85796e32.jpg" width="30px"><span>飞向云端</span> 👍（0） 💬（1）<div>上面用堆处理定时任务，如果同一时刻有很多任务需要处理，这个时候怎么办</div>2019-07-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/5f/30/4ae82e16.jpg" width="30px"><span>wordMan</span> 👍（0） 💬（1）<div>“对这 10 亿个关键词分片之后，每个文件都只有 1 亿的关键字，去除掉重复的，可能就只有 1000 万个”， 不太明白这里为什么要去掉重复的，如果重复的去掉了，后续怎么计算同一个关键字的总的搜索次数


</div>2019-05-26</li><br/>
</ul>