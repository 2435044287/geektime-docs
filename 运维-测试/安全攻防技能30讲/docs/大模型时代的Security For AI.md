你好，我是何为舟。

任何事物都有其两面性，大模型同样如此，既能够为安全能力的提升带来新的想象空间，大模型本身对于安全来说，又是新的攻击面，会引入新的安全风险。因此，在这一节中，我会尝试盘点大模型时代的Security For AI，讲解安全人员需要重点关注的方向。

大模型在快速发展的过程中，现阶段很多观点很有可能经不起时间的挑战，所以我仅希望通过这些内容抛砖引玉，启发你的思考。

## 常规AI攻击方式

Security For AI这个命题同样由来已久，最知名的应当是Deepfake带来的欺骗攻击案例。大模型在宏观的训练流程上，和传统AI并没有特别大的区别，都是准备数据、模型训练、模型应用这几个过程。因此，常规的AI攻击方式在大模型场景同样适用，这里简单盘点一下。

### 数据投毒

数据投毒简单来说，就是**想办法在训练数据集中注入恶意的数据，从而进一步影响模型训练的结果**。

这个攻击的原理应该很好理解，因为AI是不具备“常识”的，它只是呆板地学习训练数据中的规律，并进行呈现。如果训练过程中，你告诉AI说“1+1=3”，那么AI最终也会认为“1+1=3”。

但在传统AI训练中，训练数据通常是私密的，作为攻击者很难直接操控训练数据，攻击门槛很高，这也导致这一攻击场景很少被拿出来讨论。但大模型时代，训练数据通常是通过爬取公开数据得到的，这就导致针对大模型的数据投毒成本大幅度降低了。比如大模型存在的种族歧视问题，其实就是受到了爬取数据源的影响。