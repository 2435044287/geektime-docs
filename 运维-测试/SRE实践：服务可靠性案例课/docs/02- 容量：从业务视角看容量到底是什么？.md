你好，我是白园。从今天开始我们学习可靠性保障领域的第二个方面——容量。

2013年10月，百度举办了一场盛大的科技大会，会上宣布了一个重大消息：百度网盘的用户存储容量将从5GB大幅提升到2TB。这一时期，互联网行业迎来了“百盘大战”，各类网盘服务陆续出现，竞争非常激烈。在这个变革的浪潮中，百度网盘的流量激增20倍。面对这么大的流量压力，百度网盘没有出现任何故障。

这背后的关键因素就是**容量保障**。所以这节课我就带你详细了解容量保障体系，并教你如何有效地开展容量相关的工作。

## 容量的本质

首先，让我们以第一性原理来探讨这个问题，容量本质上是资源消耗与资源补充之间取得一个平衡。目标是**在确保系统可靠性的同时，尽可能地减少资源的投入**。你可以结合我给出的示意图来理解。

![图片](https://static001.geekbang.org/resource/image/fd/22/fd59c2691139d78897692e3f26ca3422.png?wh=2134x410)

### **容量为什么这么重要？**

容量管理对于系统稳定性的重要性，就像身体素质对人体健康的重要性一样，在生病的时候身体素质比较好的人往往恢复得更快，而身体素质比较差的人恢复缓慢，甚至可能引发其他基础疾病。

同样，如果服务的容量管理不善，就可能导致一系列问题，比如流量波动、热点事件以及变更操作都会引发资源冲突，从而导致故障。相反，如果容量管理得当，很多类似的问题都不会出现。所以一个好的容量管理是提升系统可靠性的基础。
<div><strong>精选留言（10）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/18/da/7b/21cb1287.jpg" width="30px"><span>轻喃</span> 👍（4） 💬（1）<div>思考题，根据经验猜测一下
cpu利用率不高，大概率问题应该出现在网络层面
如，timewait&#47;closewait过载，tcp半连接队列打满，Socket Buffer打满，文件描述符大满，nat conntrack转换表打满，磁盘io过载
再到应用层面，可能就是连接池，锁竞争之类的..
</div>2024-07-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/28/fe/fa12b300.jpg" width="30px"><span>Ryan</span> 👍（1） 💬（1）<div>一切消耗的资源都是容量，思考题里cpu没打满 只代表cpu资源容量ok，其他资源都有可能出现问题，比如 网络io、连接数、内存、磁盘io 等，网络又可以延伸为接口、中间价依赖问题，网络问题应在调用方设置合理超时时间并通过线程池把资源隔离好</div>2024-08-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/3b/ec/fc/90c25073.jpg" width="30px"><span>和力策</span> 👍（0） 💬（3）<div>这种情况一般先查网络，再查是否有第三方系统调用关系。曾经遇到一个第三方 ca 认证出问题导致业务局部中断的现象</div>2024-08-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/8e/02/abb7bfe3.jpg" width="30px"><span>张彦松</span> 👍（0） 💬（1）<div>看起来，就是被攻击了，请求失败了，根本没有走逻辑。超时可能是类似连接打满类的问题</div>2024-07-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/c6/20/124ae6d4.jpg" width="30px"><span>若水清菡</span> 👍（0） 💬（1）<div>思考题这个有可能是服务请求第三方依赖（比如数据库 redis 等）出现了延迟，导致服务整体失败率升高。</div>2024-07-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/9f/3f/346fc201.jpg" width="30px"><span>JeffreyBool</span> 👍（0） 💬（1）<div>图是用什么工具画的？</div>2024-07-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/3e/1e/0b/6976f5a3.jpg" width="30px"><span>会飞～的猪</span> 👍（0） 💬（0）<div>1.nginx性能瓶颈，转发超时了
2.数据库死锁、慢sql</div>2025-01-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/0c/86/8e52afb8.jpg" width="30px"><span>花花大脸猫</span> 👍（0） 💬（0）<div>思考题：
1. 接口层服务对外开放的连接数不足以支持当前的请求量，导致大量请求没有进入容器或者服务内部，
2. 当前业务的网络带宽打满了，得排队等待处理完才能进入
</div>2025-01-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/0f/d5/73ebd489.jpg" width="30px"><span>于加硕</span> 👍（0） 💬（0）<div>“在进行压测的时候，如果观察到流量达到某一阈值后，系统开始出现大量失败和延迟现象，但这个时候系统的 CPU 利用率却非常低，请你分析一下可能的原因”
1.从达到某一个阈值时开始出现失败和延迟，推测是应用服务触发了限流、熔断、降级措施(上下游影响，或自身)，导致该应用不走原来的计算逻辑，所以CPU利用率并不高。
2.一种特殊的应用服务，非计算密集型，或属于缓存密集型、流量密集型、IO密集型(后面三种在生产环境中存在，但量极少)，对该服务压测到一定程度后，首先出现瓶颈的资源非CPU资源，也会出现上述现象。一般调整硬件配置规格解决。</div>2024-12-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/34/b3/cf70e0f5.jpg" width="30px"><span>枕畔雪</span> 👍（0） 💬（0）<div>学习了～</div>2024-07-18</li><br/>
</ul>