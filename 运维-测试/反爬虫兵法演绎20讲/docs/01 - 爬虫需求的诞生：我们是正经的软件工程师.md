你好，我是DS Hunter，反爬虫专家。

也许你是一个爬虫工程师，也许是反爬虫工程师，甚至，也许你只是一个业务方的普通研发，被授予了反爬的重任。但是，不论你的身份是什么，“什么是爬虫”这个问题都是你必须要了解的。

为什么这么说呢？

可能你常把爬虫挂在嘴边，觉得自己已经很熟悉爬虫了，但当你尝试自己做一个爬虫或者完成一个反爬虫动作时，却发现无从下手。其实，很大的一个原因就是你对于“什么是爬虫”这个问题了解得并不透彻。

从历史的视角来了解爬虫从哪里来、能做什么，以及从诞生到现在的这段时间里都发生了什么样的变化，可以让你对“什么是爬虫”这个问题产生更深度的思考，这也是我要在课程里特地为你设置一个“历史背景篇”的主要原因。咱们接下来正式开始吧。

## **什么是爬虫？**

爬虫是一个历史悠久的需求，严格来说，它甚至比网络出现得还早。或者你也可以理解为，网络出现之后，网络和爬虫才结合成了我们所熟知的网络爬虫。因为互联网大部分的功能其实并没有什么新意，只不过是把线下的场景搬到线上来了。

而爬虫，其实就起源于线下。再聚焦一些，爬虫，起源于再平常不过的——菜市场。

### 买菜和爬虫？买菜也爬虫？

前几天我听了一首很喜欢的歌，叫《说走就走》。里面有一段话，说的是：“走世界，看精彩，从18岁讲到现在，最后到巷口去买菜。”
<div><strong>精选留言（14）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/12/65/c1/afcd981b.jpg" width="30px"><span>程序员二师兄</span> 👍（8） 💬（1）<div>本人刚接触爬虫这一块不久，看到思考题留的作业，按照自己对爬虫浅薄的理解回答一下：

1. 假如爬虫冒充搜索引擎，怎么办？

以百度搜索引擎的爬虫为例、爬虫会带有标识，如 baiduspider，可以初步判断为搜索引擎。

假如其他爬虫此时也加上了baiduspider的标识，那么可以根据robots.txt 协议来进行处理。

爬虫所抓取的链接在robots.txt协议中，进一步可以认为搜索引擎。

而往往其他爬虫不像搜索引擎，它是不遵守robots.txt协议的，它抓取的链接以及数据可能也不在约定的协议中，那么可以认为爬虫冒充了搜索引擎。

此时对这类爬虫进行拦截，识别到这类爬虫后，接口可以返回非正常数据，还见过虚假数据，让竞争对手拿到的是虚假数据。

2. 爬虫为这个世界做了什么贡献？

个人认为，爬虫对这个世界最大的贡献是数据的聚合。

没有爬虫之前，每个站点的数据都犹如一座孤岛，很难在众多孤岛找到所需要的数据，解决待满足的需求。

搜索引擎的爬虫很好的解决这个问题，只需要一个输入框，输入想问的问题，搜索引擎将爬虫抓取到的数据进行优化，将更相关的资料优先展示在网页上。

3. 你的爬虫或者反爬虫的经历是什么？有什么奇葩的经历吗？

爬虫经历：
一、
为了找到某些关键词在搜索引擎的需求以及权重。
将某一个关键词，通过爬虫的方式从各大搜索引擎获取前10条返回结果。
搜索引擎能够返回的数据，说明需求量是比较大的。

二、
通过爬虫抓取第三方数据平台，获取文章以及短视频的各方面的数据。
比如通过爬虫对短视频平台的视频去水印、视频文案提取。

反爬虫经历：

接口防刷。

简单介绍一下背景，所在的公司有电商业务，当品牌做一些活动时，参与人数会比较多，而其中有小部分人会利用爬虫来刷接口。

处理方法：
针对用户的请求及频率，如果是爬虫，频率会比较高，增加图形验证码，通过图形验证码才能后续的操作。

自己的奇葩经历：
自从了解一些爬虫知识后，看到有意思的网站或者app，总是忍不住想抓包看一下它们的接口。

经常魔怔，比如看到一些加密的请求，虽然不知道有什么意义，总是想研究一番，常常研究半天还是没能琢磨透。
</div>2022-01-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/25/4e/24/c491ac2b.jpg" width="30px"><span>ll</span> 👍（5） 💬（1）<div>我的经历：
1、 16年开始做爬虫，那个时候什么58、美团、淘宝什么的，数据都是免费爬，当时我们的一个目标就是怎么重复利用cpu，带宽，让我们的爬虫采集效率最快。那个时候淘宝的数据都是没有反爬的，我们的工作就是疯狂写爬虫，很少有反爬的，那个时候我记得我3个月写了快100跟网站的爬虫，所有爬虫一键爬起来的时候，那个壮观，现在想想都觉得我疯了；
2、后来发现有些小的障碍了，比如下一页的连接是js生成的，网站要开始限制cookie了，某些登录验证需要梳理他的js逻辑了，比如微博和百度贴吧，不过那个时候捋下逻辑，还是可以搞定的，从那个时候开始，爬虫的速度，就再也不是面试的考点了，都是问怎么安全、稳定；
3、 后来就发现了一些特别恶心的，比如请求的参数就一个很大的字符串，所有请求体的都是加密成一个字符串，验证header里也是加密的，每此请求header里的auth都是一次性的；完了js还没法逆向回去，或者说我没法逆向回去，你调试的时候还会定位到你，把你封ip，之前就被这么搞过，不过后来还是搞定了，我记得是瑞数科技的专门做的，都过去好多些年了，希望不要针对我；
4、再后来就越来越觉得，几乎每个网站都有反爬虫，但是也不是突破不了，然而突破了好像对我们来说也意义不大，因为有些硬性的指标，比如你的账户、跟ip绑定后，限制了你的行为，只能有那么多次的访问上限，基本上限制死了单个账号的数据访问量，爬虫已经不是一个人可以做的事情了，背后需要很多账号、ip这样的资源，有时候感觉就是财力的比拼；甚至后来发现天眼查充了会员后，同样的接口，没充钱的数据你拿到是假数据，还需要研究他的js再处理一下，而会员就可以爬到真数据，我发现后震惊了，立马冲了个会员，工作量一下就降低了不少，才感觉到别人产品经理已经把挣钱放到我们爬虫开发人员身上了，再后来越来越发现，爬虫已经告别了西部牛仔--一个人闯荡的、单靠技术就能过得不错的时代了，以后的数据也会越来越难获取，爬虫也不再是一个人的武林了；
5、 逐渐疏远爬虫，一想到破解后维护也是个大问题，就没有动力；

现在想想，奇葩的经历，肯定要算天眼查要挣我们爬虫开发人员的钱，我是被震惊了</div>2022-02-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/db/d2/e29f8834.jpg" width="30px"><span>lidashuang</span> 👍（3） 💬（2）<div>爬过最难爬的是美团，各种给你下毒</div>2022-01-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/09/c2/4e086a4b.jpg" width="30px"><span>demo123567</span> 👍（2） 💬（1）<div>第一个问题应该是user-agent，虽然可以模拟；然后大型搜索引擎爬虫一般都是有固定的IP段，所以应该也可以识别</div>2022-01-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/28/a1/fd2bfc25.jpg" width="30px"><span>fsc2016</span> 👍（2） 💬（1）<div>感觉现在爬虫对抗，慢慢从web转战到移动端了</div>2022-01-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg" width="30px"><span>peter</span> 👍（2） 💬（1）<div>写得太好啦，牛啊，文采真好，通顺，有趣，也没有错别字！！！！  我还有很多课程没有看，这个课程和自己目前的关系并不是很大，犹豫再三才买的。真庆幸自己没有错过这么好的文章。理工男能写这么好，不容易，不多见啊。</div>2022-01-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/23/bb/a1a61f7c.jpg" width="30px"><span>GAC·DU</span> 👍（2） 💬（1）<div>一次经历见证了一家公司技术的成长，甚至把后端由15人加到了30。从携cookie能登录到手机验证码再到扫脸登录，api加了token，后来把限流和熔断也加上了。</div>2022-01-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/04/0a/07a48224.jpg" width="30px"><span>圆桌π</span> 👍（1） 💬（1）<div>没接触过爬虫，觉得新奇，买课来看看。
第一次听说爬虫，是数据库老师说找数据，然后又半开玩笑的说最好不要。极客时间App的一门法律课里也有提及。

期待课程老师，继续加油！💪</div>2022-02-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/eb/19/0d990b03.jpg" width="30px"><span>ZeroIce</span> 👍（1） 💬（1）<div>虽然说作者没有说出真实名字，但说话风格特别像一个大佬（上一篇文章有那个大佬评论：你是个有故事的人？）</div>2022-02-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/f6/58/7458ac2e.jpg" width="30px"><span>Blue</span> 👍（1） 💬（1）<div>2. 爬虫从技术的角度上来讲，我觉得一方面提升了一些专业人员获取信息的效率，我们可以脱离浏览器，通过爬虫程序来获取我们期望得到的数据（不影响服务性能且不违规违法的前提下），这样就有更多的时间与精力去专注于解决更难更有意义的问题，这也是我当初做爬虫的一个初衷；另一方面我认为爬虫与反爬是存在良性竞争的，互相博弈可以提升各自的技术能力与认知边界，同时也让服务提供方有的放矢地设计出更具容错性，安全性的系统。</div>2022-01-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/34/df/64e3d533.jpg" width="30px"><span>leslie</span> 👍（1） 💬（1）<div>爬虫其实是变形为其中的内容再推广，这就像搜索引擎；同样像作者所说；爬取对被爬者的服务器带宽造成了很大压力，持续消耗对方资源这个就有点恶意攻击了。</div>2022-01-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/28/63/d7/efa4bfd1.jpg" width="30px"><span>默默</span> 👍（0） 💬（1）<div>加密的爬虫改怎么破解？该怎么分析js代码等？</div>2022-01-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/25/4e/24/c491ac2b.jpg" width="30px"><span>ll</span> 👍（11） 💬（0）<div>我的经历：
1、 16年开始做爬虫，那个时候什么58、美团、淘宝什么的，数据都是免费爬，当时我们的一个目标就是怎么重复利用cpu，带宽，让我们的爬虫采集效率最快。那个时候淘宝的数据都是没有反爬的，我们的工作就是疯狂写爬虫，很少有反爬的，那个时候我记得我3个月写了快100跟网站的爬虫，所有爬虫一键爬起来的时候，那个壮观，现在想想都觉得我疯了；
2、后来发现有些小的障碍了，比如下一页的连接是js生成的，网站要开始限制cookie了，某些登录验证需要梳理他的js逻辑了，比如微博和百度贴吧，不过那个时候捋下逻辑，还是可以搞定的，从那个时候开始，爬虫的速度，就再也不是面试的考点了，都是问怎么安全、稳定；
3、 后来就发现了一些特别恶心的，比如请求的参数就一个很大的字符串，所有请求体的都是加密成一个字符串，验证header里也是加密的，每此请求header里的auth都是一次性的；完了js还没法逆向回去，或者说我没法逆向回去，你调试的时候还会定位到你，把你封ip，之前就被这么搞过，不过后来还是搞定了，我记得是瑞数科技的专门做的，都过去好多些年了，希望不要针对我；
4、再后来就越来越觉得，几乎每个网站都有反爬虫，但是也不是突破不了，然而突破了好像对我们来说也意义不大，因为有些硬性的指标，比如你的账户、跟ip绑定后，限制了你的行为，只能有那么多次的访问上限，基本上限制死了单个账号的数据访问量，爬虫已经不是一个人可以做的事情了，背后需要很多账号、ip这样的资源，有时候感觉就是财力的比拼；甚至后来发现天眼查充了会员后，同样的接口，没充钱的数据你拿到是假数据，还需要研究他的js再处理一下，而会员就可以爬到真数据，我发现后震惊了，立马冲了个会员，工作量一下就降低了不少，才感觉到别人产品经理已经把挣钱放到我们爬虫开发人员身上了，再后来越来越发现，爬虫已经告别了西部牛仔--一个人闯荡的、单靠技术就能过得不错的时代了，以后的数据也会越来越难获取，爬虫也不再是一个人的武林了；
5、 逐渐疏远爬虫，一想到破解后维护也是个大问题，就没有动力；

现在想想，奇葩的经历，肯定要算天眼查要挣我们爬虫开发人员的钱，我是被震惊了</div>2022-02-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/28/63/d7/efa4bfd1.jpg" width="30px"><span>默默</span> 👍（0） 💬（0）<div>加密的爬虫随机header相关的，该怎么去破解爬虫呢？</div>2022-01-23</li><br/>
</ul>