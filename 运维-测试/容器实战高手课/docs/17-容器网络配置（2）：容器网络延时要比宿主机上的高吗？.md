你好，我是程远。

在上一讲里，我们学习了在容器中的网络接口配置，重点讲解的是veth的接口配置方式，这也是绝大部分容器用的缺省的网络配置方式。

不过呢，从veth的这种网络接口配置上看，一个数据包要从容器里发送到宿主机外，需要先从容器里的eth0 (veth\_container) 把包发送到宿主机上veth\_host，然后再在宿主机上通过nat或者路由的方式，经过宿主机上的eth0向外发送。

![](https://static001.geekbang.org/resource/image/2f/be/2fba246fb4aaa6315661a11996fa04be.jpg?wh=3200%2A1800)

这种容器向外发送数据包的路径，相比宿主机上直接向外发送数据包的路径，很明显要多了一次接口层的发送和接收。尽管veth是虚拟网络接口，在软件上还是会增加一些开销。

如果我们的应用程序对网络性能有很高的要求，特别是之前运行在物理机器上，现在迁移到容器上的，如果网络配置采用veth方式，就会出现网络延时增加的现象。

那今天我们就来聊一聊，容器网络接口对于容器中应用程序网络延时有怎样的影响，还有这个问题应该怎么解决。

## 问题重现

对于这种veth接口配置导致网络延时增加的现象，我们可以通过运行[netperf](https://hewlettpackard.github.io/netperf/)（Netperf是一个衡量网络性能的工具，它可以提供单向吞吐量和端到端延迟的测试）来模拟一下。

这里我们需要两台虚拟机或者物理机，这两台机器需要同处于一个二层的网络中。
<div><strong>精选留言（10）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/5e/96/a03175bc.jpg" width="30px"><span>莫名</span> 👍（9） 💬（3）<div>netif_rx 通常在硬件中断处理逻辑中调用。loopback、veth 等设备驱动是特例，通过调用 netif_rx 模拟数据包的接收。

除了 softirq 的性能损坏，应该还包括 docker0 网桥的自身处理逻辑（作为 veth_container 的主设备接管其数据包的处理权）以及 docker0 -&gt; eth0 的转发逻辑。

docker inspect 输出为 json 格式，推荐使用 jq 命令查看 Pid ，命令比较简洁，缺点是默认末安装：
docker inspect lat-test-1 | jq .[0].State.Pid</div>2020-12-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1a/8b/65/0f1f9a10.jpg" width="30px"><span>小Y</span> 👍（0） 💬（3）<div>晕，我有个问题。 我发现 我只有一台物理&#47;云机器，我在其上启动了netserver，然后启动了测试容器lat-test-1。使用netperf，我在容器中请求机器ip测试和 在 机器上请求自己IP测试，情况 后者的延时反而更大了。。。 这是为什么呢？</div>2022-03-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1a/a2/b5/c68d9439.jpg" width="30px"><span>豆豆</span> 👍（15） 💬（1）<div>思考题，因为他们使用不同的网络名称空间，MACVLAN 会走单独的协议栈，iptables 规则是在主机的网络名称空间，所以不会生效的，除非单独给容器的网络名称空间配置iptables 规则</div>2020-12-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/4c/49/125f5adc.jpg" width="30px"><span>mazhen</span> 👍（11） 💬（0）<div>Kubernetes的service是通过在iptables中设置DNAT规则实现的，而 DNAT 规则是在 PREROUTING 检查点，也就是在路由之前。 
而ipvlan&#47;macvlan 网络接口直接挂载在物理网络接口上，发送函数会直接找到虚拟接口对应的物理网络接口，不会再经过iptables的DNAT 规则。
</div>2022-01-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/48/39/4e95e7b9.jpg" width="30px"><span>morse</span> 👍（8） 💬（1）<div>Kubernetes 的 service 是靠 Kube-proxy实现, 

L2模式下, 出入流量不会经过 host namespace, 那么kube-proxy就无法工作.

L3模式下, 单入方向不经过 host namespace. 无法支持Kube-proxy.
</div>2021-01-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/23/81/60/71ed6ac7.jpg" width="30px"><span>谢哈哈</span> 👍（5） 💬（2）<div>在iptables标记包之前就被宿主机上的路由层面给处理完了，在mangle表和nat表的PREROUTING 和POSTROUTING链上的规则就匹配不到了</div>2020-12-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/a6/f4/a9f2f104.jpg" width="30px"><span>黑鹰</span> 👍（2） 💬（0）<div>Kubernetes 中的 Service 是通过 kube-proxy 为 Service 配置的 iptables dnat 转发规则，对后端的 Pod 做负载均衡。ipvlan&#47;macvlan 的网络接口和容器不在同一个 namespace，无法为容器配置转发到 ipvlan&#47;macvlan 设备的 iptables 规则。</div>2023-02-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/85/25/53765783.jpg" width="30px"><span>Ari</span> 👍（0） 💬（0）<div>iptables工作在3层网络以上，ipvlan工作在2层网络，因此iptables无法拦截2层的数据包从而做更多的转发处理</div>2024-07-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c4/03/f753fda7.jpg" width="30px"><span>JianXu</span> 👍（0） 💬（0）<div>不能使用service 不是使得Kubernetes 很受限制了吗？有什么两全其美的办法吗？</div>2022-08-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/42/9d/c36b7ef7.jpg" width="30px"><span>顾骨</span> 👍（0） 💬（3）<div>老师，请教一个问题，配置IPVLAN后，宿主机和容器网络貌似是不通的，这个怎么打通宿主机和容器的网络呢</div>2022-03-30</li><br/>
</ul>