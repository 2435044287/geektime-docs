你好，我是倪朋飞。

上一讲，我带你一起梳理了 eBPF 的安全能力，并教你使用 eBPF 分析和阻止了容器进程的安全问题。在开篇词中我就提到，eBPF 的主要应用场景涵盖了故障诊断、性能监控、安全控制以及网络优化等。从 [07 讲](https://time.geekbang.org/column/article/484207) 进入实战进阶篇开始，我已经为你介绍了应用于前三个场景中的内核跟踪、用户态跟踪、网络跟踪以及安全控制等。那么，对于最后一个场景，网络性能优化，eBPF 是如何发挥作用的呢？

今天，我就以最常用的负载均衡器为例，带你一起来看看如何借助 eBPF 来优化网络的性能。

## Nginx 负载均衡器

既然要优化负载均衡器的网络性能，那么首先就需要有一个优化的目标，即初始版的负载均衡器。在今天的案例中，我们使用最常用的反向代理和 Web 服务器 Nginx 作为初始版的负载均衡器，同时也使用自定义的 Nginx 作为后端的 Web 服务器。

为了方便环境的重现，负载均衡器、 Web 服务器以及客户端都运行在容器中，它们的 IP 和 MAC 等基本信息如下图所示：

![图片](https://static001.geekbang.org/resource/image/e9/15/e923026f577f7b991be2610734f9e415.jpg?wh=1920x1706)

参考 Nginx 官方文档中 [HTTP 负载均衡](https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/)的配置方法，你可以通过以下几步来搭建上述的案例环境。

1）执行下面的命令，创建上图中的4个容器：

```bash
# Webserver (响应是hostname，如 http1 或 http2)
docker run -itd --name=http1 --hostname=http1 feisky/webserver
docker run -itd --name=http2 --hostname=http2 feisky/webserver

# Client
docker run -itd --name=client alpine

# Nginx
docker run -itd --name=nginx nginx
```
<div><strong>精选留言（13）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/5e/96/a03175bc.jpg" width="30px"><span>莫名</span> 👍（7） 💬（4）<div>key:
ac 11 00 05 ac 11 00 01 00 00 00 50 00 00 be 12
02 00 00 00

以该 key 为例，struct sock_key 共 5 个 u32 类型的字段，每个字段四个字节，其中：

ac 11 00 05  -&gt;    sip，网络字节序，源 IP 172.17.0.5，对应负载均衡器容器 IP
ac 11 00 01   -&gt;    dip, 网络字节序，目的 IP 172.17.0.1，对应 Docker 网桥 docker0 IP 地址，即本机所有容器的默认网关。
00 00 00 50 -&gt; sport, 网络字节序，源端口 80。
00 00 be 12  -&gt; dport, 网络字节序，目的端口 48658。
02 00 00 00 -&gt; family，主机字节序，AF_INET 协议，具体定义为： #define AF_INET  2	  &#47;* Internet IP Protocol *&#47;

对 SKB 类型的 BPF 程序了解不多，这里想请教下倪老师，目的 IP (172.17.0.1) 为什么是 Docker 网桥的 IP ？跟 sk_msg 转发逻辑有关么？我理解正常情况下目的 IP 应该是客户端容器 IP。</div>2022-02-11</li><br/><li><img src="" width="30px"><span>Geek9635</span> 👍（3） 💬（2）<div>在实际应用sockmap时，遇到一个性能问题
我的业务模型是同一个pod内，部署nginx及一个java写的api-proxy程序，两者之间通过127.0.0.1通信（长连接）
想使用sockmap能力在进程之间加速，实际测试用wrk打流（通过域名）到pod的nginx，发现使用sockmap并没有带来性能的提升（从sar看pod内lo口流量是0）
抓取sock_sendmsg到sock_recvmsg之间的时间分布见后：
nginx是多进程单线程的，那个java程序是单进程多线程的（抓取的时间分布是两者之间的send-&gt;revc时间差，第一列是耗时区间，第二列是次数）

从耗时分布情况看，sockmap并没用明显优势（业务cpu已基本打满）
但如果简单用netperf tcp_rr测试，sockmap基本耗时分布在8-16usecs，无socketmap基本在16-32usecs

想请教老师，这是sockmap本身的性能问题（实现sock之间报文转发的流程也挺复杂的），还是使用方式方法不对？
这种复杂模型的性能分析也非常难，老师是否有毕竟好的方法，谢谢

@normal(usecs): 				
[4, 8)                37 |
[8, 16)           131639 |
[16, 32)         2206394 |
[32, 64)          796467 |
[64, 128)         511059 |
[128, 256)        378776 |
[256, 512)        294895 |
[512, 1K)         140723 |
[1K, 2K)           26766 |
[2K, 4K)           10600 |
[4K, 8K)            8053 |
[8K, 16K)           2607 |
[16K, 32K)             6 |

@ebpf sockmap(usecs): 
[4, 8)             20150 |
[8, 16)          1046770 |
[16, 32)         1597489 |
[32, 64)          574600 |
[64, 128)         425788 |
[128, 256)        369249 |
[256, 512)        327745 |
[512, 1K)         187195 |
[1K, 2K)           32752 |
[2K, 4K)            9646 |
[4K, 8K)            7825 |
[8K, 16K)           2140 |
[16K, 32K)           187 |
</div>2022-03-17</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKlibz5085KSjlsAqSrYKiboLvzm3TYibVAMHjbCz6ibpcUWMa2AOUctyvVRSuVe3HP7YwYcwzwYmh5GQ/132" width="30px"><span>maosd199554</span> 👍（2） 💬（3）<div>老师，这种只适合LB跟服务部署同一个服务器吧？</div>2022-02-12</li><br/><li><img src="" width="30px"><span>Geek9635</span> 👍（1） 💬（2）<div>有人能复现老师的测试结果么？
我在centos8.3上测试，性能没有太大变化（抓包确认sockmap功能生效）

nsenter -t 201559 -n sar -n DEV 1
Average:        IFACE   rxpck&#47;s   txpck&#47;s    rxkB&#47;s    txkB&#47;s   rxcmp&#47;s   txcmp&#47;s  rxmcst&#47;s   %ifutil
Average:           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
Average:         eth0  20805.67  20663.00   1595.32   2001.95      0.00      0.00      0.00      0.00

测试过程中，nginx cpu占用非常高
Tasks: 645 total,  12 running, 633 sleeping,   0 stopped,   0 zombie
%Cpu(s):  1.9 us, 31.2 sy,  0.0 ni, 65.2 id,  0.2 wa,  0.3 hi,  1.2 si,  0.0 st
MiB Mem : 385191.7 total, 336110.5 free,   9242.4 used,  39838.9 buff&#47;cache
MiB Swap:      0.0 total,      0.0 free,      0.0 used. 361619.3 avail Mem

   PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND                                                                            
373492 101       20   0    9388   3504   2076 R  97.1   0.0   4:40.45 nginx                                                                              
373493 101       20   0    9388   3508   2076 R  97.1   0.0   4:14.85 nginx                                                                              
373495 101       20   0    9388   3496   2076 R  97.1   0.0   6:18.71 nginx                                                                              
373496 101       20   0    9388   3476   2076 R  97.1   0.0   5:07.61 nginx                                                                              
373498 101       20   0    9388   3504   2076 R  97.1   0.0   5:28.36 nginx                                                                              
373501 101       20   0    9388   3480   2076 R  97.1   0.0   6:23.90 nginx                                                                             
373503 101       20   0    9388   3476   2076 R  97.1   0.0   5:37.27 nginx                                                                           
</div>2022-03-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg" width="30px"><span>piboye</span> 👍（1） 💬（2）<div>要开启cgroup v2，grub修改重启才搞定😅</div>2022-02-23</li><br/><li><img src="" width="30px"><span>从远方过来</span> 👍（0） 💬（1）<div>老师，本示例直接将msg从一个sock转发到另一个sock, 那是不是说跳过了内核处理?</div>2022-03-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/8a/b0/6ab66f1b.jpg" width="30px"><span>牛金霖</span> 👍（0） 💬（1）<div>请教一个问题，在哪儿能看到各种各类型的BPF程序的作用和生效条件。

还有就是各种MAP的适用场景呢？</div>2022-02-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/cd/db/7467ad23.jpg" width="30px"><span>Bachue Zhou</span> 👍（1） 💬（2）<div>有两个问题请教下老师：
1. 为什么 skops-&gt;local_port 这个字段就要转成网络序，skops-&gt;remote_port 就不需要？
2. 我发现在启用了这两个 BPF 程序后，如果 nginx 容器被关闭，wrk 依然无法正常工作，那 nginx 容器在 BPF 程序内扮演了什么角色？</div>2023-04-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2b/56/ac/a047a944.jpg" width="30px"><span>李清泉🍻</span> 👍（0） 💬（0）<div>老师，我还是没看懂，怎么重定向的时候用的是msg-&gt;local_port作为要重定向的socket的目标呢？我们应该不知道对方要发到哪个IP port吧？</div>2024-05-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2b/56/ac/a047a944.jpg" width="30px"><span>李清泉🍻</span> 👍（0） 💬（0）<div>老师，能出视频么，我基础不好，看得不太懂</div>2024-03-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/55/d8/610fae56.jpg" width="30px"><span>～～</span> 👍（0） 💬（0）<div>请问这里挂载到&#47;sys&#47;fs&#47;cgroup是不是只对容器进程生效, 如果我用shell打开一个进程, 是不是不能生效. 我发现ops程序只能观测到容器的网络操作</div>2023-09-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/2f/7d/a2f23b0d.jpg" width="30px"><span>fqh</span> 👍（0） 💬（0）<div>所以这个case里面是 把 socket bpf 挂载到了 宿主层面的 套接字层面？ 

为什么不像后面xdp case 那样，是把 socket bpf 放到独立的一个容器内部？</div>2023-06-16</li><br/><li><img src="" width="30px"><span>Geek_6aca30</span> 👍（0） 💬（0）<div>为什么这里的操作没像下篇一样影响到系统其它网络连接呢？看代码是对所有新建的连接都进行了转发。</div>2022-04-19</li><br/>
</ul>