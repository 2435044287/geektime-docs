你好，我是倪朋飞。

上一节，我用一个 Nginx+PHP 的案例，给你讲了服务器 CPU 使用率高的分析和应对方法。这里你一定要记得，当碰到无法解释的 CPU 使用率问题时，先要检查一下是不是短时应用在捣鬼。

短时应用的运行时间比较短，很难在 top 或者 ps 这类展示系统概要和进程快照的工具中发现，你需要使用记录事件的工具来配合诊断，比如 execsnoop 或者 perf top。

这些思路你不用刻意去背，多练习几次，多在操作中思考，你便能灵活运用。

另外，我们还讲到 CPU 使用率的类型。除了上一节提到的用户 CPU 之外，它还包括系统 CPU（比如上下文切换）、等待 I/O 的 CPU（比如等待磁盘的响应）以及中断 CPU（包括软中断和硬中断）等。

我们已经在上下文切换的文章中，一起分析了系统 CPU 使用率高的问题，剩下的等待 I/O 的 CPU 使用率（以下简称为 iowait）升高，也是最常见的一个服务器性能问题。今天我们就来看一个多进程I/O的案例，并分析这种情况。

## 进程状态

当 iowait 升高时，进程很可能因为得不到硬件的响应，而长时间处于不可中断状态。从 ps 或者 top 命令的输出中，你可以发现它们都处于 D 状态，也就是不可中断状态（Uninterruptible Sleep）。既然说到了进程的状态，进程有哪些状态你还记得吗？我们先来回顾一下。
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/10/3f/57/a014199a.jpg" width="30px"><span>书林</span> 👍（78） 💬（3）<div>每个人的机器配置不一样，所以会出现有的机器iowait不明显，有的机器被打爆。解决办法是用docker cgroup选项对 block io做限制。假设硬盘设备为 &#47;dev&#47;nvme0n1，测试如下：
1. 限制块设备的读写 iops 为 3: `docker run --privileged --name=app9 --device &#47;dev&#47;nvme0n1:&#47;dev&#47;nvme0n1 --device-write-iops &#47;dev&#47;nvme0n1:3 --device-read-iops &#47;dev&#47;nvme0n1:3  -itd feisky&#47;app:iowait-new2`
2. 可以查看host机器 cgroup 已为对应 docker container 添加了相关限制：
```
cat &#47;sys&#47;fs&#47;cgroup&#47;blkio&#47;docker&#47;&quot;docker-container-id&quot;&#47;blkio.throttle.write_iops_device
259:0 3
cat &#47;sys&#47;fs&#47;cgroup&#47;blkio&#47;docker&#47;&quot;docker-container-id&quot;&#47;blkio.throttle.read_iops_device
259:0 3
```
3.  
```
docker exec -it  &quot;docker-container-id&quot; &#47;bin&#47;bash
root@4cc5e6c74cc0:&#47;# dd iflag=direct if=&#47;dev&#47;nvme0n1 of=&#47;dev&#47;null bs=1k count=1000
1000+0 records in
1000+0 records out
1024000 bytes (1.0 MB, 1000 KiB) copied, 340.004 s, 3.0 kB&#47;s
```
`dd` 每次从 &#47;dev&#47;nvme0n1 设备读取数据写到 &#47;dev&#47;null 设备，每次读取 1kB，一共1000次，必须为 direct 选项。可以观测到拷贝速度为 3 kB&#47;s，即 1kB * 3，说明cgroup 限制 `blkio.throttle.read_iops_device` 生效。

4. 观察host机器 iowait 已经上去。
```
top - 12:10:22 up  1:25,  1 user,  load average: 0.88, 0.81, 0.83
任务: 780 total,   1 running, 227 sleeping,   0 stopped, 552 zombie
%Cpu0  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 s
%Cpu1  :  2.7 us,  0.0 sy,  0.0 ni, 97.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 s
%Cpu2  :  0.0 us,  0.0 sy,  0.0 ni,  0.0 id,100.0 wa,  0.0 hi,  0.0 si,  0.0 s
%Cpu3  :  5.3 us,  7.9 sy,  0.0 ni, 84.2 id,  0.0 wa,  0.0 hi,  2.6 si,  0.0 s
MiB Mem :   7863.3 total,    230.4 free,   3847.2 used,   3785.8 buff&#47;cache
MiB Swap:   8192.0 total,   8191.5 free,      0.5 used.   3191.1 avail Mem 
```
zombie数那么高是因为这个 docker container 已经运行20多分钟了。

供大家参考:)</div>2018-12-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/13/52/db1b01fc.jpg" width="30px"><span>白华</span> 👍（73） 💬（4）<div>老师以后的案例能不能使用centos7系统进行操作？做的很多实验和你的都会有部分偏差，这次偏差更大，相信学习你课程的大部分都是用虚拟机跑的项目，用centos系统使用率会很高，而且实际生产中用centos系统肯定大于Ubuntu，造成的实验偏差会不会也是系统的原因。我也遇到了没有出现D状态的进程，出现了大量Z进程。平均负载并没有提升，反而是下降了。iowait并没有变化。所以恳请您使用centos系统来教学吧</div>2018-12-05</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/ib8rSTG2Kln7m4M10qpb6ehPaWsA3dsib5OBsMDyol1QuwS6JiaBFJ6a2omytoS4QvLCHIw291IzBYrmj3W1gdNmA/132" width="30px"><span>丁兆鹏</span> 👍（34） 💬（8）<div>centos7 中模拟一下一起docker中无法启动app
 docker ps -a
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES
54a43bfd9ddb        feisky&#47;app:iowait   &quot;&#47;app&quot;              7 seconds ago       Exited (1) 6 seconds ago                       app

docker logs app也为空。

既然是c程序，使用gdb调试，发现在select_disk()

	const char *sd_prefix = &quot;sd&quot;;
	const char *xvd_prefix = &quot;xvd&quot;;
...
		if (strncmp(sd_prefix, entry-&gt;d_name, 2) == 0 || strncmp(xvd_prefix, entry-&gt;d_name, 3) == 0)

看看机器上磁盘格式如下：
df -h
Filesystem      Size  Used Avail Use% Mounted on
&#47;dev&#47;vda1        99G   16G   79G  17% &#47;
&#47;dev&#47;vdb1        99G  5.5G   88G   6% &#47;data1

找到原因了,磁盘前缀不同，无法找到的盘，修改app.c代码
        const char *sd_prefix = &quot;vd&quot;;
        const char *xvd_prefix = &quot;vdb&quot;;

然后就可以正常启动了。
docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
ceb6eec8129a        feisky&#47;app:iowait   &quot;&#47;app&quot;              2 seconds ago       Up 1 second                             app</div>2018-12-17</li><br/><li><img src="https://wx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIPs72cLhiaib6m4jO15AVuYicMs8ZQxm8nuxA4Ml3Sic1W81ROJK7Pa3Fj56wGX6gstzDQkDyuyKW0aA/0" width="30px"><span>黄涛</span> 👍（13） 💬（3）<div>我是centOS，也是无法启动，参考评论中的方式，修改启动参数为：
docker run --privileged --name=app -itd feisky&#47;app:iowait &#47;app -d &#47;dev&#47;vdb1
就可以了</div>2019-06-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/58/25/2088864b.jpg" width="30px"><span>林贻民</span> 👍（13） 💬（5）<div>老师您好，我有个疑惑：不可中断状态睡眠不能被其他进程或者中断打断，那照道理说如果不可中断进程很多（超过CPU）逻辑核数时，系统应该处于卡死状态，可是实际上并不是这样的，其他进程照样在执行，说明是存在进程调度，也就是存在重调度中断，键盘输入也有响应，说明也存在硬件中断，这个似乎和不可中断进程状态进程的不可中断产生了冲突？是我有什么地方理解错了吗？</div>2019-03-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/9a/f5/71eee10b.jpg" width="30px"><span>深蓝</span> 👍（5） 💬（1）<div>同问，关于Uninterruptible sleep(D)状态
的进程如何有效的处理，以前运维的时候遇到过，貌似只能重启机器，不知道还有什么更好的办法</div>2018-12-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/12/04/5837b21c.jpg" width="30px"><span>Brown羊羊</span> 👍（5） 💬（3）<div>没有模拟出来系统I&#47;O瓶颈，可以帮忙看下吗：
容器运行起来后只发现一个app进程
[root@liyang2 ~]# ps aux|grep &#47;app
root     23619  0.0  0.0   4368   380 pts&#47;0    Ss+  17:12   0:00 &#47;app
root     23777  0.0  0.0 112648   952 pts&#47;0    S+   17:12   0:00 grep --color=auto &#47;app

CPU情况  wa也没有很高
%Cpu(s):  1.0 us,  1.5 sy,  0.0 ni, 94.0 id,  3.3 wa,  0.0 hi,  0.2 si,  0.0 st

系统：redhat7  3.10.0-327.el7.x86_64 x86_64 GNU&#47;Linux  </div>2018-12-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/1b/82/69581d8a.jpg" width="30px"><span>姜小鱼</span> 👍（4） 💬（2）<div>这个案例的iowait比较高，但是并不影响cpu使用率。因为准确来说，iowait也是属于cpu idle状态的一部分，他和僵尸进程影响的只是平均负载和系统资源</div>2018-12-05</li><br/><li><img src="" width="30px"><span>Xg huang</span> 👍（3） 💬（1）<div>请教一下老师，一般在写代码访问io的时候，是不可中断还是可中断？</div>2019-01-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/e8/45/c58cb283.jpg" width="30px"><span>帆帆帆帆帆帆帆帆</span> 👍（3） 💬（1）<div>有一个问题没有想明白：
平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数，和 CPU 使用率并没有直接关系。比如不可中断状态的进程多了（IO繁忙），平均负载会升高，但此时cpu使用率并不高。但问题是CPU使用率=1-空闲时间&#47;总CPU时间，IO繁忙，那进程iowait就应该很高，此时CPU使用率也应该很高呀。和前面的分析不一致呢。求解答。
</div>2018-12-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/dd/a2/8277208d.jpg" width="30px"><span>超</span> 👍（2） 💬（1）<div>近日刚碰到一套生产系统出问题，我们逻辑cpu是120的实体机，跑oracle数据库，业务刚接进来就卡死了，一分钟平均负载200多，运行队列也两百多，cpu的usr部分四五十，sys部分四十左右，iowait基本是零，idle个位数，top和pidstat都无法输出，perftop也无法，只能dstat。请老师指导下思路，在查到底什么问题。</div>2018-12-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/62/d7/5bfdecf2.jpg" width="30px"><span>chen泽林¹⁵⁶²⁶¹⁸⁷³³⁹</span> 👍（1） 💬（1）<div>安装dstat后出现except getopt.error, exc。
dstat工具支持python2 ，可能系统安装了python3。 
如果安装了python2和3的话，可以用
使用 &#47;usr&#47;bin&#47;python2 &#47;usr&#47;bin&#47;dstat</div>2019-06-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/41/af/4307867a.jpg" width="30px"><span>JJj</span> 👍（1） 💬（1）<div>老师，你好，iowait是不是可以认为有IO性能问题，io的操作需要系统调用吧，比如write，read，对应sys CPU是不是会高？</div>2019-03-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/65/ae/9727318e.jpg" width="30px"><span>Boy-struggle</span> 👍（1） 💬（1）<div>R状态是不可屏蔽状态嘛？长时间关闭中断会有什么问题？</div>2019-03-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/93/43/0e84492d.jpg" width="30px"><span>Maxwell</span> 👍（1） 💬（1）<div>为什么用top命令能查看到app的进程为Z状态，但是ps aux|grep &#47;app ,状态却不是Z状态呢？
top输出：
109341 root      20   0       0      0      0 Z   9.8  0.0   0:00.30 app                         
109342 root      20   0       0      0      0 Z   8.9  0.0   0:00.27 app 
使用ps aux|grep &#47;app:
root     108152  0.0  0.0   4512  1600 pts&#47;0    Ss+  16:22   0:00 &#47;app
root     109364  0.0  0.0  21536  1048 pts&#47;1    S+   17:06   0:00 grep --color=auto &#47;app</div>2019-02-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/4c/c8/bed1e08a.jpg" width="30px"><span>辣椒</span> 👍（1） 💬（1）<div>评论里是高手辈出啊，两个问题都是看评论区的评论都解决了</div>2019-01-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/0f/53/92a50f01.jpg" width="30px"><span>徐洲更</span> 👍（1） 💬（1）<div>关于不可中断的IO负荷导致服务器性能提高，这两天刚好有了一个实战机会

https:&#47;&#47;www.jianshu.com&#47;p&#47;d7edbff7dda6</div>2018-12-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/ec/1a/ef9d89ef.jpg" width="30px"><span>龙</span> 👍（1） 💬（2）<div>docker run --privileged --name=app --device &#47;dev&#47;sda5:&#47;dev&#47;sda5 --device-write-iops &#47;dev&#47;sda5:3 --device-read-iops &#47;dev&#47;sda5:3 -itd feisky&#47;app:iowait
报错：
   &#47;usr&#47;bin&#47;docker-current: Error response from daemon: oci runtime error: container_linux.go:247: starting container process caused &quot;process_linux.go:327: setting cgroup config for procHooks process caused \&quot;failed to write 8:5 3 to blkio.throttle.read_iops_device: write &#47;sys&#47;fs&#47;cgroup&#47;blkio&#47;system.slice&#47;docker-92e10bae59a6520ef090c08f6718396497edcdc4abaace7d820afb566ac5e41a.scope&#47;blkio.throttle.read_iops_device: invalid argument\&quot;&quot;.
谁知道这是啥原因吗？</div>2018-12-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/3d/ad/bee0df03.jpg" width="30px"><span>wing</span> 👍（1） 💬（1）<div>centos7,docker 镜像无法正常启动。
feisky&#47;app:iowait     &quot;&#47;app&quot;                   7 minutes ago       Exited (1) 7 minutes ago </div>2018-12-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/3b/c6/5954cbb7.jpg" width="30px"><span>#</span> 👍（1） 💬（1）<div>#man top    
   20. S  --  Process Status
           The status of the task which can be one of:
               D = uninterruptible sleep
               R = running
               S = sleeping
               T = stopped by job control signal
               t = stopped by debugger during trace
               Z = zombie
#man ps
PROCESS STATE CODES
       Here are the different values that the s, stat and state output specifiers (header &quot;STAT&quot; or &quot;S&quot;) will display to describe the state of a process:

               D    uninterruptible sleep (usually IO)
               R    running or runnable (on run queue)
               S    interruptible sleep (waiting for an event to complete)
               T    stopped by job control signal
               t    stopped by debugger during the tracing
               W    paging (not valid since the 2.6.xx kernel)
               X    dead (should never be seen)
               Z    defunct (&quot;zombie&quot;) process, terminated but not reaped by its parent

       For BSD formats and when the stat keyword is used, additional characters may be displayed:

               &lt;    high-priority (not nice to other users)
               N    low-priority (nice to other users)
               L    has pages locked into memory (for real-time and custom IO)
               s    is a session leader
               l    is multi-threaded (using CLONE_THREAD, like NPTL pthreads do)
               +    is in the foreground process group
在ubuntu和centos都没看到idel状态，是内核还是版本问题呢？</div>2018-12-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/64/05/6989dce6.jpg" width="30px"><span>我来也</span> 👍（1） 💬（1）<div>[D7打卡]
系统环境: ubuntu 18.04虚拟机 4cpu 6G内存.
启动docker后观测到的数据如下:
# ps aux | grep &#47;app
root      2134  0.0  0.0   4376  1044 pts&#47;0    Ss+  14:07   0:00 &#47;app
# ps aux | grep app
root      2134  0.0  0.0   4376  1044 pts&#47;0    Ss+  14:07   0:00 &#47;app
root      2189  0.0  0.0      0     0 pts&#47;0    Z+   14:07   0:00 [app] &lt;defunct&gt;
...
# top -d1
top - 14:39:41 up  2:46,  5 users,  load average: 0.03, 0.07, 0.26
任务: 1065 total,   1 running, 231 sleeping,   0 stopped, 781 zombie
%Cpu(s):  0.0 us,  3.0 sy,  0.0 ni, 97.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem :  6098388 total,  2850004 free,  1182712 used,  2065672 buff&#47;cache
#  pidstat -wut -p 2134 1
 cswch&#47;s nvcswch&#47;s 切换次数极低,几乎为0.
# vmstat 1 11
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b 交换 空闲 缓冲 缓存   si   so    bi    bo   in   cs us sy id wa st
 1  0      0 2852420 203804 1861316    0    0     0     0  333  588  0  1 99  0  0
同上
同上
 0  0      0 2851176 203804 1861412    0    0 131072     0  598  923  1  4 94  2  0
 0  0      0 2850644 203804 1861444    0    0     0     0  438  759  1  1 98  0  0
同上
同上
同上
 0  0      0 2849548 203812 1861388    0    0 131072     0  502  827  0  3 96  1  0
[几乎每5秒有一次bi升高, 且伴随着top中zombie增加2个.]
# pstree | grep app
        |-dockerd-+-docker-containe-+-docker-containe-+-app---1082*[app]
[再次说明僵尸进程是app产生的]
# perf top -g -p 2134
-   96.67%     0.00%  app      app                [.] main
   - main
      - 86.67% read
           entry_SYSCALL_64_after_hwframe
           do_syscall_64
           sys_read
           vfs_read
           __vfs_read
           new_sync_read
           blkdev_read_iter
           generic_file_read_iter
         - blkdev_direct_IO
            - 72.00% bio_iov_iter_get_pages
                 iov_iter_get_pages
               + get_user_pages_fast
[samples数不多 可能参考意义不大]
# docker exec -ti app &#47;bin&#47;bash
-rwxr-xr-x    1 root root 13040 Oct 10 07:06 app*
[只有可执行程序]
个人总结:虽然知道是app进程导致的,且几乎以固定频率5s多出2个僵尸进程.各项指标也算正常,并未有iowait升高的状况.并不知道进一步该如何分析了</div>2018-12-05</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/PiajxSqBRaEL5xnfuicbtRz4F87AAjZX6oCEjMtYiaIu4iaQichQmy0vEBA6Sumic1RDvUCeuBEqj6iatnt2kENbKYmuw/132" width="30px"><span>dexter</span> 👍（1） 💬（1）<div>我虚拟机是reht6.5，这几天用yum安装docker，一致找不到docker包（其他程序可以yum），不知道docker安装是在哪个库里面，这2天的实验还没有做</div>2018-12-05</li><br/><li><img src="" width="30px"><span>无名老卒</span> 👍（0） 💬（1）<div>将读写的iops都限制在3：
```bash
root@fdm:~# docker run --privileged --name app2 --device &#47;dev&#47;sda:&#47;dev&#47;sda --device-write-iops &#47;dev&#47;sda:3 --device-read-iops &#47;dev&#47;sda:3 -itd feisky&#47;app:iowait 
c7befa77604a55ac41d31ef6328f4664d544fe4559f5609217c309eae188ffc5
```

查看top，负载已经上去了，同时iowait也在99%了。

```
top - 14:21:06 up 9 days, 12:36,  9 users,  load average: 148.06, 80.83, 34.35
Tasks: 360 total,   1 running, 283 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.0 us,  0.5 sy,  0.0 ni,  0.0 id, 99.5 wa,  0.0 hi,  0.0 si,  0.0 st
```
使用pidstat是可以看到是app 这个程序导致的

```
root@fdm:~# pidstat -d 5 5
...

02:21:01 PM   UID       PID   kB_rd&#47;s   kB_wr&#47;s kB_ccwr&#47;s iodelay  Command
02:21:06 PM     0     74213  13081.04      0.00      0.00   36966  app
02:21:06 PM     0     74214  13081.04      0.00      0.00   37121  app

root@fdm:~# iostat -d -x 5 5 sda

Device            r&#47;s     w&#47;s     rkB&#47;s     wkB&#47;s   rrqm&#47;s   wrqm&#47;s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
sda              5.39    0.00   3679.04      0.00     0.00     0.00   0.00   0.00    1.78    0.00   0.00   682.67     0.00   0.89   0.48

Device            r&#47;s     w&#47;s     rkB&#47;s     wkB&#47;s   rrqm&#47;s   wrqm&#47;s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
sda              5.40    0.00   3686.40      0.00     0.00     0.00   0.00   0.00    0.89    0.00   0.00   682.67     0.00   0.44   0.24

Device            r&#47;s     w&#47;s     rkB&#47;s     wkB&#47;s   rrqm&#47;s   wrqm&#47;s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
sda              6.00    0.00   4096.00      0.00     0.00     0.00   0.00   0.00    3.07    0.00   0.01   682.67     0.00   1.07   0.64
```
但是使用了限制IO时，就发现了一些问题：一是运行了很久，但是僵尸进程没有涨一个，但是负载就上去了；二是看负载上去的原因是wa的占用率上去了，但使用iostat看磁盘使用率还是插正常的；三是为什么限制了容器使用的io量，但还是导致宿主机的负载升高这么多，这是为什么？请老师指点一下。</div>2019-04-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/e1/21/5f0bf9a0.jpg" width="30px"><span>阿文</span> 👍（0） 💬（1）<div>root@linux:~# docker ps -a
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES
73e18b629c09        feisky&#47;app:iowait   &quot;&#47;app&quot;              7 seconds ago       Exited (1) 5 seconds ago                       app</div>2019-03-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/0b/34/f41d73a4.jpg" width="30px"><span>王盛武</span> 👍（0） 💬（1）<div>倪老师，请问上一节实验里的docker镜像有没下载仓库或者下载地址？谢谢</div>2019-03-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/65/ae/9727318e.jpg" width="30px"><span>Boy-struggle</span> 👍（0） 💬（1）<div>这里的IO指的是硬盘io还是cpu总线IO?</div>2019-03-05</li><br/><li><img src="" width="30px"><span>元天夫</span> 👍（0） 💬（1）<div>补卡，回头看有一个好处就是可以看看大家的留言，评论中也可以受益匪浅啊</div>2019-02-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/93/43/0e84492d.jpg" width="30px"><span>Maxwell</span> 👍（0） 💬（1）<div>老师，有三个问题：
1.运行ps aux|grep &#47;app，并没有出现D状态的进程，我的是：
root     108152  0.0  0.0   4512  1600 pts&#47;0    Ss+  16:22   0:00 &#47;app
root     108938  0.0  0.0  21536  1076 pts&#47;1    S+   16:51   0:00 grep --color=auto &#47;app
我的硬件配置CPU为四核(E2 1230 V2,逻辑四核)，应该比你的配置略高，是否是硬件方面的问题

2.运行案例中，使用vmstat 查看并没有发现iowait升高，是否和我的磁盘有关?(我使用的是SATA SSD，低端固态)
3.top中查看的wait应该不是iowait 吧，这个和vmstat 中的wait应该不是一个意思吧？</div>2019-02-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/98/12/a169bdcd.jpg" width="30px"><span>Geek_477c02</span> 👍（0） 💬（1）<div>遇到有大量的D状态的进程，导致负载到7000多，但是cpu和iowait都不高，除了重启设备还有什么办法解决？</div>2019-02-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/48/d7/8b69bf21.jpg" width="30px"><span>请叫我小强</span> 👍（0） 💬（1）<div>不知道有没有同学碰到这个问题：在使用 make build 的时候出现无法认证的错误。
gcc -o app app.c
docker build -t feisky&#47;app:iowait -f Dockerfile .
Sending build context to Docker daemon 30.72 kB
Step 1&#47;4 : FROM ubuntu
Get https:&#47;&#47;registry-1.docker.io&#47;v2&#47;: x509: certificate signed by unknown authority
make: *** [build] Error 1

这个问题可以尝试在 Dockerfile 文件中把 FROM ubuntu 修改成 FROM daocloud.io&#47;ubuntu 来解决。

</div>2019-01-09</li><br/>
</ul>