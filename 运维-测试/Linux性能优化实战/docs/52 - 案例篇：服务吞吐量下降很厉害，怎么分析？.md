你好，我是倪朋飞。

上一节，我们一起学习了怎么使用动态追踪来观察应用程序和内核的行为。先简单来回顾一下。

所谓动态追踪，就是在系统或者应用程序还在正常运行的时候，通过内核中提供的探针，来动态追踪它们的行为，从而辅助排查出性能问题的瓶颈。

使用动态追踪，便可以在不修改代码也不重启服务的情况下，动态了解应用程序或者内核的行为。这对排查线上的问题、特别是不容易重现的问题尤其有效。

在 Linux 系统中，常见的动态追踪方法包括 ftrace、perf、eBPF/BCC 以及 SystemTap 等。

- 使用 perf 配合火焰图寻找热点函数，是一个比较通用的性能定位方法，在很多场景中都可以使用。
- 如果这仍满足不了你的要求，那么在新版的内核中，eBPF 和 BCC 是最灵活的动态追踪方法。
- 而在旧版本内核，特别是在 RHEL 系统中，由于 eBPF 支持受限，SystemTap 和 ftrace 往往是更好的选择。

在 [网络请求延迟变大](https://time.geekbang.org/column/article/82833) 的案例中，我带你一起分析了一个网络请求延迟增大的问题。当时我们分析知道，那是由于服务器端开启 TCP 的 Nagle 算法，而客户端却开启了延迟确认所导致的。

其实，除了延迟问题外，网络请求的吞吐量下降，是另一个常见的性能问题。那么，针对这种吞吐量下降问题，我们又该如何进行分析呢？
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKsz8j0bAayjSne9iakvjzUmvUdxWEbsM9iasQ74spGFayIgbSE232sH2LOWmaKtx1WqAFDiaYgVPwIQ/132" width="30px"><span>2xshu</span> 👍（17） 💬（2）<div>老师，有个疑问。
套接字优化部分，你用ss -s输出的两个队列，根据“关于 Linux 网络，你必须知道这些（下）”你讲的内容，当链接处于listening状态是，Send-Q和Recv-Q都是半链接队列，但是你这里却都是调的全连接队列啊？不是应该调整tcp_max_syn_backlog吗？</div>2019-03-25</li><br/><li><img src="https://wx.qlogo.cn/mmopen/vi_32/PiajxSqBRaEKQMM4m7NHuicr55aRiblTSEWIYe0QqbpyHweaoAbG7j2v7UUElqqeP3Ihrm3UfDPDRb1Hv8LvPwXqA/132" width="30px"><span>ninuxer</span> 👍（6） 💬（3）<div>打卡day55
缺乏由现象联想到可能原因的系统性思维～</div>2019-03-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/11/4b/fa64f061.jpg" width="30px"><span>xfan</span> 👍（4） 💬（2）<div>内核选项 tcp_tw_reuse，不是直接修改内核参数就好了么，为什么还有修改后的tag:3 ,这里不太清楚</div>2019-03-28</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKCmqW21Zguv8kPiayib4U42B3jLGk2Y4Leia0fQjnU0Lfgic8BwbdMIePiayDadFKzV9kSt3F8jRicZxxA/132" width="30px"><span>泡泡</span> 👍（4） 💬（1）<div>wrk命令-c参数用来模拟连接数为1000，
为什么输出中的连接数有1910，不理解</div>2019-03-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/93/43/0e84492d.jpg" width="30px"><span>Maxwell</span> 👍（4） 💬（1）<div>在公司局域网下做性能测试，如何判断网络会不会成为压测的瓶颈呢？也就是说如果开了500线程进行压测，会不会因为网络瓶颈，导致请求无法发送到服务器端？</div>2019-03-25</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTL7cOHiaxJBPvdic53UfP4VC2P4EsyYOGNEwhgYsrP4kw7MFhI7fKQ0GnIiadIHUEYVD85AkrcGv5DLg/132" width="30px"><span>burner</span> 👍（1） 💬（1）<div>老师，系统cpu只用了一半，但是就出现502和499的请求错误，是否意味这应用服务已经过载，还是系统连接数过载，查看netstat发现有28万失败的连接尝试，</div>2019-08-13</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTL9hlAIKQ1sGDu16oWLOHyCSicr18XibygQSMLMjuDvKk73deDlH9aMphFsj41WYJh121aniaqBLiaMNg/132" width="30px"><span>腾达</span> 👍（1） 💬（2）<div>net.ipv4.tcp_tw_reuse = 1 这里是影响到socket的客户端（nginx作为一个客户端连接php的服务端）的行为吗？ 不是影响到服务端的time_wait数量？ 我弄了个tomcat，用ab压测，tw_use=1, 用ss -s看time_wait 还很高啊，1万多。</div>2019-04-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/df/43/1aa8708a.jpg" width="30px"><span>子杨</span> 👍（0） 💬（2）<div>服务端端口号不是近似无限的吗，这里怎么又有限制了。</div>2020-07-22</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTL9hlAIKQ1sGDu16oWLOHyCSicr18XibygQSMLMjuDvKk73deDlH9aMphFsj41WYJh121aniaqBLiaMNg/132" width="30px"><span>腾达</span> 👍（0） 💬（1）<div>老师，针对我提的问题，您的回复是：“不过你可以docker exec到容器内部查看”，我已经逐一对比过容器内的、我已知的参数了。未发现不同。您能否把最后一次的配置参数上传一下到github？</div>2019-04-08</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTL9hlAIKQ1sGDu16oWLOHyCSicr18XibygQSMLMjuDvKk73deDlH9aMphFsj41WYJh121aniaqBLiaMNg/132" width="30px"><span>腾达</span> 👍（0） 💬（1）<div>有2个问题：
1、在做perf，制作火焰图的部分，我自己本地看到的函数热点是类似：inet_sendmsg, tcp_write_xmit, e1000_xmit_frame 之类的，后续再对内核参数net.ipv4.tcp_tw_reuse做设置为1的处理后，函数热点依然是这几个。似乎我的机器上的热点是在发送数据，而不是在端口重用？
2、老师最后1个步骤的镜像，即：
$ docker run --name nginx --network host --privileged -itd feisky&#47;nginx-tp:3
$ docker run --name phpfpm --network host --privileged -itd feisky&#47;php-fpm-tp:3
这2个的配置能上传一下到github吗？我自己依照优化步骤修改的参数，放到镜像里去跑，压测后Requests&#47;sec只能达到： 1919，而是用老师的tag=3的镜像，压测后得到Requests&#47;sec是3107。我把我已知的参数都对比了一遍，如下：
sysctl  net.ipv4.ip_local_port_range=&#39;10000 65535&#39;
sysctl  net.ipv4.tcp_tw_reuse=1
sysctl  net.ipv4.tcp_fin_timeout=3
sysctl  net.ipv4.tcp_max_syn_backlog=8192
sysctl  net.netfilter.nf_conntrack_max=1048576
sysctl  net.core.somaxconn=65536
还有nginx、php的backlog=8192，php的max_children=40(我给了40，不是老师的20)。
发现都是一样的。不知道哪里有问题。
老师，你能把优化最后的配置文件上传一份到github吗？</div>2019-04-05</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTL9hlAIKQ1sGDu16oWLOHyCSicr18XibygQSMLMjuDvKk73deDlH9aMphFsj41WYJh121aniaqBLiaMNg/132" width="30px"><span>腾达</span> 👍（0） 💬（1）<div>有3个问题：
1.第一次运行 docker run --name nginx --network host --privileged -itd feisky&#47;nginx-tp这个命令，我参考的是对应的github.com&#47;linux-perf-examples&#47;nginx-throughput&#47; 下的一些文件知道了参数配置，问题：nginx里的init.sh运行的时候，sysctl修改的网络参数是作用在docker内的nginx？还是作用在宿主ubuntu上？
2.文章里查看、修改网络参数，并未提示说要进docker容器内部去修改，这个是在宿主ubuntu上做的修改吗？比如说到 sysctl -w net.netfilter.nf_conntrack_max=1048576 这个，是直接在宿主ubuntu上修改的吗？
3.从docker运行后续集个tag=2，3的镜像开始，例如：docker run --name nginx --network host --privileged -itd feisky&#47;nginx-tp:3 这个命令拉的镜像的参数修改了哪些地方？从哪里可以看到这些参数？</div>2019-04-01</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/8OPzdpDraQMvCNWAicicDt54sDaIYJZicBLfMyibXVs4V0ZibEdkZlbzxxL7aGpRoeyvibag5LaAaaGKSdwYQMY2hUrQ/132" width="30px"><span>code2</span> 👍（0） 💬（1）<div>用桌面linux分析服务器性能，有些勉强。</div>2019-03-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/93/43/0e84492d.jpg" width="30px"><span>Maxwell</span> 👍（0） 💬（1）<div>Sar测试的只是网络的发送和接收数据吧，好像并不能发现网络的瓶颈导致压测请求无法发送至服务器端？</div>2019-03-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/93/43/0e84492d.jpg" width="30px"><span>Maxwell</span> 👍（0） 💬（1）<div>这个火焰图咋分析？我这边和你的好像不太一样，系统我也是ubantu18.04
</div>2019-03-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/93/43/0e84492d.jpg" width="30px"><span>Maxwell</span> 👍（0） 💬（1）<div>运行最后一次docker镜像，wrk测试结果中还是有很多error(read ),请问这个error指的是什么错误呢？
Running 10s test @ http:&#47;&#47;192.168.32.145
  2 threads and 1000 connections
  Thread Stats   Avg      Stdev     Max   +&#47;- Stdev
    Latency    97.36ms   22.29ms 394.39ms   90.57%
    Req&#47;Sec     5.01k     1.00k    7.01k    75.00%
  Latency Distribution
     50%   91.10ms
     75%  100.83ms
     90%  117.53ms
     99%  195.90ms
  99832 requests in 10.08s, 20.72MB read
  Socket errors: connect 0, read 11483, write 0, timeout 0
Requests&#47;sec:   9902.41
Transfer&#47;sec:      2.05MB
</div>2019-03-26</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKsz8j0bAayjSne9iakvjzUmvUdxWEbsM9iasQ74spGFayIgbSE232sH2LOWmaKtx1WqAFDiaYgVPwIQ/132" width="30px"><span>2xshu</span> 👍（0） 💬（1）<div>老师，怎么观察全链接的状况哇？ss -s？</div>2019-03-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/9b/a8/6a391c66.jpg" width="30px"><span>geraltlaush</span> 👍（0） 💬（1）<div>老师,我使用root@ubuntu:~&#47;FlameGraph# perf script -i ~&#47;perf.data | .&#47;stackcollapse-perf.pl --all
报出Failed to open &#47;usr&#47;bin&#47;containerd-shim, continuing without symbols错误，在网上搜了下也没找到正确答案，这是什么情况</div>2019-03-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/64/05/6989dce6.jpg" width="30px"><span>我来也</span> 👍（36） 💬（0）<div>[D52打卡]
哈哈,看专栏的同时,也在生产环境中执行下查看套接字的命令.
居然还发现了一个高并发时的隐患.
`ss -ltnp`
  有些监听端口半链接队列的值偏小,只有32.而有些都是128.
  赶紧查看程序源码,发现是调用框架的接口时未传递backlog的值,导致使用框架默认的值32.
  哈哈,这个程序是专门处理客户端连接的,虽然目前Recv-Q都为0,但也确实是一个隐患吧.
`ss -s` 
  看到的连接数和各状态数还正常.
`netstat -s | grep -wE &quot;socket|listen&quot;`
  结果还比较稳定.
像 sysctl 相关的配置,由于没有权限,就无缘查看了.

不得不说,综合案例篇的内容真不错.老师也是够狠,设置了这么多的坑.填了一个一个又一个.
1.内核连接数限制 nf_conntrack.
2.php程序的工作进程数量
3.半链接队列偏小,导致高并发时的丢包.
4.系统分配的临时端口号范围.
5.系统的端口复用参数配置.

调优过程中,有时甚至是一波未平一波又起,很是惊险刺激.

</div>2019-03-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/62/81/ad80f427.jpg" width="30px"><span>Lane</span> 👍（3） 💬（0）<div>一天看2篇，终于追上进度了</div>2019-03-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/e0/c5/c324a7de.jpg" width="30px"><span>jorin@zou</span> 👍（1） 💬（0）<div>这个案例很强</div>2020-06-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/57/6e/b6795c44.jpg" width="30px"><span>夜空中最亮的星</span> 👍（1） 💬（0）<div>报个到</div>2019-03-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/d8/1e/f495cac7.jpg" width="30px"><span>krain</span> 👍（0） 💬（0）<div>nf_conntrack: table full这个报错应该不是因为是docker起的服务造成的吧，我理解host模式应该不走nat</div>2024-10-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/28/23/18/4284361f.jpg" width="30px"><span>易飞</span> 👍（0） 💬（0）<div>思路很绝</div>2024-04-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/62/dc/8876c73b.jpg" width="30px"><span>moooofly</span> 👍（0） 💬（0）<div>虽然这么“曲折”的案例是刻意创造出来的，不过作为示例还是不错的~</div>2022-11-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/4c/f1/8dc266ee.jpg" width="30px"><span>儿戏</span> 👍（0） 💬（0）<div>老师，请问下 nginx 用wrk 进行压测的时候报[error] 7696#0: *3730171 no live upstreams while connecting to upstream这个错误，用您教的方法都已经优化过了，还是没找到这个问题原因，nginx 连接后端的服务还是Nginx，为了保证后端服务没问题才用nginx。 针对这个问题，我应该如何排除呢？</div>2020-10-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/09/56/2628852c.jpg" width="30px"><span>星之所在</span> 👍（0） 💬（0）<div>老师我的一台8G内存的虚拟机这个nf_conntrack_max最大值能调整多大，太大了貌似也没用对吧</div>2020-04-27</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erKFNFAQl3ibwlic54a5SQYAMhQYeVtMnSmMahZZjyqG2d66whxbEE3I3IyD07pSmte5DSibr71m6A9g/132" width="30px"><span>初音未来</span> 👍（0） 💬（0）<div>老师问一个关于docker的问题，修改docker内的应用配置后必须保存新的镜像，然后再启动新镜像吗，这个的话生产环境怎么办</div>2020-04-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/4b/08/52954cd7.jpg" width="30px"><span>丁乐洪</span> 👍（0） 💬（0）<div>~最近碰到个问题，Rest接口访问返回404，是不是可以用动态跟踪来排查</div>2020-03-28</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJGMphabeneYRlxs1biaO9oKic6Dwgbe312561lE56V93uUHgXXAsGmK1pH18mvpElygoJh8SUtQPUA/132" width="30px"><span>董皋</span> 👍（0） 💬（0）<div>打卡</div>2020-03-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/f8/4b/5ae62b10.jpg" width="30px"><span>Geek_b04b12</span> 👍（0） 💬（1）<div>我的ubuntu系统为什么没有nf_conntrack_count 等一些参数，提示找不到。。</div>2020-02-16</li><br/>
</ul>