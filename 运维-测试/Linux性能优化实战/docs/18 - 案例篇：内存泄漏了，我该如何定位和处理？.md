你好，我是倪朋飞。

通过前几节对内存基础的学习，我相信你对 Linux 内存的工作原理，已经有了初步了解。

对普通进程来说，能看到的其实是内核提供的虚拟内存，这些虚拟内存还需要通过页表，由系统映射为物理内存。

当进程通过 malloc() 申请虚拟内存后，系统并不会立即为其分配物理内存，而是在首次访问时，才通过缺页异常陷入内核中分配内存。

为了协调 CPU 与磁盘间的性能差异，Linux 还会使用 Cache 和 Buffer ，分别把文件和磁盘读写的数据缓存到内存中。

对应用程序来说，动态内存的分配和回收，是既核心又复杂的一个逻辑功能模块。管理内存的过程中，也很容易发生各种各样的“事故”，比如，

- 没正确回收分配后的内存，导致了泄漏。
- 访问的是已分配内存边界外的地址，导致程序异常退出，等等。

今天我就带你来看看，内存泄漏到底是怎么发生的，以及发生内存泄漏之后该如何排查和定位。

说起内存泄漏，这就要先从内存的分配和回收说起了。

## 内存的分配和回收

先回顾一下，你还记得应用程序中，都有哪些方法来分配内存吗？用完后，又该怎么释放还给系统呢？

前面讲进程的内存空间时，我曾经提到过，用户空间内存包括多个不同的内存段，比如只读段、数据段、堆、栈以及文件映射段等。这些内存段正是应用程序使用内存的基本方式。
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="" width="30px"><span>Scott</span> 👍（62） 💬（3）<div>我比较关心老版本的Linux怎么做同样的事，毕竟没有办法升级公司服务器的内核。</div>2019-01-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/58/78/fe19274b.jpg" width="30px"><span>睡在床板下</span> 👍（41） 💬（9）<div>谈谈自己生产环境运行3个月的内存泄露经验吧：
现象：服务程序运行90天，监控系统告警内存达到阈值，内存泄漏800M。现实：生产环境、难复现。

- 保存core文件。 统计top10 大小块内存分配百分比

- 发现20字节大小内存申请了 3700w次，大概700M

- 通过工具搜索已有符号文件中大小为20字节的结构体、类，但是可能包含第三方库、组件没有符号文件，导致分析遇阻，未果

- 通过随机抽查20字节内存地址内容，希望找到有效信息，但几乎都是 0x00 0x10 0x00 ， 没字符串，猜不出什么内容，未果

- 通过3700w次数申请，平均每小时17000次左右。 通过完善的日志系统，分析1w~3w量级的消息，大概4个，review代码，问题解决

- 问题定位总共花费了4个小时左右。分析内存泄漏工具、方法很多，但是我觉的更重要的是完善的监控系统和日志系统。

</div>2020-07-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/93/43/0e84492d.jpg" width="30px"><span>Maxwell</span> 👍（23） 💬（6）<div>如果是java应用程序，也可以用这个方法定位么？</div>2019-01-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/99/27/47aa9dea.jpg" width="30px"><span>阿卡牛</span> 👍（12） 💬（4）<div>老师，你这个例子是已经知道哪个进程有内存泄露了，请问如何找出哪个进程呢？</div>2019-01-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/1b/82/69581d8a.jpg" width="30px"><span>姜小鱼</span> 👍（5） 💬（1）<div>老师，memleak只能检测用户程序的内存泄漏吧？如果检测内核态谋和模块内存泄漏呢，Kmemleak能否讲一下呢？</div>2019-05-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/54/98/52ca7053.jpg" width="30px"><span>Vicky🐣🐣🐣</span> 👍（3） 💬（4）<div>1. 如果执行&#47;usr&#47;share&#47;bcc&#47;tools&#47;memleak -a -p [pid] 就会报错Exception: Failed to attach BPF to uprobe
但是执行&#47;usr&#47;share&#47;bcc&#47;tools&#47;memleak -a，就不会报错，但是里面并没有和app相关函数
2. free观察情况如下，新机器，并没有任何其他高占用内存的进程，很是奇怪
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 1  0      0 218244 112044 463144    0    0    21    31  148  344  1  0 99  0  0
 0  0      0 218268 112044 463144    0    0     0     0  168  402  0  1 99  0  0
 0  0      0 217928 112044 463144    0    0     0     7  182  427  1  0 99  0  0
 0  0      0 217836 112044 463228    0    0    23     0  317  730  1  1 98  0  0
 0  0      0 217836 112048 463236    0    0     0    17  186  437  1  0 99  0  0
 0  0      0 215804 112052 463232    0    0     0    19  202  476  1  1 98  0  0
 0  0      0 215860 112052 463240    0    0     0     5  221  490  1  1 99  0  0
 0  0      0 217040 112056 463244    0    0     0    15  207  481  1  0 99  0  0
 0  0      0 217040 112056 463244    0    0     0     0  156  363  0  0 100  0  0
 0  0      0  76976 112056 463296    0    0    24    12  221  546 11  3 86  0  0
 0  0      0  77008 112060 463316    0    0     0    11  178  407  1  1 98  0  0
 0  0      0  75140 112060 463324    0    0     0    27  176  812  2  3 95  0  0
 0  0      0  74584 112060 463328    0    0     0     7  174  819  1  1 98  0  0
 0  0      0  74616 112060 463332    0    0     0     0  183  417  0  0 99  0  0
 0  0      0 216884 112060 463332    0    0     0    83  176  403  1  0 98  1  0
 0  0      0 216884 112064 463328    0    0     0     9  180  448  0  1 99  0  0
 0  0      0 217012 112064 463336    0    0     0     4  193  452  0  1 99  0  0
</div>2019-02-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/54/98/52ca7053.jpg" width="30px"><span>Vicky🐣🐣🐣</span> 👍（3） 💬（5）<div>老师，很多同学都问这个问题了，麻烦解答一下吧
ubuntu 4.15.0-29
# &#47;usr&#47;share&#47;bcc&#47;tools&#47;memleak -a -p 21642
Attaching to pid 21642, Ctrl+C to quit.
perf_event_open(&#47;sys&#47;kernel&#47;debug&#47;tracing&#47;events&#47;uprobes&#47;p__lib_x86_64_linux_gnu_libc_2_27_so_0x97070_21642_bcc_21882&#47;id): Input&#47;output error
Traceback (most recent call last):
  File &quot;&#47;usr&#47;share&#47;bcc&#47;tools&#47;memleak&quot;, line 416, in &lt;module&gt;
    attach_probes(&quot;malloc&quot;)
  File &quot;&#47;usr&#47;share&#47;bcc&#47;tools&#47;memleak&quot;, line 406, in attach_probes
    pid=pid)
  File &quot;&#47;usr&#47;lib&#47;python2.7&#47;dist-packages&#47;bcc&#47;__init__.py&quot;, line 989, in attach_uprobe
    raise Exception(&quot;Failed to attach BPF to uprobe&quot;)
Exception: Failed to attach BPF to uprobe
</div>2019-02-23</li><br/><li><img src="" width="30px"><span>Geek_kur7vg</span> 👍（2） 💬（1）<div>老师，”这一次，我们终于看到了内存分配的调用栈，原来是 fibona...“这里如何看到是这函数未释放呢？输出可否解释下呢</div>2019-04-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/5e/c6/78e515ee.jpg" width="30px"><span>唯安格</span> 👍（2） 💬（1）<div>老师，我运行：$ &#47;usr&#47;share&#47;bcc&#47;tools&#47;memleak -a -p $(pidof app) 并没有看到内存泄漏的问题。之后还看了app的源码。源码内的确没有调用free()函数。请问这可能是什么情况？
root@ubuntu:&#47;# &#47;usr&#47;share&#47;bcc&#47;tools&#47;memleak -p $(pidof app) -a
Attaching to pid 84307, Ctrl+C to quit.
[02:42:22] Top 10 stacks with outstanding allocations:
[02:42:27] Top 10 stacks with outstanding allocations:
[02:42:32] Top 10 stacks with outstanding allocations:
[02:42:37] Top 10 stacks with outstanding allocations:
[02:42:43] Top 10 stacks with outstanding allocations:
[02:42:48] Top 10 stacks with outstanding allocations:
[02:42:53] Top 10 stacks with outstanding allocations:
[02:42:58] Top 10 stacks with outstanding allocations:
[02:43:03] Top 10 stacks with outstanding allocations:
^Croot@ubuntu:&#47;# docker exec app cat &#47;app.c
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;pthread.h&gt;
#include &lt;unistd.h&gt;

long long *fibonacci(long long *n0, long long *n1)
{
	long long *v = (long long *) calloc(1024, sizeof(long long));
	*v = *n0 + *n1;
	return v;
}

void *child(void *arg)
{
	long long n0 = 0;
	long long n1 = 1;
	long long *v = NULL;
	for (int n = 2; n &gt; 0; n++) {
		v = fibonacci(&amp;n0, &amp;n1);
		n0 = n1;
		n1 = *v;
		printf(&quot;%dth =&gt; %lld\n&quot;, n, *v);
		sleep(1);
	}
}


int main(void)
{
	pthread_t tid;
	pthread_create(&amp;tid, NULL, child, NULL);
	pthread_join(tid, NULL);
	printf(&quot;main thread exit\n&quot;);
	return 0;
</div>2019-03-12</li><br/><li><img src="" width="30px"><span>元天夫</span> 👍（2） 💬（2）<div>还有一个很low的问题，Linux version 2.6.32-504.23.4.el6.x86_64 (mockbuild@c6b9.bsys.dev.centos.org) (gcc version 4.4.7 20120313 (Red Hat 4.4.7-11)，这个是我查看的内核信息，这个显示内核版本是4.4.7对吗？</div>2019-02-22</li><br/><li><img src="" width="30px"><span>元天夫</span> 👍（2） 💬（1）<div>老师，请教个问题，pmap -x下，看到有的输出项的脏页数比较大，有104万，这个算大吗</div>2019-02-22</li><br/><li><img src="" width="30px"><span>风一样的男子</span> 👍（2） 💬（1）<div>老师，java web应用有没有类似memleak可以定位代码的好用工具？</div>2019-01-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/33/33/30fb3ac3.jpg" width="30px"><span>____的我</span> 👍（2） 💬（1）<div>老师 我有一个进程内存使用超出正常范围 用pmap命令看到有较多8mb 60mb的匿名内存段使用 这个有什么好的方式定位这些内存段是怎么分配来的吗</div>2019-01-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/ea/8b/613c162e.jpg" width="30px"><span>nomoshen</span> 👍（2） 💬（1）<div>目前我司用的内核版本还是2.6的；而且用valgrind会对线上正在执行的程序有很大性能影响吧；对内存泄露这块还是很难把握的；希望老师能x细聊这块</div>2019-01-03</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/2o1Izf2YyJSnnI0ErZ51pYRlnrmibqUTaia3tCU1PjMxuwyXSKOLUYiac2TQ5pd5gNGvS81fVqKWGvDsZLTM8zhWg/132" width="30px"><span>划时代</span> 👍（2） 💬（2）<div>memleak好像要比valgrind进行内存泄漏检测要方便很多。</div>2019-01-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/97/e1/0f4d90ff.jpg" width="30px"><span>乖，摸摸头</span> 👍（1） 💬（1）<div>还是bcc得问题，装上运行就报错
[root@VM_0_17_centos tools]# .&#47;cachetop
Traceback (most recent call last):
  File &quot;.&#47;cachetop&quot;, line 263, in &lt;module&gt;
    curses.wrapper(handle_loop, args)
  File &quot;&#47;usr&#47;lib64&#47;python2.7&#47;curses&#47;wrapper.py&quot;, line 43, in wrapper
    return func(stdscr, *args, **kwds)
  File &quot;.&#47;cachetop&quot;, line 171, in handle_loop
    b = BPF(text=bpf_text)
  File &quot;&#47;usr&#47;lib&#47;python2.7&#47;site-packages&#47;bcc&#47;__init__.py&quot;, line 318, in __init__
    raise Exception(&quot;Failed to compile BPF text&quot;)
Exception: Failed to compile BPF text</div>2019-08-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/97/e1/0f4d90ff.jpg" width="30px"><span>乖，摸摸头</span> 👍（1） 💬（3）<div>每次都要报这个错，无论yum安装还是编译安装都要报这个错
root@VM_0_17_centos tools]# .&#47;cachestat
In file included from &lt;built-in&gt;:3:
In file included from &#47;virtual&#47;include&#47;bcc&#47;helpers.h:23:
In file included from &#47;lib&#47;modules&#47;5.2.5-1.el7.elrepo.x86_64&#47;build&#47;include&#47;linux&#47;log2.h:12:
In file included from &#47;lib&#47;modules&#47;5.2.5-1.el7.elrepo.x86_64&#47;build&#47;include&#47;linux&#47;bitops.h:19:
&#47;lib&#47;modules&#47;5.2.5-1.el7.elrepo.x86_64&#47;build&#47;arch&#47;x86&#47;include&#47;asm&#47;bitops.h:209:9: error: &#39;asm goto&#39; constructs are not supported yet
        return GEN_BINARY_RMWcc(LOCK_PREFIX __ASM_SIZE(bts), *addr, c, &quot;Ir&quot;, nr);
               ^
&#47;lib&#47;modules&#47;5.2.5-1.el7.elrepo.x86_64&#47;build&#47;arch&#47;x86&#47;include&#47;asm&#47;rmwcc.h:60:32: note: expanded from macro &#39;GEN_BINARY_RMWcc&#39;
#define GEN_BINARY_RMWcc(X...) RMWcc_CONCAT(GEN_BINARY_RMWcc_, RMWcc_ARGS(X))(X)
                               ^
&#47;lib&#47;modules&#47;5.2.5-1.el7.elrepo.x86_64&#47;build&#47;arch&#47;x86&#47;include&#47;asm&#47;rmwcc.h:10:28: note: expanded from macro &#39;RMWcc_CONCAT&#39;
#define RMWcc_CONCAT(a, b) __RMWcc_CONCAT(a, b)
                           ^
&#47;lib&#47;modules&#47;5.2.5-1.el7.elrepo.x86_64&#47;build&#47;arch&#47;x86&#47;include&#47;asm&#47;rmwcc.h:9:30: note: expanded from macro &#39;__RMWcc_CONCAT&#39;
#define __RMWcc_CONCAT(a, b) a ## b
                             ^
note: (skipping 2 expansions in backtrace; use -fmacro-backtrace-limit=0 to see all)
&#47;lib&#47;modules&#47;5.2.5-1.el7.elrepo.x86_64&#47;build&#47;arch&#47;x86&#47;include&#47;asm&#47;rmwcc.h:54:2: note: expanded from macro &#39;GEN_BINARY_RMWcc_6&#39;
        __GEN_RMWcc(op &quot; %[val], &quot; arg0, var, cc,                       \
        ^
&#47;lib&#47;modules&#47;5.2.5-1.el7.elrepo.x86_64&#47;build&#47;arch&#47;x86&#47;include&#47;asm&#47;rmwcc.h:21:2: note: expanded from macro &#39;__GEN_RMWcc&#39;
        asm_volatile_goto (fullop &quot;; j&quot; #cc &quot; %l[cc_label]&quot;             \
        ^
&#47;lib&#47;modules&#47;5.2.5-1.el7.elrepo.x86_64&#47;build&#47;include&#47;linux&#47;compiler_types.h:187:37: note: expanded from macro &#39;asm_volatile_goto&#39;
#define asm_volatile_goto(x...) asm goto(x)
                                    ^
In file included from &lt;built-in&gt;:3:</div>2019-08-04</li><br/><li><img src="" width="30px"><span>suke</span> 👍（1） 💬（1）<div>老师,看了您好多片文章,您的对问题的处理方法都是基于裸机的,现在公司好多都是基于容器,基于虚拟环境的,好多命令和工具不是那么起作用,能不能这方面多写一些</div>2019-08-02</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJlN0AUA3CiaZzorL5Jq1oWPqxOYJwBxnHETFQ7DQFwA7EpQS5tS6mCHribW0rnsyFia88Z0eyqqNYdA/132" width="30px"><span>Geek_638ba5</span> 👍（1） 💬（2）<div>老师 有个问题讨论下
我们的环境跑了一个http的缓存进程，当大量http请求过来时候，缓存进程的内存一直在涨，即使缓存过期了也没有明显降下去。 对于这种守护进程的情况，如何判断内存增长究竟是由于内存泄漏还是内存碎片还是其他原因？(此案例中valgrind不可用，因为这个进程是fork()之后调用exec()产生的，而valgrind不支持此种情况多进程的追踪)</div>2019-06-25</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIe7XMR0W1B8wWNm18KvfMyLUaMLdsD8bcvfhjpN3tRCuBFTE1Mr3ibdNq54t9QgZe4Hv8WC0Ikpaw/132" width="30px"><span>woniu</span> 👍（1） 💬（1）<div>最近在线上遇到用户态cpu很高的问题。apache+php的环境，用strace 追踪发现80%以上的系统syscall都是brk,但是系统的内存占用很低并且很平稳。怀疑是php代码造成的内存泄漏，但是apache回收了内存。请老师指点一下！</div>2019-03-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/9c/f3/e01dfe3a.jpg" width="30px"><span>于競</span> 👍（1） 💬（1）<div>目前遇到一个疑似内存泄漏的问题：nginx代理运行后内存会不断的减少；每次reload之后，内存又会释放。由于服务器也不能随便升级内核安装memleak等这些额外工具，目前也还没定位到具体原因在哪。怀疑是不是因为nginx配置中有些变量赋值和if判断的配置导致的。想请教下，看能不能给些思路。</div>2019-03-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/16/f6/b864cf41.jpg" width="30px"><span>missing</span> 👍（1） 💬（2）<div>老师好！我的系统是ubuntu 14.04,内核版本是4.4.0-31-generic，可以跑cachetop,cachestat，但一跑memleak就会报错。报错如下：
oot@ubuntu:~# memleak -a -p $(pidof mysqld)
&#47;virtual&#47;main.c:18:1: error: could not open bpf map: Invalid argument
is maps&#47;stacktrace map type enabled in your kernel?
BPF_STACK_TRACE(stack_traces, 10240);
^
&#47;virtual&#47;include&#47;bcc&#47;helpers.h:220:3: note: expanded from macro &#39;BPF_STACK_TRACE&#39;
  BPF_TABLE(&quot;stacktrace&quot;, int, struct bpf_stacktrace, _name, roundup_pow_of_two(_max_entries))
  ^
&#47;virtual&#47;include&#47;bcc&#47;helpers.h:75:76: note: expanded from macro &#39;BPF_TABLE&#39;
#define BPF_TABLE(_table_type, _key_type, _leaf_type, _name, _max_entries) \
                                                                           ^
&#47;virtual&#47;include&#47;bcc&#47;helpers.h:71:4: note: expanded from macro &#39;\
BPF_F_TABLE&#39;
}; \
   ^
&#47;virtual&#47;main.c:83:25: error: bpf_table stack_traces failed to open
        info.stack_id = stack_traces.get_stackid(ctx, BPF_F_REUSE_STACKID|BPF_F_USER_STACK);
                        ^
2 errors generated.
Traceback (most recent call last):
  File &quot;&#47;usr&#47;share&#47;bcc&#47;tools&#47;memleak&quot;, line 394, in &lt;module&gt;
    bpf = BPF(text=bpf_source)
  File &quot;&#47;usr&#47;lib&#47;python2.7&#47;dist-packages&#47;bcc&#47;__init__.py&quot;, line 320, in __init__
    raise Exception(&quot;Failed to compile BPF text&quot;)
Exception: Failed to compile BPF text

请问是什么原因呢?</div>2019-02-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/38/5b/26539a0b.jpg" width="30px"><span>marvinren</span> 👍（1） 💬（1）<div>我在试用阿里云发现内存并没有减少，这是因为有其他机制保护内存泄露了么？
root@iZhp35alkngu5crtmh4wjaZ:~&#47;mylab# vmstat 5
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 1  0      0 6445872 692400 810976    0    0  1351     7   15    4  0  1 98  1  0
 0  0      0 6444344 692400 810996    0    0     0     2  579 1071  1  2 97  0  0
 0  0      0 6444348 692400 810968    0    0     0     2  644 1177  1  2 96  0  0
 0  0      0 6462500 692400 810976    0    0     0     3  610 1048  1  2 97  0  0
 0  0      0 6462876 692400 810976    0    0     0     2  663 1131  1  2 97  0  0
 0  0      0 6462692 692400 810996    0    0     0     3  644 1088  1  2 97  0  0
 0  0      0 6462496 692404 810988    0    0     0     2  662 1186  1  2 97  0  0
</div>2019-01-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/97/82/394c88ad.jpg" width="30px"><span>西红柿牛腩</span> 👍（1） 💬（2）<div>老师，麻烦请教下在docker中执行会出错的问题：
系统版本Ubuntu18.04.1
内核：Linux top 4.15.0-43-generic #46-Ubuntu SMP Thu Dec 6 14:45:28 UTC 2018 x86_64 x86_64 x86_64 GNU&#47;Linux
在执行完：
docker run --name=app -itd feisky&#47;app:mem-leak
docker logs app
执行
&#47;usr&#47;share&#47;bcc&#47;tools&#47;memleak -a -p $(pidof app)
开始报错：
root@top:~&#47;app# &#47;usr&#47;share&#47;bcc&#47;tools&#47;memleak -a -p $(pidof app)
&#47;virtual&#47;main.c:18:1: error: could not open bpf map: Cannot allocate memory
is maps&#47;stacktrace map type enabled in your kernel?
BPF_STACK_TRACE(stack_traces, 10240);
^
&#47;virtual&#47;include&#47;bcc&#47;helpers.h:220:3: note: expanded from macro &#39;BPF_STACK_TRACE&#39;
  BPF_TABLE(&quot;stacktrace&quot;, int, struct bpf_stacktrace, _name, roundup_pow_of_two(_max_entries))
  ^
&#47;virtual&#47;include&#47;bcc&#47;helpers.h:75:76: note: expanded from macro &#39;BPF_TABLE&#39;
#define BPF_TABLE(_table_type, _key_type, _leaf_type, _name, _max_entries) \
                                                                           ^
&#47;virtual&#47;include&#47;bcc&#47;helpers.h:71:4: note: expanded from macro &#39;\
BPF_F_TABLE&#39;
}; \
   ^
&#47;virtual&#47;main.c:83:25: error: bpf_table stack_traces failed to open
        info.stack_id = stack_traces.get_stackid(ctx, BPF_F_REUSE_STACKID|BPF_F_USER_STACK);
                        ^
2 errors generated.
Traceback (most recent call last):
  File &quot;&#47;usr&#47;share&#47;bcc&#47;tools&#47;memleak&quot;, line 394, in &lt;module&gt;
    bpf = BPF(text=bpf_source)
  File &quot;&#47;usr&#47;lib&#47;python2.7&#47;dist-packages&#47;bcc&#47;__init__.py&quot;, line 320, in __init__
    raise Exception(&quot;Failed to compile BPF text&quot;)
Exception: Failed to compile BPF text

但是我把app.c和app-fix.c单独拿出来，自行编译运行时，执行memleak，就工作正常了。对docker操作不熟，搞不懂为什么，还请老师有时间指教一下。</div>2019-01-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/0f/84/d8e63885.jpg" width="30px"><span>仲鬼</span> 👍（1） 💬（1）<div>老师好，之前哪节课讲过pmap？并没有找到</div>2019-01-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/13/2d/ad1bfe92.jpg" width="30px"><span>江湖中人</span> 👍（1） 💬（1）<div>有个疑问请教下：
for (int n = 2; n &gt; 0; n++) {
        v = fibonacci(&amp;n0, &amp;n1);
        n0 = n1;
        n1 = *v;
        free(v);    &#47;&#47; 释放内存
        printf(&quot;%dth =&gt; %lld\n&quot;, n, *v);
        sleep(1);
    }
这段代码中，在free(v)之後，printf中仍用到了v中存储的数据，不会有问题吗</div>2019-01-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/d9/47/247fd305.jpg" width="30px"><span>holen</span> 👍（1） 💬（1）<div>老师， 有没有 centos 下不用升级内核就可以检测内存的工具，像 valgrind 工具没办法指定 pid 检测,   有没有更好的工具推荐下呢</div>2019-01-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/34/bb/0b971fca.jpg" width="30px"><span>walker</span> 👍（1） 💬（1）<div>valgrind不是实时的查看命令，把检查结果存入到一个文本中。
valgrind --tool=memcheck --leak-check=full --xtree-leak=yes --show-mismatched-frees=yes test.txt</div>2019-01-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/57/6e/b6795c44.jpg" width="30px"><span>夜空中最亮的星</span> 👍（1） 💬（1）<div>老师，代码段里面可否把  代码前面的 $ 或 # 号，去掉。带着还的手动去掉下才能执行代码</div>2019-01-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/2f/f4/2dede51a.jpg" width="30px"><span>小老鼠</span> 👍（1） 💬（1）<div>用java就不要用free了吧</div>2019-01-02</li><br/>
</ul>