你好，我是张磊。今天我和你分享的主题是：Kubernetes默认调度器调度策略解析。

在上一篇文章中，我主要为你讲解了Kubernetes默认调度器的设计原理和架构。在今天这篇文章中，我们就专注在调度过程中Predicates和Priorities这两个调度策略主要发生作用的阶段。

首先，我们一起看看Predicates。

**Predicates在调度过程中的作用，可以理解为Filter**，即：它按照调度策略，从当前集群的所有节点中，“过滤”出一系列符合条件的节点。这些节点，都是可以运行待调度Pod的宿主机。

而在Kubernetes中，默认的调度策略有如下四种。

**第一种类型，叫作GeneralPredicates。**

顾名思义，这一组过滤规则，负责的是最基础的调度策略。比如，PodFitsResources计算的就是宿主机的CPU和内存资源等是否够用。

当然，我在前面已经提到过，PodFitsResources检查的只是 Pod 的 requests 字段。需要注意的是，Kubernetes 的调度器并没有为 GPU 等硬件资源定义具体的资源类型，而是统一用一种名叫 Extended Resource的、Key-Value 格式的扩展字段来描述的。比如下面这个例子：
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJFE8tbvsHARh0SaZk4aMBD4l3LQgicS44vPje09EWOES0ls4Q5vv4unsgJJZCm4P9ia8TYWa3kWsvA/132" width="30px"><span>鱼自由</span> 👍（7） 💬（1）<div>老师，最后你提到为 Priorities 设置权重，请问，这个操作在哪里进行？</div>2019-01-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/03/5e/818a8b1b.jpg" width="30px"><span>Alex</span> 👍（4） 💬（1）<div>podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: name
                operator: In
                values:
                - nginx-demo
            topologyKey: &quot;kubernetes.io&#47;hostname&quot;

我用了反亲和的特性让pod分到了不同的机器上，不知道是否回答了你的问题</div>2018-11-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/9a/0f/da7ed75a.jpg" width="30px"><span>芒果少侠</span> 👍（90） 💬（7）<div>思考题答案，个人认为有三个解决思路
1. 为pod.yaml设置PreferredDuringSchedulingIgnoredDuringExecution（注意不是required），可以指定【不想和同一个label的pod】放在一起。调度器随后会根据node上不满足podAntiAffinity的pod数量打分，如果不想一起的pod数量越多分数越少。就能够尽量打散同一个service下多个副本pod的分布。
关于这一思路，k8s官网也给出了相同应用的例子。【preferredDuringSchedulingIgnoredDuringExecution 反亲和性的一个例子是 “在整个域内平均分布这个服务的所有 pod”（这里如果用一个硬性的要求是不可行的，因为您可能要创建比域更多的 pod）。】 -- https:&#47;&#47;k8smeetup.github.io&#47;docs&#47;concepts&#47;configuration&#47;assign-pod-node&#47;

2. SelectorSpreadPriority，kubernetes内置的一个priority策略。具体：与services上其他pod尽量不在同一个节点上，节点上同一个Service里pod数量越少得分越高。
3. 自定义策略，实现自己的负载均衡算法（一次性哈希等）。

参考资料：
0. PreferredDuringSchedulingIgnoredDuringExecution ：在调度期间尽量满足亲和性或者反亲和性规则，如果不能满足规则，POD也有可能被调度到对应的主机上。在之后的运行过程中，系统不会再检查这些规则是否满足。
1. https:&#47;&#47;wilhelmguo.cn&#47;blog&#47;post&#47;william&#47;Kubernetes%E8%B0%83%E5%BA%A6%E8%AF%A6%E8%A7%A3
2. https:&#47;&#47;blog.fleeto.us&#47;post&#47;adv-scheduler-in-k8s&#47;
3. https:&#47;&#47;zhuanlan.zhihu.com&#47;p&#47;56088355
4. https:&#47;&#47;kuboard.cn&#47;learning&#47;k8s-advanced&#47;schedule&#47;#filtering</div>2020-03-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/69/88/528442b0.jpg" width="30px"><span>Dale</span> 👍（14） 💬（1）<div>在工作中遇到这个问题，需要将pod尽量分散在不通的node上，通过kubernetes提供的反亲和性来解决的。从回答中老师希望从Priorities阶段中实现，我的想法如下：
1、首先在Predicates阶段，已经选择出来一组可以使用的node节点
2、在Priorities阶段，根据资源可用情况将node从大到小排序，加上node节点个数为m
3、根据pod中配置replicate个数n，在node列表中进行查找出资源可用的top n的node节点
4、如果node节点个数m不满足pod中的replicate个数，每次选择top m之后，重新计算node的资源可用情况，在选择top（n-m）的node，会存在node上有多个情况，最大程度上保证pod分散在不同的node上</div>2018-12-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/37/3b/495e2ce6.jpg" width="30px"><span>陈斯佳</span> 👍（11） 💬（0）<div>第四十二课:Kubernetes默认调度器调度策略解析
Predicates在调度过程中的作用，可以理解为Filter，也就是按照调度策略从当前的集群所有节点中“过滤”出一些符合条件的节点来运行调度的 Pod。

默认的调度策略有三种，一种是GerenalPredicates，负责最基础的调度策略，比如计算宿主机CPU和内存资源等是否够用的PodFitsResource；还有检查宿主机名字是否和Pod的spec.nodeName一致的PodFitsHost。第二类是和Volume相关的过滤规则，比如NoDiskConflict是检查多个Pod申明挂载的持久化Volume是否有冲突。第三类是宿主机相关的过滤条件，主要考察待调度的Pod是否满足Node本身条件，比PodToleratesNodeTaints，负责检查Node 的“污点”taint机制，而 NodeMemoryPressurePredicate，检查的是当前节点的内存是不是已经不够充足，如果是的话，那么待调度 Pod 就不能被调度到该节点上。第四种类型是和Pod相关的过滤规则，这一组规则，跟 GeneralPredicates 大多数是重合的。而比较特殊的，是 PodAffinityPredicate。在具体执行的时候， 当开始调度一个 Pod 时，Kubernetes 调度器会同时启动 16 个 Goroutine，来并发地为集群里的所有 Node 计算 Predicates，最后返回可以运行这个 Pod 的宿主机列表。

在 Predicates 阶段完成了节点的“过滤”之后，Priorities 阶段的工作就是为这些节点打分。这里打分的范围是 0-10 分，得分最高的节点就是最后被 Pod 绑定的最佳节点。Priorities 里最常用到的一个打分规则，是LeastRequestedPriority。这个算法实际上就是在选择空闲资源（CPU 和 Memory）最多的宿主机。此外，还有 NodeAffinityPriority、TaintTolerationPriority 和 InterPodAffinityPriority 这三种 Priority。在默认 Priorities 里，还有一个叫作 ImageLocalityPriority 的策略。它是在 Kubernetes v1.12 里新开启的调度规则，即：如果待调度 Pod 需要使用的镜像很大，并且已经存在于某些 Node 上，那么这些 Node 的得分就会比较高。</div>2021-11-03</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/ZwGweFhVUTfOrrYRk6Dic1IBxFyj2ZgsI1UXQeic2B5uJFdjicsIicnKrJts9v7nGUTCQlSKNUpmvYULq5KjqWjU4g/132" width="30px"><span>freeman</span> 👍（9） 💬（1）<div>首先筛选出满足资源的机器。如果可用节点大于等于需求副本集则一个node一份，顺序取node调度即可，如果node节点少于副本数量，则进行一次调度后，剩下的副本重复上面的事情。直到为每个副本找到对应node或者调度失败。</div>2018-11-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/5e/50/d2cdb05c.jpg" width="30px"><span>陈小白( ´･ᴗ･` )</span> 👍（5） 💬（2）<div>这个我们线上就遇到了，一开始编排好差不多100个系统，发现其中几台主机一堆pod ，内存cpu 都很吃紧，而其他的主机却十分空闲。重启也没用。另外还想请教老师一个问题，我们遇到主机内存不足的时候，经常出现docker  hang 住了，就是docker 的命令完全卡死，没反应，不知道老师有遇到过呢？</div>2019-12-30</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/c6wA5mBibxZWTgatn1xbfsTFZ42bvzYOvicyaOtMREicmaVFntPBwjvhnkPgQ4ZlJgagJSN2oxpvEwYSt5SGOiarzg/132" width="30px"><span>tyamm</span> 👍（4） 💬（0）<div>老师，我有个疑问。这个课程后面会讲到如何搭建master节点高可用么？？</div>2018-11-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/c3/10/3f18e402.jpg" width="30px"><span>王景迁</span> 👍（3） 💬（1）<div>根据老师上面说的，默认的调度策略不是应该有四种类型吗？为什么文章开头说是三种类型？</div>2019-10-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/7e/fe/84914832.jpg" width="30px"><span>小朱</span> 👍（1） 💬（0）<div>请教一个问题，自己启动了另一个scheduler，某一个node上同时存在default-scheduler和second-scheduler调度的资源。但是scheduler只统计schedulerName是自己的pod，这样就和node上面kubelet统计的资源就出现了不一致，这种设计是为什么呢？</div>2018-12-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/bd/07/bf8c31fb.jpg" width="30px"><span>周娄子</span> 👍（1） 💬（0）<div>写一个类似nginx一致性hash的算法</div>2018-11-28</li><br/><li><img src="" width="30px"><span>Geek_4df222</span> 👍（0） 💬（0）<div>思考题：将pod尽可能分布在不同节点上，这是一个优先的调度策略，应该在Priorities 阶段定义，具体来说，根据根据节点上的pod数量，pod数量越少，node的分数越高。</div>2023-08-27</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erB48RAtQ79ocHgDv4abRUlzYwOwTqO2un7L4xOrhz5L67YGbRfvh2kdY9kOQU5Q5ePVgZuFtz1lw/132" width="30px"><span>卢俊义</span> 👍（0） 💬（0）<div>一个dev开启后pod运行在node1的话，重启后是否不管node1的资源占用情况如何，新的pod是否还会运行在node1呢。线上项目遇到很多pod都会堆积在一个node节点，其他节点有空闲资源</div>2023-06-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1e/ff/f3/de2233f5.jpg" width="30px"><span>浅陌</span> 👍（0） 💬（1）<div>请问默认情况下，这些predicate的算法和priority的算法都会被执行吗</div>2022-10-22</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKq0oQVibKcmYJqmpqaNNQibVgia7EsEgW65LZJIpDZBMc7FyMcs7J1JmFCtp06pY8ibbcpW4ibRtG7Frg/132" width="30px"><span>zhoufeng</span> 👍（0） 💬（1）<div>Predicates 单词字面意思是“谓词”，谁知道为啥调度算法要取这个名字，有什么说法吗</div>2022-09-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/3b/ba/3b30dcde.jpg" width="30px"><span>窝窝头</span> 👍（0） 💬（0）<div>避免堆叠的话，一个是将资源剩余量的节点权重增高，镜像较多的节点权重降低，避免pod都调度到一起去</div>2022-07-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/28/6c/de/1934abe6.jpg" width="30px"><span>刘超</span> 👍（0） 💬（1）<div>你们好问个问题，关于POD选择调度NODE的预选、优选过程。 通过预选后，返回可调度NODE节点列表，再对筛选出来的的NODE进行&quot;优选打分&quot;，那么在哪里可以清晰的看到每个优选的NODE节点的得分呢？</div>2021-11-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/e2/13/ae1c85ec.jpg" width="30px"><span>梁尔诗</span> 👍（0） 💬（0）<div>都可以用反亲和性实现 required硬亲和为预选阶段执行  prefered为优选阶段执行</div>2021-08-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/23/a7/3b/d4f90be0.jpg" width="30px"><span>Spark</span> 👍（0） 💬（0）<div>kubelet 在启动 Pod 前，会执行一个 Admit 操作来进行二次确认。这里二次确认的规则，就是执行一遍 GeneralPredicates。
原文这句话对应的源码我看了一下，好像没有podfitshost，podfitsnodes这类检查吧</div>2021-08-04</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/a8PMLmCTCBa40j7JIy3d8LsdbW5hne7lkk9KOGQuiaeVk4cn06KWwlP3ic69BsQLpNFtRTjRdUM2ySDBAv1MOFfA/132" width="30px"><span>Ilovek8s</span> 👍（0） 💬（0）<div>Required Scheduling During Required Scheduling Execute</div>2021-06-18</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/RRQerM7wWWB23jYmINBltjKhicIFTmWMQyGcy64PIRk7RO2vBhdnegLs3oFicVs65zIUxy0CsxyN15AexQUY3WqA/132" width="30px"><span>long</span> 👍（0） 💬（1）<div>张老师您好，请教一个问题：
请问schedule在调度打分的时候会考虑node当前的实际使用率吗？ 谢谢！</div>2021-05-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1e/a6/3e/3d18f35a.jpg" width="30px"><span>三颗豆子</span> 👍（0） 💬（0）<div>是不是找个函数，让Priority的权重设置为“该节点上POD数量越多，权重越低”这样就好了。</div>2021-05-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/d7/37/d8c8acdf.jpg" width="30px"><span>求渔</span> 👍（0） 💬（0）<div>自定义Scoring插件中的priority算法， 使用各节点pod与平均节点pod数之间的方差的剩余百分比来打分</div>2020-10-27</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTI2vn8hyjICTCletGs0omz28lhriaZKX2XX9icYzAEon2IEoRnlXqyOia2bEPP0j7T6xexTnr77JJic8w/132" width="30px"><span>Geek_c22199</span> 👍（0） 💬（0）<div>自定义的话，设置一些规则算法让“堆叠”量化起来</div>2020-04-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/bb/3c/a8a909e4.jpg" width="30px"><span>东东</span> 👍（0） 💬（0）<div>找到堆叠的原因，然后通过调整priorities相应的权重来避免“堆叠”</div>2019-07-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/96/50/bde525b1.jpg" width="30px"><span>北卡</span> 👍（0） 💬（0）<div>细看调度这一块，觉得k8s的设计真的蛮精彩的。</div>2019-01-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/70/49/d7690979.jpg" width="30px"><span>tommyCmd</span> 👍（0） 💬（0）<div>最近也一直被这个问题困扰，讨论一个方案是把pod的qos都设为Guaranteed，希望老师能够解答困惑</div>2018-12-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/a0/e2/e683ce9a.jpg" width="30px"><span>马希民</span> 👍（0） 💬（1）<div>想请教一下老师，在做优先级选择的时候节点的剩余资源是如何得到的？是通过已经分配在该节点上的pod声明limit的资源总和还是通过该节点当前的实际资源使用情况呢？按照调度器使用缓存的说法，似乎应该是前者，那么如果一个pod在声明的时候要求了远远高于本身所需的资源，就会造成这个节点资源的极大浪费？小白不太懂，老师多多指教</div>2018-11-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/64/80/61107e24.jpg" width="30px"><span>快乐就好</span> 👍（0） 💬（0）<div>问一下，我用二进制安装的kubernetes，在实现master的HA时，通过haproxy反代多个Master, 我测试手动注入kube-apiserver故障，进行一下调度和资源操作都是正常的，然后我恢复故障的apiserver，资源操作也都是正常，我用的kube-proxy 是ipvs, 我就发现node上通过ipvsadm查看流量转发情况到apiserver的其他两个master 节点都是有的，但是新恢复的apiserver上面是没有一点流量过来，是否在恢复前需要把故障节点etcd 信息给清了，再启动故障的服务，然后master 状态信息自动同步过来。</div>2018-11-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/60/42/8a79c613.jpg" width="30px"><span>wilder</span> 👍（0） 💬（0）<div>沙发</div>2018-11-28</li><br/>
</ul>