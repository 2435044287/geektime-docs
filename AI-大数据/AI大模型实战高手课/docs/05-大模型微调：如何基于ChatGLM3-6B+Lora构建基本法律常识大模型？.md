你好，我是独行。

上节课我们本地化部署了ChatGLM3-6B，对于大模型有了进一步的了解。这节课我会从实际需求出发，完整地讲解一个AI大模型需求，从提出到完整落地的过程，学完这节课的内容，你也可以在自己所在的企业进行AI大模型落地实践了。

目前我们接触的无论是千亿大模型，如130B、ChatGPT，还是小规模的大模型，如6B、LLaMA2，都是通用大模型，就是说通过通用常识进行预训练的，如果我们在实际使用过程中，需要大模型具备某一特定领域知识的能力，我们就需要对大模型进行能力增强，具体如何做呢？

## 如何增强模型能力？

微调是其中的一个方法，当然还有其他方式，比如外挂知识库或者通过Agent调用其他API数据源，下面我们详细介绍下这几种方式的区别。

- **微调**是一种让预先训练好的模型适应特定任务或数据集的方案，成本相对较低，这种情况下，模型会学习训练者提供的微调数据，并且具备一定的理解能力。
- **知识库**使用向量数据库或者其他数据库存储数据，为大语言模型提供信息来源外挂。
- **API** 和知识库类似，为大语言模型提供信息来源外挂。

简单理解，微调相当于让大模型去学习一门新的学科，在回答的时候进行闭卷考试，知识库和API相当于为大模型提供了新学科的课本，回答的时候进行开卷考试。几种模式并不冲突，我们可以同时使用几种方案来优化模型，提升内容输出能力，下面我简单介绍下几种模式的优缺点。
<div><strong>精选留言（20）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/12/0a/a4/828a431f.jpg" width="30px"><span>张申傲</span> 👍（16） 💬（1）<div>第5讲打卡~
思考题：微调效果应该也和数据量关系很大吧？如果在小数据量下，训练的轮数过多，是不是会降低大模型的泛化能力？就类似人的学习过程，如果把一本没有营养的书反复看了很多遍，反而容易形成思维定式，降低抽象能力。</div>2024-06-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/32/c2/ffa6c819.jpg" width="30px"><span>冰冻柠檬</span> 👍（3） 💬（1）<div>轮数过多就过拟合了啊，泛化能力自然不会好。</div>2024-06-14</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erG6I79WlHDjs51JOff9GBibD4Fh2PhITQMvmh2aTUVzH2BKia1tFLLoQr7VFeZddywwRoZlVUyhDDQ/132" width="30px"><span>Geek_frank</span> 👍（2） 💬（2）<div>打卡第五课。微调轮数过高不利于提升模型的泛化能力。但这个感觉也不好把控。所以有什么最佳实践之类的吗。比如先把参数调小一点。然后经过多次微调调整参数来使模型推理达满意的准确度</div>2024-07-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/25/43/ab300d7c.jpg" width="30px"><span>jsw</span> 👍（2） 💬（1）<div>微调可以了，调整了一些参数，原理的没有按老是的修改。但是感觉微调后的效果很差。</div>2024-06-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/25/43/ab300d7c.jpg" width="30px"><span>jsw</span> 👍（1） 💬（2）<div>原始数据是CSV格式，包含4列：title、question、reply、is_best，需要通过Python语言处理该CSV文件，来构建大语言模型的微调数据集，目标数据集格式是JSON的，单条数据格式为：{&quot;conversations&quot;:[{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:&quot;value1&quot;},{&quot;role&quot;:&quot;assistant&quot;,&quot;content&quot;:&quot;value2&quot;}]}，需要将原始CSV文件里的title列填充到目标JSON文件里的value1处，原始CSV文件里的reply填充到目标JSON文件里的value1处，请注意：最终生成的不是JSON数组，而是每个JSON对象生成一行，出示示例代码。
-----
原始CSV文件里的reply填充到目标JSON文件里的value1处，这里应该是“value2”才对吧。</div>2024-06-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/a0/a4/b060c723.jpg" width="30px"><span>阿斯蒂芬</span> 👍（1） 💬（2）<div>需要业务逻辑特别是一定业务推理能力的时候，微调应该是比纯知识向量检索更合适的。大模型时代，“面向微调”在一定程度上让非资深算法人员也有了“训练模型”的可能😁</div>2024-06-19</li><br/><li><img src="" width="30px"><span>Geek_999422</span> 👍（0） 💬（1）<div>请问老师这个案例的目标输出是什么呢？</div>2025-01-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2e/0c/66/51161385.jpg" width="30px"><span>like life</span> 👍（0） 💬（1）<div>“学习率是一个控制参数更新幅度的超参数。在优化算法中，学习率决定了在反向传播期间参数更新的步长大小。太高的学习率可能导致训练过程不稳定，而太低的学习率可能导致训练进展缓慢或陷入局部最小值”，训练轮数太多，学习率是不是可能会更高导致训练过程不稳定呢?</div>2024-11-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/3b/90/42/8e5e7b36.jpg" width="30px"><span>艾尔·马格尼弗科</span> 👍（0） 💬（1）<div>  File &quot;&#47;home&#47;naiqi&#47;.cache&#47;huggingface&#47;modules&#47;transformers_modules&#47;model&#47;modeling_chatglm.py&quot;, line 685, in get_masks
    past_length = past_key_values[0][0].shape[0]
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: &#39;str&#39; object has no attribute &#39;shape&#39;
老师，这种情况该怎么办呢</div>2024-09-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/22/c7/21/2ae0f713.jpg" width="30px"><span>吧啦啦能量</span> 👍（0） 💬（1）<div>老师，这种情况该怎么办呢OutOfMemoryError: CUDA out of memory. Tried to allocate 214.00 MiB. GPU</div>2024-08-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/a0/c3/c5db35df.jpg" width="30px"><span>石云升</span> 👍（0） 💬（1）<div>好课程，学到了。</div>2024-08-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/38/24/14/4b9add72.jpg" width="30px"><span>夏龙飞</span> 👍（0） 💬（1）<div>准备数据的时候只有train.json和dev.json，但是后边的config中train_file、val_file、test_file有三个，这个怎么设置？</div>2024-07-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/05/7f/a7df049a.jpg" width="30px"><span>Standly</span> 👍（0） 💬（1）<div>文章里多次提到的“反向传播”是什么意思？</div>2024-07-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/25/43/ab300d7c.jpg" width="30px"><span>jsw</span> 👍（0） 💬（2）<div>微调老是报这个错误 OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 
RTX 3090(24GB) 的配置应该够吧，有什么办法调低内存占用吗？</div>2024-06-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/ce/6d/530df0dd.jpg" width="30px"><span>徐石头</span> 👍（0） 💬（2）<div>默认情况下 chatglm3-6b 的微调需要 23G 左右的显存，而且启动运行只需要12G</div>2024-06-17</li><br/><li><img src="" width="30px"><span>Geek_0ef63f</span> 👍（0） 💬（1）<div>没有讲拟合的内容啊</div>2024-06-15</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJZO3Xkicd9Cy8tAian8JnxqVianHNggKcMtdx6sKrQygxnCUKib3ERXIxiaFqkFyhPibkCGzpbdTOiaGvSA/132" width="30px"><span>roman</span> 👍（0） 💬（1）<div>老师 好，我想确认一下 本文中的数据集是不是链接放错了，点开是一个电商广告文案生成数据集，我理解微调应该是需要准备法律相关的数据集吧</div>2024-06-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/4c/cd/6285a3f3.jpg" width="30px"><span>黄琼</span> 👍（0） 💬（1）<div>微调轮数过多会产生过拟合，性能不一定更好。
另：文中出现的finetune_demo 文件夹是自已新建的还是有示例代码提供参考？有些来的突然。</div>2024-06-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/46/3d/55653953.jpg" width="30px"><span>AI悦创</span> 👍（0） 💬（0）<div>能不能把知识库、API 微调教学一下呀，help</div>2025-02-14</li><br/><li><img src="" width="30px"><span>Geek_999422</span> 👍（0） 💬（0）<div>&quot;它们是可训练的，因为在训练过程中，通过反向传播算法这些参数会根据损失函数的梯度不断更新，以减小模型输出与真实标签之间的差异&quot; 这里的真实标签是指什么呢？</div>2025-01-06</li><br/>
</ul>