你好，我是独行。

这节课我会给你介绍大模型中非常重要的一个技术指标：**上下文长度**。我们知道，AI问答类产品和传统问答类产品，在使用层面上有一个很重要的区别就是上下文，AI问答产品可以根据上下文进行更加深层次的问答，给我们的感觉就是很智能，很人性化。

前阵子非常火的AI问答产品Kimi，就是以超长上下文著称，比如支持200万字长文本输入，一次性输入几本书，可以准确进行内容整理输出；再比如GPT-4-turbo，支持128K上下文长度，还有像6B，最新版本已经支持32K上下文长度。

以前各大厂商在宣传自己的产品的时候，讲的最主要的一方面就是**参数规模**，现在除了参数规模，还经常提的就是支持的上下文长度，所以业界有人笑称，大模型卷完参数，开始卷上下文了。今年3月份，阿里云通义千问已经将上下文长度直接提升至1000万字，是Kimi的5倍，而且免费提供给客户使用，一下子卷到了极致。

那么为什么大厂都开始卷上下文长度了呢？

## 为什么超长上下文很重要？

用Kimi所在公司月之暗面（Moonshot）的创始人杨植麟的话说，Lossless long context is everything，杨植麟判断AI产品的终极价值是提供个性化的交互，⽽lossless long-context是实现这⼀点的基础。**模型的微调不应该⻓期存在， 用户跟模型的交互历史就是最好的个性化过程。**
<div><strong>精选留言（3）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/12/0a/a4/828a431f.jpg" width="30px"><span>张申傲</span> 👍（5） 💬（1）<div>第23讲打卡~
我们是做AI角色扮演聊天应用的，目前就遇到了比较棘手的“大模型记忆”问题，特别是在用户和角色经历了多轮对话之后，如何让AI角色拥有媲美人类的记忆，准确地记住之前经历的事情和说过的话，目前对我们来说是一个比较大的挑战，貌似行业内也没有特别好的解决方案，大家也都在探索阶段~</div>2024-07-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/3a/78/b0/13b19797.jpg" width="30px"><span>潇洒哥66</span> 👍（0） 💬（1）<div>token是上下文的线性函数，自注意力机制是token数量的二次方。因此，自注意力会带来上下文二次方的相关性计算</div>2024-10-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/a0/c3/c5db35df.jpg" width="30px"><span>石云升</span> 👍（3） 💬（0）<div>首先，我们需要了解标准自注意力机制的计算复杂度。假设我们有一个序列长度为 n 的输入，每个 token 的嵌入维度为 d。
标准自注意力机制的计算复杂度为 O(n^2 * d)。这是因为：
1.我们需要计算 n 个 token 之间的所有 n^2 个注意力分数。
2.对于每个注意力分数，我们需要进行 d 维的点积操作。
现在，让我们看看当上下文长度增加时，计算量如何变化：
1.线性增长的情况：
如果上下文长度从 n 增加到 2n，计算复杂度会变为 O((2n)^2 * d) = O(4n^2 * d)。
计算量增加：(4n^2 * d) &#47; (n^2 * d) = 4 倍
2.平方增长的情况：
如果上下文长度从 n 增加到 n^2，计算复杂度会变为 O((n^2)^2 * d) = O(n^4 * d)。
计算量增加：(n^4 * d) &#47; (n^2 * d) = n^2 倍
具体计算示例：假设我们有一个原始上下文长度为 1,000 tokens，嵌入维度为 512 的模型。
1.如果上下文长度增加到 2,000 tokens：

- 原始计算量：1,000^2 * 512 = 512,000,000
- 新计算量：2,000^2 * 512 = 2,048,000,000
- 计算量增加了 4 倍
2.如果上下文长度增加到 10,000 tokens：

- 原始计算量：1,000^2 * 512 = 512,000,000
- 新计算量：10,000^2 * 512 = 51,200,000,000
- 计算量增加了 100 倍
需要注意的是，这只是理论上的计算量增加。在实际应用中，还需考虑以下因素：
1.内存限制：随着序列长度的增加，所需的内存也会急剧增加。
2.优化技术：如稀疏注意力、滑动窗口注意力等可以降低计算复杂度。
3.硬件效率：某些长度可能更适合特定硬件的并行处理能力。
4.实际实现：某些优化可能使实际增长略低于理论值。
总的来说，计算量的增加与上下文长度的平方成正比。这就是为什么处理超长上下文时，我们需要特殊的优化技术，如稀疏注意力、局部注意力等，来降低计算复杂度。</div>2024-09-08</li><br/>
</ul>