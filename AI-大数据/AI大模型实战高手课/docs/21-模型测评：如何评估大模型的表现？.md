你好，我是独行。

这一节课我们来聊聊模型测评，和我们软件测试一样，模型训练完也需要进行测试。软件测试我们一般会关注功能完整性、性能水平、运行稳定性等。大模型也一样，它会关注推理效率、性能等等。我们先来了解下各个厂商为什么要做模型测评。

## 背景

一方面，不论是软件还是大模型，厂商都需要对其功能有效性进行测试，通过业界相对标准的方式去测，可以看清楚自己产品的真正实力以及和其他竞争产品的差距。另一方面，一些大厂希望通过刷新一些著名榜单，来提升自己产品的知名度和竞争力，比如在大模型之前，比较出名的就是各个数据库厂商，像TiDB、阿里云的PolarDB等等，都会在自己的官方网站上介绍其性能指标，比较出名的基准像TPC-C、TPC-C、Sysbench等，最后结论就是比MySQL性能提升多少多少这种。不可否认，这确实是一种好的方式。

如果你关注各个大模型厂商的网站，一定会经常看到下面这样的评测数据，这是阿里云通义千问介绍页面上放出的一组评测数据。

![图片](https://static001.geekbang.org/resource/image/b9/04/b9424a33eeef98998bb9921551191304.png?wh=809x600)

以下是[原文内容](https://github.com/QwenLM/Qwen/blob/main/README_CN.md)：

> *Qwen系列模型相比同规模模型均实现了效果的显著提升。我们评测的数据集包括MMLU、C-Eval、 GSM8K、 MATH、HumanEval、MBPP、BBH等数据集，考察的能力包括自然语言理解、知识、数学计算和推理、代码生成、逻辑推理等。Qwen-72B在所有任务上均超越了LLaMA2-70B的性能，同时在10项任务中的7项任务中超越GPT-3.5*\*。*
<div><strong>精选留言（1）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/a0/c3/c5db35df.jpg" width="30px"><span>石云升</span> 👍（3） 💬（0）<div>题目太单一：都是选择题，应该加点开放式或主观题，测试更全面。
难度分级不够：难度划分可以更细，增加挑战性。
缺人类反馈：可以用专家反馈，评估更准确。
题库更新慢：题目固定，跟不上技术进步，需要动态更新。
分析不深入：除了看对错，也要分析模型的推理和理解过程。</div>2024-09-06</li><br/>
</ul>