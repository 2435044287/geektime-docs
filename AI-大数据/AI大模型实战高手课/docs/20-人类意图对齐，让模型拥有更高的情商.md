你好，我是独行。

这节课我会向你介绍大模型背后的“大功臣”——**Alignment**，翻译过来就是与人类意图对齐。Alignment是一类技术的统称，并非指某一个技术。在第一节课我向你介绍ChatGPT为什么崛起的时候，讲到过NLP技术的突破，其中之一就是与人类意图对齐，里面最重要的一项技术就是**基于人类反馈的强化学习，简称RLHF。**

## RLHF由来

我们知道 GPT-3于2020年3月份发布，在当时算是一个非常强大的模型，可以使用精心设计的文本提示来引导它执行自然语言任务。但是，GPT-3也可能产生不真实、有毒或反映有害情绪的内容，原因我们之前讲过，GPT-3的训练数据主要来自于互联网，而互联网中掺杂了各种各样的内容，有些是正常的，有些则不正常。所以直接输出内容很可能会不符合人类意图，官方称这种情况为“不安全”。

后来为了解决这个问题，OpenAI基于RLHF做了指令微调模型 **InstructGPT**，使大模型输出的有害内容大大减少，虽然参数少了100倍以上，但与175B参数的GPT-3输出相比，仅有1.3B规模参数InstructGPT模型的输出更加符合人类意图。我们看一下官网披露的GPT-3经过SFT和指令微调后，在各种指标方面的对比。
<div><strong>精选留言（2）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/12/0a/a4/828a431f.jpg" width="30px"><span>张申傲</span> 👍（6） 💬（1）<div>第20讲打卡~
思考题：个人认为RLHF可以算作一种更加复杂的微调技术，它和传统微调的目的一致，都是通过调整模型的参数来改善它的性能。但是相比于传统微调技术，RLHF不仅有预定义好的标注数据，而且还引入了人类反馈作为奖励信号，相当于强化学习+监督学习，所以应该会比传统的微调技术更加复杂、且更与人类的预期保持一致。</div>2024-07-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/a0/c3/c5db35df.jpg" width="30px"><span>石云升</span> 👍（1） 💬（0）<div>RLHF可以被视为一种特殊的、更复杂的微调技术。它不仅调整了模型的参数，还引入了人类偏好作为指导，使模型在更广泛的情况下能够产生符合人类期望的输出。</div>2024-09-06</li><br/>
</ul>