你好，我是独行。

上一节课我们详细讲解了基于ChatGLM3-6B + LangChain + 向量数据库的企业内部知识系统，在这个演示项目中，其实已经用到了API的封装，我们从WebUI界面提问，通过接口将数据传到后端服务，从而获得响应。大模型是没有Web API的，所以需要我们进行一次封装，将大模型的核心接口封装成Web API来为用户提供服务，这是企业自建大模型的必经之路。

在这里我们需要引入一个类似于SpringBoot的框架，用来做接口服务化，在Python技术体系里，有一个框架叫 **FastAPI**，可以很方便地实现接口注册，所以我们这节课会基于FastAPI对大模型的接口进行封装。实际上光写一个Demo不算难，但是如果要完整地用于工程化项目，还是有不少事情要注意，所以这节课我会把各种各样和API相关的细节梳理出来，学完这节课的内容，再结合前面学习的大模型部署，你本地搭建的大模型基本可以对外提供服务了。

## 接口封装

提供Web API服务需要两个技术组件：Uvicorn和FastAPI。

Uvicorn作为Web服务器，类似Tomcat，但是比Tomcat轻很多。允许异步处理 HTTP 请求，所以非常适合处理并发请求。基于uvloop和httptools，所以具备非常高的性能，适合高并发请求的现代Web应用。
<div><strong>精选留言（16）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/12/0a/a4/828a431f.jpg" width="30px"><span>张申傲</span> 👍（13） 💬（3）<div>第7讲打卡~
思考题：这里应该主要考虑的是不同语言的优势和适用场景。使用Python实现大模型的核心API，应该是因为Python是机器学习领域最主流的语言，包括了像pytorch、TensorFlow等主流的框架，而且一些主流的大模型像ChatGPT也提供了完善的Python SDK，使用起来比较方便。而在外面又套了一层Java应用，应该是考虑这么多年来Java在Web服务端领域积累下来的完善的生态，针对用户端的应用，可以快速构建起服务鉴权、路由、熔断、降级、限流、可观测等等能力。</div>2024-06-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/2d/86/07a10be2.jpg" width="30px"><span>Lee</span> 👍（3） 💬（1）<div>老师 我等不及了  上瘾了  催更催更</div>2024-06-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/f4/b7/5c775af5.jpg" width="30px"><span>狒狒</span> 👍（1） 💬（1）<div>python基础不是太好，有可用的完整代码吗，文中的代码似乎不能直接使用，比如结构版本与实际案例不一致，在controller中引入serveice时无法直接引入</div>2024-12-02</li><br/><li><img src="" width="30px"><span>Geek_4d9162</span> 👍（0） 💬（1）<div>老师 紧急求助，按课程我用开源大模型封装成问答服务后，如何让大模型回答“你是谁”这类自我认知问题时，按我的要求回答？比如回答 我是xx人开发的大模型</div>2025-01-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/3c/7a/e8/c86468e8.jpg" width="30px"><span>神经蛙</span> 👍（0） 💬（1）<div>大模型的输出降级方案有哪些考虑方案？比如如果被引导输出不当言论，如何快速封掉这个漏洞？机器负载达到上限后又该如何处理？</div>2024-11-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/48/f3/65c7e3ef.jpg" width="30px"><span>cricket1981</span> 👍（0） 💬（1）<div>请问国内如何使用huggingface? 很多国外大模型网站被墙了，有什么替代方案或work around方法吗？</div>2024-08-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/57/a3/4bcf1b59.jpg" width="30px"><span>大宽</span> 👍（0） 💬（1）<div>老师 ，示例示例中的大模型推理框架用的是哪个呢，可否引入 VLLM</div>2024-07-13</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erG6I79WlHDjs51JOff9GBibD4Fh2PhITQMvmh2aTUVzH2BKia1tFLLoQr7VFeZddywwRoZlVUyhDDQ/132" width="30px"><span>Geek_frank</span> 👍（0） 💬（1）<div>打卡第六课：python封装大模型借口提供AI服务有天然的优势，基本上大模型都是用python开发的，开源的也都有python的lib。使用python进行API封装或者模型微调都特别方便。 在AI服务之上的一些政增值服务可能java来提供更好一些，因为java适合业务的封装。但也不是非java不可，python也可以实现不做的业务服务。我现在做的一个运维一体化平台很多业务实现都是python来做的。比如认证鉴权，配置仓库，在线作业管理等等</div>2024-07-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2c/2b/8e/4d24c872.jpg" width="30px"><span>season</span> 👍（0） 💬（3）<div>课程里面的代码，有代码仓库吗？</div>2024-06-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/8b/4b/15ab499a.jpg" width="30px"><span>风轻扬</span> 👍（0） 💬（1）<div>如果大家在跑demo的时候遇到这个错误。TypeError: ChatGLMForConditionalGeneration.chat() missing 1 required positional argument: &#39;query&#39;，可以尝试将chat_service.py文件中，model.chat方法的第二个入参去掉&quot;prompt=&quot;，只保留message.prompt即可</div>2024-06-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/22/45/bd/df16cf9d.jpg" width="30px"><span>0.0</span> 👍（0） 💬（2）<div>dify 是否更优雅</div>2024-06-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/8b/4b/15ab499a.jpg" width="30px"><span>风轻扬</span> 👍（0） 💬（1）<div>老师，ModelManager类中，两个from_pretrained方法的第一个入参都是模型的路径吧？我按照文中的代码实现了一下，在一张3090 gpu上跑，python版本3.11，一直报：TypeError: ChatGLMForConditionalGeneration.chat() missing 1 required positional argument: &#39;query&#39;，查了查ChatGLMForConditionalGeneration，并没有chat方法，不知道咋排查了。。。。</div>2024-06-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/8b/4b/15ab499a.jpg" width="30px"><span>风轻扬</span> 👍（0） 💬（1）<div>思考题，是为了解耦？工程类的代码修改，不太应该让模型跟着一起发布。</div>2024-06-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/d4/44/0ec958f4.jpg" width="30px"><span>Eleven</span> 👍（0） 💬（2）<div>我微调完成后再去composite_demo启动，对话好像没有效果呀
</div>2024-06-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/ce/6d/530df0dd.jpg" width="30px"><span>徐石头</span> 👍（0） 💬（1）<div>为什么这么设计？
高内聚，低耦合。把大模型 API 封装在 python 中作为一个微服务从整个架构中独立出来，它只处理和大模型相关的，不和其他业务接口放一起。它就可以单独部署，扩容，隔离，而且由单独的人维护，也是微服务的优势。
如果我来做，我还会增加 rpc接口，注册到注册中心。</div>2024-06-17</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJZO3Xkicd9Cy8tAian8JnxqVianHNggKcMtdx6sKrQygxnCUKib3ERXIxiaFqkFyhPibkCGzpbdTOiaGvSA/132" width="30px"><span>roman</span> 👍（0） 💬（1）<div>老师 好，再套一层应用我的想法是这样的，不知道是否正确 
1.保护底层大模型应用：套一层类似Java应用层的应用可以做到将大模型的底层服务和客户端的访问进行隔离，避免一些客户端注入攻击类型的场景
2.给应用层提供更多可扩展的场景，底层大模型只是一个工具，但是如何让工具更好的满足不同的业务场景，可能就需要在应用层做一些扩展，比如应用层可以针对同一个问题调用多个大模型（避免出现大模型单点故障问题）等场景的扩展</div>2024-06-17</li><br/>
</ul>