前面，我和你聊过了不要把用户画像当成银弹，也不要觉得一无是处。对于一个早期的推荐系统来说，基于内容推荐离不开为用户构建一个初级的画像，这种初级的画像一般叫做用户画像（User Profile），一些大厂内部还习惯叫做UP，今天我就来讲一讲从大量文本数据中挖掘用户画像常常用到的一些算法。

## 从文本开始

用户这一端比如说有：

1. 注册资料中的姓名、个人签名；
2. 发表的评论、动态、日记等；
3. 聊天记录（不要慌，我举个例子而已，你在微信上说的话还是安全的）。

物品这一端也有大量文本信息，可以用于构建物品画像（ Item Profile ），并最终帮助丰富 用户画像（User Profile），这些数据举例来说有：

1. 物品的标题、描述；
2. 物品本身的内容（一般指新闻资讯类）；
3. 物品的其他基本属性的文本。

文本数据是互联网产品中最常见的信息表达形式，数量多、处理快、存储小，因为文本数据的特殊地位，所以今天我专门介绍一些建立用户画像过程中用到的文本挖掘算法。

## 构建用户画像

要用物品和用户的文本信息构建出一个基础版本的用户画像，大致需要做这些事：

**1. 把所有非结构化的文本结构化，去粗取精，保留关键信息；**  
  
**2. 根据用户行为数据把物品的结构化结果传递给用户，与用户自己的结构化信息合并。**

第一步最关键也最基础，其准确性、粒度、覆盖面都决定了用户画像的质量。仿佛如果真的要绘制一个用户的模样，要提前给他拍照，这个拍照技术决定了后面的描绘情况，无论是采用素描、油画、工笔还是写意。这一步要用到很多文本挖掘算法，稍后会详细介绍。

第二步会把物品的文本分析结果，按照用户历史行为把物品画像（ Item Profile ）传递给用户。你也许会问：传递是什么意思？没关系，这个稍后我会介绍。

### 一、结构化文本

我们拿到的文本，常常是自然语言描述的，用行话说，就是“非结构化”的，但是计算机在处理时，只能使用结构化的数据索引，检索，然后向量化后再计算；所以分析文本，就是为了将非结构化的数据结构化，好比是将模拟信号数字化一样，只有这样才能送入计算机，继续计算。这个很好理解，不多解释。

从物品端的文本信息，我们可以利用成熟的NLP算法分析得到的信息有下面几种。

1. 关键词提取：最基础的标签来源，也为其他文本分析提供基础数据，常用TF-IDF和TextRank。
2. 实体识别：人物、位置和地点、著作、影视剧、历史事件和热点事件等，常用基于词典的方法结合CRF模型。
3. 内容分类：将文本按照分类体系分类，用分类来表达较粗粒度的结构化信息。
4. 文本 ：在无人制定分类体系的前提下，无监督地将文本划分成多个类簇也很常见，别看不是标签，类簇编号也是用户画像的常见构成。
5. 主题模型：从大量已有文本中学习主题向量，然后再预测新的文本在各个主题上的概率分布情况，也很实用，其实这也是一种聚类思想，主题向量也不是标签形式，也是用户画像的常用构成。
6. 嵌入：“嵌入”也叫作Embedding，从词到篇章，无不可以学习这种嵌入表达。嵌入表达是为了挖掘出字面意思之下的语义信息，并且用有限的维度表达出来。

下面我来介绍几种常用的文本结构化算法。

**1 TF-IDF**

TF全称就是Term Frequency，是词频的意思，IDF就是 Inverse Document Frequency 是逆文档频率的意思。TF-IDF提取关键词的思想来自信息检索领域，其实思想很朴素，包括了两点：在一篇文字中反复出现的词会更重要，在所有文本中都出现的词更不重要。非常符合我们的直觉，这两点就分别量化成TF和IDF两个指标：

1. TF，就是词频，在要提取关键词的文本中出现的次数；
2. IDF，是提前统计好的，在已有的所有文本中，统计每一个词出现在了多少文本中，记为n，也就是文档频率，一共有多少文本，记为N。

IDF就是这样计算：  
![](https://static001.geekbang.org/resource/image/a9/63/a90ba5c08a6ea42773633b278ceca863.png?wh=292%2A144)  
计算过程为：词出现的文档数加1，再除总文档数，最后结果再取对数。

IDF的计算公式有这么几个特点：

1. 所有词的N都是一样的，因此出现文本数越少(n)的词，它的IDF值越大；
2. 如果一个词的文档频率为0，为防止计算出无穷大的IDF，所以分母中有一个1；
3. 对于新词，本身应该n是0，但也可以默认赋值为所有词的平均文档频率。

计算出TF和IDF后，将两个值相乘，就得到每一个词的权重。根据该权重筛选关键词的方式有：

1. 给定一个K，取Top K个词，这样做简单直接，但也有一点，如果总共得到的词个数少于K，那么所有词都是关键词了，显然这样做不合理；
2. 计算所有词权重的平均值，取在权重在平均值之上的词作为关键词；

另外，在某些场景下，还会加入以下其他的过滤措施，如：只提取动词和名词作为关键词。
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/10/24/06/f0fe5cb5.jpg" width="30px"><span>张哲</span> 👍（30） 💬（1）<div>这一期信息量好大……</div>2018-03-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（11） 💬（1）<div>谢谢刑无刀老师的分享。建议各种步骤和场景的工业化工具及其性能和便利，对我们读者提供了不小的价值。

1. 词嵌入里面提到学习到的包含更多语义信息的(新)词向量可以“用于聚类，会得到比使用词向量聚类更好的语义聚类效果”。这里的聚类是指文中之前提到的LDA等聚类模型来找出文章的主题吗？

2. “主题模型：从大量已有文本中学习主题向量，然后再预测新的文本在各个主题上的概率分布情况，也很实用，其实这也是一种聚类思想，主题向量也不是标签形式，也是用户画像的常用构成。”请问这里已有文中的主题向量中的主题词如果是通过LDA提取是不是需要有个停用词表排除那些所有文档中词频都很高的词？它不像TF-IDF会自动排除所有文档中词频高的词。 这种场景的聚类就是判别新的文本和哪些主题的文本比较相似（”距离“接近或主题”概率“较大），然后判别新的文本的主题？

3. ”向量中各个维度上的值大小代表了词包含各个语义的多少“ 有这句说明挺好的。我第一次阅读不太理解，后来查了一些文章，有一个解释我觉得比较直观，Word2Vec生成的向量值可以看成是N维语义空间(N个语义词)中的坐标值(每个坐标轴对应一个语义)。当2个词在同一个N维语义空间中的距离接近时，说明2个词的含义接近。</div>2018-03-14</li><br/><li><img src="" width="30px"><span>Drxan</span> 👍（9） 💬（1）<div>无刀老师，能否建立个微信群啊，大家可以对您每期的课程内容一起学习讨论</div>2018-03-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/26/a7/177581b8.jpg" width="30px"><span>jt120</span> 👍（6） 💬（2）<div>针对embedding，我不太理解，之前理解就是一种映射关系，但文里为什么说结果是稠密的，这是怎么保证的</div>2018-03-14</li><br/><li><img src="" width="30px"><span>行行行</span> 👍（3） 💬（2）<div>老师，关于word2vec，有几个疑问
1  工业上如果通过word2vec得到文档的向量呢，是用累加一个文档中各个词的词向量得到的稠密向量表示吗
2 用于聚类，是用上面得到的文档向量来做吗
3 到底是如何通过计算词和词之间的相似度，扩充标签的呢</div>2018-03-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/26/a7/177581b8.jpg" width="30px"><span>jt120</span> 👍（3） 💬（1）<div>上面提到的都是主流的推荐系统方法，例如电影，图书，新闻这些经典场景。
但对于特殊商品，例如房子，明显和上面不同，低频，高价，并且房子的特征基本都是分类特征，针对这种场景，如何选择特征，如何推荐了？</div>2018-03-14</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/pIic2qhfe5ZpmQmSdxjrwVoGaZ7xLxwLSSdeZjicqtTV1E2Kl7sFGutCOW71N5ulRAXSgItuZN9I7Xkeg4icgdFSA/132" width="30px"><span>wjj</span> 👍（1） 💬（1）<div>老师，TF—IDF中的Top K 排序，实际工作中超参数K值一般取多少？</div>2019-05-31</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIFDmC78F2ciaDVn24E36emK6mE43ZsRxeLeGHVM9IqeVn0uQabzO2Kdc9JNTOKBUeghJbOBpww2EA/132" width="30px"><span>Kendal</span> 👍（1） 💬（1）<div>邢老师你好，我是初学者，第二部分把物品的结构化信息传递给用户这里，您把他看成是一个特征选取的问题。这里没太看懂，还望能够详细解释下。
假设用户A和我们给他展现的100个物品有了2类操作（消费10，没消费90）。我的理解是这边每个被消费的物品（10个）都有自己的特征向量（假设n维），我们的任务是找到这n维里面到底哪m维是用户真正关心的。这个理解对吗？然后如何选取到这m维，并把它们融合到用户自己原来的向量中又是如何具体实现的？
谢谢指点！</div>2019-02-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/51/47/6875debd.jpg" width="30px"><span>预见</span> 👍（1） 💬（1）<div>我来补充林彦同学的第三点，”向量中各个维度上的值大小代表了词包含各个语义的多少“ 。第一遍看到这句话的时候我没有看懂，查阅资料后才明白。比如使用word embedding，一个单词“北京”，用5维向量“首都，中国，大城市，南方，没雾霾”来表示的话，他的向量形式就是[1, 1, 1, 0, 0]，各个维度的值的大小代表了词包含各个语义的多少。老师要是讲的再细致一点，给出示例就更好了</div>2018-12-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/64/af/8ec3f465.jpg" width="30px"><span>尹士</span> 👍（1） 💬（1）<div>Fasttext准确率跟cnn比，有差距，我的实验结果，不知邢老师参数如何设置的，可以工程中使用fasttext</div>2018-04-01</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK6mh3xlaMoGtWjmVJh2LutdLcQcPbKNjRlVru3bx8ynPhgwuGhhdzTkwEMoXbvBtgkcDSfom1kZg/132" width="30px"><span>夜雨声烦</span> 👍（0） 💬（1）<div>挺好的  了解了好多</div>2019-11-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/65/c2/8ade8f57.jpg" width="30px"><span>Ann</span> 👍（0） 💬（1）<div>老师，求微信交流群 进入组织</div>2019-03-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/2d/58/aa35c402.jpg" width="30px"><span>无法言喻.</span> 👍（0） 💬（1）<div>老师，主题模型和文本都是通过分类或者聚类算法得到， 有什么区别呢？</div>2018-12-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/e5/2a/738f90d7.jpg" width="30px"><span>Virgil 阿城</span> 👍（0） 💬（1）<div>老师，依据《概率与统计》教材中，卡方分布的卡方值= 累加（(观察值 - 期望值)&#47;期望值））和您公式里的结果对不上。
我猜如果可以应该也是能相互推导，请问能否解释下具体原因？</div>2018-10-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/2c/53/8c2f976a.jpg" width="30px"><span>那年岁月</span> 👍（0） 💬（1）<div>问一下老师，w2v算法把文章中没一个词都做向量化，是把文章所有词向量累加还是等长关键词累加，如果把所有词累加，那么对篇幅小的文章不公平。</div>2018-05-04</li><br/><li><img src="" width="30px"><span>行行行</span> 👍（0） 💬（1）<div>老师我的理解是新增益越大特征越有区分度，如果这个特征在正例中，用户和这个特征越相关，但如果这个特征正例和负例中都有呢，该怎么算</div>2018-03-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/23/63/36cf68ef.jpg" width="30px"><span>清风六十五</span> 👍（0） 💬（1）<div>简洁扼要，很不错</div>2018-03-15</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJOBwR7MCVqwZbPA5RQ2mjUjd571jUXUcBCE7lY5vSMibWn8D5S4PzDZMaAhRPdnRBqYbVOBTJibhJg/132" width="30px"><span>ヾ(◍°∇°◍)ﾉﾞ</span> 👍（0） 💬（1）<div>一到实际内容的时候，就会在听说过的和没听说过的，懂了的还有不知道是什么的知识点里无法理顺思路了</div>2018-03-14</li><br/><li><img src="" width="30px"><span>Drxan</span> 👍（0） 💬（1）<div>无刀老师，不好意思早晨地铁上把字打错了，请见谅！文中提到利用每个用户所见过的物品的文档集合来生成用户画像特征，一般是如何确定哪些物品是用户见过的呢？如果我只有用户的购买记录，而没有浏览、收藏等记录，那是否可以从用户没有购买过的商品中随机抽取部份商品作为负样本呢</div>2018-03-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/82/15/abb7bfe3.jpg" width="30px"><span>莘哥（cheung！）</span> 👍（0） 💬（1）<div>都有哪些成熟的用户画像开发框架呢？</div>2018-03-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/28/d8/356428c2.jpg" width="30px"><span>叶晓锋</span> 👍（0） 💬（1）<div>重要的应该是把文本数据转换成机器能处理的量化数据</div>2018-03-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/28/d8/356428c2.jpg" width="30px"><span>叶晓锋</span> 👍（0） 💬（1）<div>文本来是非常重要的数据源，一般来说物品端的数据比较好控制，包括标题，描述，说明等，用户端的数据会少一些。</div>2018-03-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/22/69/c85fdb98.jpg" width="30px"><span>微微一笑</span> 👍（0） 💬（1）<div>内容很精彩，很受启发。咨询一个问题：在短文本领域，不适合使用LDA吧？如果不适合,有什么替代方法吗？谢谢</div>2018-03-14</li><br/><li><img src="" width="30px"><span>Drxan</span> 👍（0） 💬（1）<div>无天老师，文中提到利用每个用户所见过的物品的文档集合来生成用户画像特征，一般是如何确定哪些物品是用户见过的呢？如果我只有用户的购买记录，而没有浏览、收藏等记录，那是否可以从用户没有购买过的商品中随机抽取部份商品作为负样本呢</div>2018-03-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/21/9b/347301f6.jpg" width="30px"><span>关羽</span> 👍（0） 💬（1）<div>老师，请问下IDF算法，log的底是什么？10？</div>2018-03-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/28/1d/b7cf921d.jpg" width="30px"><span>三竹先生</span> 👍（3） 💬（0）<div>真的好难啊，邢老师能多结合一下例子吗？方便理解😂😂😂</div>2018-03-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1a/5e/06/8de84ec3.jpg" width="30px"><span>🐱无限大人🐱</span> 👍（2） 💬（0）<div>这篇好干，得好好啃一下</div>2019-11-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/8f/d8/47c24ec8.jpg" width="30px"><span>张苗</span> 👍（2） 💬（0）<div>这一节都是自然语言处理的基本知识，幸好毕业是做的这块，不然够我喝一壶~😂</div>2019-07-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/a7/b2/274a4192.jpg" width="30px"><span>漂泊的小飘</span> 👍（2） 💬（0）<div>幸亏学过人工智能，不然这一章够我喝一壶了</div>2019-07-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/5b/14/a1567454.jpg" width="30px"><span>涛</span> 👍（2） 💬（1）<div>我有一个比较困扰我的问题就是信息增益和WOE、IV之间的区别，我不明白为什么金融欺诈中选取重要性特征都用WOE和IV 方法，而其它领域很少看见用这两个方法进行筛选特征的，谢谢！</div>2018-09-25</li><br/>
</ul>