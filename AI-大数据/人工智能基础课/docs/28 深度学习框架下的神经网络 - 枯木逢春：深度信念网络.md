2006年，深度学习的祖师爷乔弗里·辛顿提出了深度信念网络模型，它吹响了连接主义学派复兴的号角，也打开了通向人工智能新世界的大门。

**深度信念网络是一种概率生成模型，能够建立输入数据和输出类别的联合概率分布**。网络中包含多个隐藏层，隐藏层中的隐藏变量通常是二进制数，用来对输入信号进行特征提取。输入信号从深度信念网络的最底层输入，并自下而上有向地传递给隐藏层。而在网络最上面的两层中，神经元之间的连接是没有方向并且对称的，这两个层次共同构成了联想记忆。

从功能上看，深度信念网络的每一个隐藏层都代表着对输入数据的一种中间表示，而隐藏层中的每个神经元都代表着输入数据不同层次上的特征，不同层神经元之间的连接则代表着不同层次特征之间的联系，所有特征和特征之间的所有关系共同形成了对输入数据的抽象描述。

**从结构上看，复杂的深度信念网络可以看成由若干简单的学习单元构成的整体，而构成它的基本单元就是受限玻尔兹曼机**（restricted boltzmann machine）。受限玻尔兹曼机早在1986年便已诞生，可直到20年后才因辛顿的成果而得到重视。

**受限玻尔兹曼机的模型非常简单，就是一个两层的神经网络，包括一个可见层和一个隐藏层**。可见层用来接收数据，隐藏层则用来处理数据。可见层和隐藏层以全连接的方式相连，也就是任意两个不同层次中的神经元都会两两相连。但同一层中的神经元则不会互相连接，因而每个层内也就没有信息流动，这正是其名称中“受限”的来源。

回忆一下神经网络中介绍过的神经元的工作机制：每个隐藏神经元的输入都是数据向量中所有元素的线性组合，这个线性组合和偏置信号相加后，共同作为神经元传递函数的输入，而传递函数的输出就是隐藏神经元的输出。但受限玻尔兹曼机所做的远非得到个输出这么简单的事情，它还要以无监督的方式对数据进行重构。即使没有更深层的网络结构，数据也会在输入层和隐藏层中进行多次前向和反向的传递。

在隐藏神经元得到输出后，受限玻尔兹曼机需要将输出结果反馈给可见层。具体的做法是保持所有连接的权重系数不变，但是将方向反转，这样一来，每个隐藏单元的输出就会按照已经确定的系数反馈给可见层，可见层的每个神经元接收到的反馈信息是不同隐藏单元输出的线性组合。反馈信息和一组新的偏置分量求和就得到了对原始输入的估计，估计值和原始输入的差值则表示了重构误差。通过让重构误差在可见层和隐藏层之间循环往复地传播，就可以求出使重构误差最小化的一组权重系数。

以上的学习算法就是由辛顿提出的**对比散度（contrastive divergence）方法**，它既能让隐藏层准确地提取可见层的特征，也能根据隐藏层的特征较好地还原出可见层。当隐藏层和可见层的神经元都使用S型函数作为传递函数时，神经元的输出就可以视为单个节点的激活概率。在这种情况下，对比散度方法具体的训练过程包括以下几个步骤：
<div><strong>精选留言（3）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（1） 💬（1）<div>Hinton的”A Practical Guide to Training Restricted Boltzmann Machines”里面提到了”Encouraging sparse hidden activities”。sparsity target p 的值在0.01与0.1的9次方之间。另外有一些文章提到了用regularization的方法得到稀疏的隐藏单元来得到更好的效果。
Yoshua的”Classification using Discriminative Restricted Boltzmann Machines”里的Sparse HDRBM的Error rate也不错。

深度信念网络是由RBM组成的。有一些DBN的应用里面提及了过度完备的自编码器。我的理解是可以融合来提升一些应用场景的解决效果的</div>2018-02-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（1） 💬（0）<div>学习打卡</div>2023-05-13</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（1） 💬（0）<div>极客时间
21天打卡行动 26&#47;21
&lt;&lt;人工智能基础课28&gt;&gt; 深度信念网络
回答老师问题:
自编码器中，稀疏性既可以降低运算量，也能提升训练效果，发挥着重要的作用。那么同样的原理能否应用在受限玻尔兹曼机和深度信念网络之中呢？
在图像识别和人脸识别的应用中有关于稀疏性深度信念网络的报告论文;
受限于基础,无法解读,但粘贴了;
[来源:https:&#47;&#47;www.ixueshu.com&#47;document&#47;994906362d7885de27f1353aa9e7801f318947a18e7f9386.html
http:&#47;&#47;cdmd.cnki.com.cn&#47;Article&#47;CDMD-10151-1017196524.htm]
今天所学:
1,2006 年，深度学习的祖师爷乔弗里·辛顿提出了深度信念网络模型，
2,深度信念网络是一种概率生成模型，能够建立输入数据和输出类别的联合概率分布。
3,从结构上看，复杂的深度信念网络可以看成由若干简单的学习单元构成的整体，而构成它的基本单元就是受限玻尔兹曼机;
4,受限玻尔兹曼机的模型非常简单，就是一个两层的神经网络，包括一个可见层和一个隐藏层。
5,由辛顿提出的对比散度（contrastive divergence）方法，它既能让隐藏层准确地提取可见层的特征，也能根据隐藏层的特征较好地还原出可见层。当隐藏层和可见层的神经元都使用 S 型函数作为传递函数时，神经元的输出就可以视为单个节点的激活概率。
6,对比散度的训练过程本质上是求出一个最符合训练数据集统计特性的概率分布，也就是使训练数据集出现的概率最大的分布参数;
7,将几个受限玻尔兹曼机堆叠在一起，就可以得到深度信念网络（deep belief network）;
8,深度信念网络的无监督预训练也是逐层实现的。
9,栈式自编码器使用 softmax 分类器实现有监督微调，深度信念网络采用的方法则是在最顶层的受限玻尔兹曼机上又添加了额外的反向传播层。反向传播层以受限玻尔兹曼机的输出作为它的输入，执行有监督的训练，再将训练误差自顶向下地传播到每一个受限玻尔兹曼机当中，以实现对整个网络的微调。这也是为什么深度信念网络要在最顶上的两层进行无方向连接的原因。在实际的使用中，用来做微调的网络无需被局限在反向传播上，大部分用于分类的判别模型都能够胜任这个任务。
10,其实相比于深度信念网络这个具体的模型，辛顿的贡献更大程度上在于对深度模型训练方法的改进。不夸张地说，正是这套训练策略引领了近十年深度学习的复兴。这种复兴不仅体现在训练效率的提升上，更体现在研究者对训练机制的关注上;
11,传统的反向传播方法应用于深度结构在原则上是可行的，可实际操作中却无法解决梯度弥散（gradient vanishing）的问题;所谓梯度弥散指的是当误差反向传播时，传播的距离越远，梯度值就变得越小，参数更新的也就越慢;
12,但随着研究的不断深入，事实表明无监督预训练并没有人们想象地那么神奇。良好的初始化策略完全可以比逐层预训练更加高效，而梯度弥散的根源并不是反向传播算法的问题，而是在于非线性传递函数非理想的性质。虽然目前深度信念网络的应用远不如卷积神经网络等其他模型广泛，但它却是一次吃螃蟹的成功尝试。如果没有这次尝试，也许我们依然在单个隐藏层的神经网络中兜兜转转，坐井观天;
重点:
1,深度信念网络是一种生成模型，能够建立输入和输出的联合概率分布；
2,受限玻尔兹曼机是构成深度信念网络的基本单元，是由可见层和隐藏层构成的神经网络；
3,受限玻尔兹曼机的训练方法是对比散度法，通过可见层和隐藏层的多轮交互实现；
4,深度神经网络的通用训练方式是无监督逐层预训练和有监督微调的结合</div>2020-01-13</li><br/>
</ul>