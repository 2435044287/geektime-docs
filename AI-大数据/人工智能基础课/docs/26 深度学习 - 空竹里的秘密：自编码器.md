自编码器（auto-encoder）是一类执行无监督学习任务的神经网络结构，它的目的是学习一组数据的重新表达，也就是编码。

在结构上，自编码器是包含若干隐藏层的深度前馈神经网络，其独特之处是输入层和输出层的单元数目相等；在功能上，自编码器的目的不是根据输入来预测输出，而是重建网络的输入，正是这样的功能将自编码器和其他神经网络区分开来。由于自编码器的图形表示像极了杂技中使用的道具空竹，因而也得了个“空竹网络”的雅号。

**自编码器结构由编码映射和解码映射两部分组成**。如果将编码映射记作$\\phi$，解码映射记作$\\psi$，自编码器的作用就是将输入$\\mathbf{X}$改写为$(\\psi \\circ \\phi) (\\mathbf{X})$，这相当于将输入从一个表象下转换到另一个表象下来表示，就像量子力学中粒子不同表象之间的变化一样。**如果以均方误差作为网络训练中的损失函数，自编码器的目的就是找到使均方误差最小的编解码映射的组合**，即

$$ \\phi, \\psi = \\arg \\min\_{\\phi, \\psi} || \\mathbf{X} - (\\phi \\circ \\psi) (\\mathbf{X}) || ^ 2$$

在最简单的情形，也就是只有一个隐藏层的情形下，自编码器隐藏层的输出就是编码映射。当隐藏层的维度小于输入数据的维度时，这就是个欠完备（undercomplete）的自编码器。欠完备自编码器的作用相当于对输入信号做了主成分分析，隐藏层的$k$个线性神经元在均方误差准则下保留贡献最大的$k$个主成分，原始信号就被投影到由这$k$个主成分所展成的新空间上。在自编码器的另一端，输出层将隐藏层的输出转换为自编码器的整体输出，从而实现了解码映射的功能。

如果隐藏神经元的传递函数是非线性的，编码映射就能够捕捉到输入分布中更加复杂的特征，均方误差准则也可以写成对数似然函数$-\\log p(\\mathbf{X} | \\phi (\\mathbf{X}))$的形式。当误差$p(\\mathbf{X} | \\phi (\\mathbf{X}))$满足高斯分布时，均方误差和最大似然是等价的。

**从信息论的角度看，编码映射可以看成是对输入信源$\\mathbf{X}$的有损压缩**。有损压缩的特点决定了它不可能对所有输入都具有较小的信息量损失，因而学习的作用就是习得在训练数据集上更加精确的映射，并希望这样的映射在测试数据上同样表现良好，也就是使自编码器具有较好的泛化性能。

当自编码器的隐藏单元数目大于输入信号的维度，也就是编码映射的分量数目大于输入信号的分量数目时，这就是个过度完备（overcomplete）的自编码器。过度完备的自编码器面临的一个严重问题是如果没有额外约束的话，那么它可能只能够习得识别功能，得到的编码映射和解码映射都是恒等映射，这显然是白费功夫。
<div><strong>精选留言（6）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（4） 💬（1）<div>基于人工经验的特征提取是基于使用者自己对特征，特征组合(含特征运算)与模型预测关系的理解做的特征处理。好比计算一个图形的各个角的度数，角的数量等来预测是哪种形状的图形。

深度网络的特征提取是尽量让每一层网络抽象出上层数据的共性，基于这些共性做出判断作为下一层网络的输入再推导更抽象的特征。每层能推导出什么特征很多时候不是根据经验或使用者对特征与预测结果能解释清楚的关系来选择的，更多是基于整个网络的表现决定的。好比文中提到的例子里第一层网络是抽象出边缘属性，第二层是抽象出轮廓属性，接下来再推断图形。现实中是不是用3层，每层的输出有什么直观的含义事先我的理解是使用者并不知道。</div>2018-02-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/63/2c/2750bc59.jpg" width="30px"><span>历尽千帆</span> 👍（1） 💬（2）<div>“当误差满足高斯分布时，均方误差和最大似然是等价的”这句话是什么意思呢，王老师？均方误差是怎么和最大似然等价的，一脸茫然</div>2019-01-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/63/2c/2750bc59.jpg" width="30px"><span>历尽千帆</span> 👍（2） 💬（0）<div>数据可视化的数据降噪和降维被认为是自编码器的两个主要的实际应用。</div>2019-01-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-05-11</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（0） 💬（0）<div>极客时间
21天打卡行动 24&#47;21
&lt;&lt;人工智能基础课26&gt;&gt;空竹里的秘密：自编码器
回答老师问题:
深度神经网络既可以用于分类，也可以用于特征提取，而自编码器恰恰具有提取特征的作用。那么如何看待基于深度学习的特征提取和基于人工经验的特征提取之间的区别呢？
我的理解是alphago zero和alphago 的区别,一个是在于让机器自己感知更多的维度,另一个是
人工干预的结果;
今日所学:
1,自编码器（auto-encoder）是一类执行无监督学习任务的神经网络结构，它的目的是学习一组数据的重新表达，也就是编码。
2,自编码器结构由编码映射和解码映射两部分组成;
3,如果以均方误差作为网络训练中的损失函数，自编码器的目的就是找到使均方误差最小的编解码映射的组合;
4,从信息论的角度看，编码映射可以看成是对输入信源 X 的有损压缩。有损压缩的特点决定了它不可能对所有输入都具有较小的信息量损失，因而学习的作用就是习得在训练数据集上更加精确的映射，并希望这样的映射在测试数据上同样表现良好，也就是使自编码器具有较好的泛化性能。
5,在实际中，训练深度自编码器的普遍策略是先训练一些浅层自编码器，再利用这些浅层自编码器贪心地预训练深度结构，因而浅层自编码器可以看作深度自编码器的中间件。
6,用浅层自编码器搭建成的深度自编码器被称为栈式自编码器（stacked autoencoder）。
7,栈式自编码器的训练策略可以归结为两个步骤：无监督预训练 + 有监督微调;
8,自编码器面对的一个问题是，对输入信号什么样的表达才能称为好的表达呢？同深度学习领域中的大多数问题一样，这个问题也不存在标准答案。从不同的角度回答它，得到的就是对原始自编码器的不同改进。前文中提到的过度完备的自编码器就是改进之一。由于在高维的隐藏层中，大部分神经元是被抑制的，只有少数能够输出特征表达，因而这类结构又被称为稀疏自编码器（sparse autoencoder）;
9,逆概率最大化问题可以转化为负对数似然的最小化，并利用基于梯度的方法求解。另一种训练方法是得分匹配，
10,去噪自编码器的作用是对抗信号中的噪声，收缩自编码器（contractive autoencoder）的作用则是对抗信号中的微小扰动，这可以通过在损失函数中添加显式的正则化项来实现。正则化项首先要计算隐藏层输出值关于权重的雅各比矩阵（Jacobi matrix），再来计算得到的雅各比矩阵的 Frobenius 范数，也就是矩阵所有元素平方和的平方根。通过雅各比矩阵和 F 范数的计算，收缩自编码器就能抑制训练样本在低维度流形曲面上的扰动。
11,由于均方误差只适用于描述不同数值之间的差异，而不能描述不同概率分布的区别，因而变分自编码器同样使用 KL 散度作为误差度量。在有限的训练数据集上，变分自编码器能够学习到样本的概率分布，并利用这个概率分布进一步生成新的样本，因而可以作为生成模型使用。
重点提取:
1,自编码器是一种无监督学习方式，目的在于学习数据的重新表达；
2,多个浅层自编码器级联可以得到深度的栈式自编码器，并使用无监督预训练结合有监督微调的方式加以训练；
3,稀疏自编码器利用稀疏的高维表达提取出训练集中隐含的统计规律；
4,变分自编码器对隐藏层做参数化处理，可以用于学习数据的生成模型。</div>2020-01-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/f7/77/0b774a80.jpg" width="30px"><span>大聪小才</span> 👍（0） 💬（0）<div>如人眼的可视摄入，人脑的个性过滤，人嘴的个性化表达?只是这里的个性变成机器的单一功能的发挥。</div>2018-02-15</li><br/>
</ul>