1963年，在前苏联莫斯科控制科学学院攻读统计学博士学位的弗拉基米尔·瓦普尼克和他的同事阿列克谢·切尔沃宁基斯共同提出了支持向量机算法，随后几年两人又在此基础上进一步完善了统计学习理论。可受当时国际环境的影响，这些以俄文发表的成果并没有得到西方学术界的重视。直到1990年，瓦普尼克随着移民潮到达美国，统计学习理论才得到了它应有的重视，并在二十世纪末大放异彩。瓦普尼克本人也于2014年加入Facebook的人工智能实验室，并获得了包括罗森布拉特奖和冯诺伊曼奖章等诸多个人荣誉。

具体说来，**支持向量机是一种二分类算法，通过在高维空间中构造超平面实现对样本的分类**。最简单的情形是训练数据线性可分的情况，此时的支持向量机就被弱化为线性可分支持向量机，这可以视为广义支持向量机的一种特例。

线性可分的数据集可以简化为二维平面上的点集。在平面直角坐标系中，如果有若干个点全部位于$x$轴上方，另外若干个点全部位于$x$轴下方，这两个点集就共同构成了一个线性可分的训练数据集，而$x$轴就是将它们区分开来的一维超平面，也就是直线。

如果在上面的例子上做进一步的假设，假定$x$轴上方的点全部位于直线$y = 1$上及其上方，$x$轴下方的点全部位于直线$y = -2$上及其下方。如此一来，任何平行于$x$轴且在(-2, 1)之间的直线都可以将这个训练集分开。那么问题来了：在这么多划分超平面中，哪一个是最好的呢？
<div><strong>精选留言（12）</strong></div><ul>
<li><img src="" width="30px"><span>geoxs</span> 👍（2） 💬（1）<div>关于棋盘的例子，我觉得这样说更好一点:棋盘的棋子本来没有颜色，所以厮杀后就无法分类了，这时候加上一个颜色维度，人类就可以看一眼就对棋子进行准确的分类</div>2019-01-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/08/d6/4b0506bb.jpg" width="30px"><span>lonelyandrew</span> 👍（1） 💬（1）<div>软间隔最大化下的约束条件，第二个不等式，≤右侧的表达式是否应该为-1＋&#47;xi_i？</div>2018-06-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/58/f7/22ea9761.jpg" width="30px"><span>wolfog</span> 👍（1） 💬（1）<div>王老师您第九段的那两个公式(WTX+B&gt;=1).我在看其他资料的时候,假设函数间隔为1,所以就有了y(WTX+B)&gt;=1(根据定义，函数间隔是y(WTX+B)的最小值，而y是结果标签，只能取1或者-1）所以根据不等式的运算WTX+B要么大于等于1或者小于等于-1吧</div>2018-02-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/93/41/abb7bfe3.jpg" width="30px"><span>陈邓~cd</span> 👍（0） 💬（1）<div>“通过合理设置参数 w和 b
，可以使每个样本点到最优划分超平面的距离都不小于 -1，”最末尾，距离是不小于1？</div>2018-06-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/52/46/dffc60d2.jpg" width="30px"><span>凛</span> 👍（0） 💬（1）<div>感觉数学底子还不够，有点吃不太懂，得再学习学习...不过特别喜欢核函数这个东西，从低维升级到高维，解决线性不可分的问题。至少我先学习下这种解决问题的思路先，生活中也是，很多事情上升个维度思考，问题就很简单了。不过如果老师能再讲更细一些，比如画一些图之类的，然后简单推一下公式。如果还能加一个推导过程的视频就太好了，哈哈哈😂 </div>2018-01-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/8e/8b/38b93ca0.jpg" width="30px"><span>听天由己</span> 👍（16） 💬（5）<div>昨天还说要有抽象思维能力，今天的支持向量机就是直观的考验了。就像之前的同学说的，如果老师能够加上图其实就能够理解内涵了。

从低维到高维，这就是空间构建的方法。支持向量是最优分界线上的边缘样本，而机是机器学习的算法，全称为 Support Vectors Machines。

在知乎和其他资料上有较为清楚的解释。以下是链接，和我一样的不懂的同学请戳。

https:&#47;&#47;www.zhihu.com&#47;question&#47;21094489</div>2018-01-26</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（3） 💬（0）<div>21天打卡行动 10&#47;21
&lt;&lt;人工智能基础课12&gt;&gt;支持向量机
回答老师问题:支持向量机主要用于解决分类任务，那么它能否推而广之，用于解决回归任务呢？在回归任务中，支持向量又应该如何表示呢？
经查资料分享:支持向量回归叫SVR,obj=C(i=1∑l(ξ+ξ∗)+2C1ωTω）,支持向量机回归模型基于不同的损失函数产生了很多变种,个理解,SVR模型中要加损失厌恶的,找到一个分离超平面(超曲面)，使得期望风险最小。
今日所学:
1,支持向量机的由来:1963 年，在前苏联莫斯科控制科学学院攻读统计学博士学位的弗拉基米尔·瓦普尼克和他的同事阿列克谢·切尔沃宁基斯共同提出了支持向量机算法;
2,支持向量机是一种二分类算法，通过在高维空间中构造超平面实现对样本的分类;
3,线性可分支持向量机就是在给定训练数据集的条件下，根据间隔最大化学习最优的划分超平面的过程
4,测试距离是个归一化的距离，因而被称为几何间隔,这个距离是非归一化的距离，被称为函数间隔
5,函数间隔和几何间隔的区别就在于未归一化和归一化的区别。
6,线性可分支持向量机是使硬间隔最大化的算法;
7,线性支持向量机的通用性体现在将原始的硬间隔最大化策略转变为软间隔最大化;
8,误分类引入惩罚函数,
9,不论是线性可分支持向量机还是线性支持向量机，都只能处理线性问题，对于非线性问题则无能为力;
10,将原始低维空间上的非线性问题转化为新的高维空间上的线性问题，这就是核技巧的基本思想;
11,核函数有两个特点。第一，其计算过程是在低维空间上完成的，因而避免了高维空间（可能是无穷维空间）中复杂的计算；第二，对于给定的核函数，高维空间 \mathcal{H} 和映射函数 \phi 的取法并不唯一。一方面，高维空间的取法可以不同；另一方面，即使在同一个空间上，映射函数也可以有所区别;
12,核函数的使用涉及一些复杂的数学问题，其结论是一般的核函数都是正定核函数;
13,在支持向量机的应用中，核函数的选择是一个核心问题,核函数的包含:线性核,多项式核,高斯核,拉普拉斯核,Sigmoid 核,
14,核函数可以将线性支持向量机扩展为非线性支持向量机
15,持向量机的一个重要性质是当训练完成后，最终模型只与支持向量有关，这也是“支持向量机”这个名称的来源。正如发明者瓦普尼克所言：支持向量机这个名字强调了这类算法的关键是如何根据支持向量构建出解，算法的复杂度也主要取决于支持向量的数目;
名词:法向量,截距,低维欧几里得空间,高维希尔伯特空间\mathcal{H},SMO 算法
总结:这一张很抽象,抽象到一个不能用由生物分子合成的三维空间的人去想象那些在多维空间存在一个超平面把我们 要分类的的事物,分出因果后,再通过核函数压缩直至降维;
老师讲的重点:
1,线性可分支持向量机通过硬间隔最大化求出划分超平面，解决线性分类问题；
2,线性支持向量机通过软间隔最大化求出划分超平面，解决线性分类问题；
3,非线性支持向量机利用核函数实现从低维原始空间到高维特征空间的转换，在高维空间上解决非线性分类问题；
4,支持向量机的学习是个凸二次规划问题，可以用 SMO 算法快速求解。
</div>2019-12-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/bc/66/117bdb99.jpg" width="30px"><span>MJ小朋友</span> 👍（1） 💬（0）<div>老师讲的很不错，另外我又看了书上关于对偶拉格朗日的引入解参数w和b</div>2018-01-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-05-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/22/57/77/e094e9a9.jpg" width="30px"><span>Geek_HanX2</span> 👍（0） 💬（0）<div>思路非常清楚！是一份很好的学习提纲！</div>2023-02-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/f5/73/f7d3a996.jpg" width="30px"><span>！null</span> 👍（0） 💬（0）<div>wT⋅x+b=0
w是法向量，T是啥？</div>2021-08-30</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/8OPzdpDraQMvCNWAicicDt54sDaIYJZicBLfMyibXVs4V0ZibEdkZlbzxxL7aGpRoeyvibag5LaAaaGKSdwYQMY2hUrQ/132" width="30px"><span>code2</span> 👍（0） 💬（0）<div>把公式重排一下版，显示出来的很杂乱，看不清楚。</div>2019-02-10</li><br/>
</ul>