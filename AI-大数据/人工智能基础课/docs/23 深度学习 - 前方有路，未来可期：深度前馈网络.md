深度前馈网络（Deep Feedforward Network）是具有深度结构的前馈神经网络，可以看成是进化版的多层感知器。与只有一个或两个隐藏层的浅层网络相比，深度前馈网络具有更多的隐藏层数目，从而具备了更强的特征提取能力。

深度前馈网络不考虑输入数据可能具备的任何特定结构，也就是不使用关于数据的先验信息。但特征提取能力增强的代价是运算复杂度的提升。因而，**网络架构的建立、损失函数的选择、输出单元和隐藏单元的设计、训练误差的处理等问题就成为深度前馈网络设计中的一系列核心问题**。

**在深度前馈网络的设计中，确定架构是首要考虑的关键问题**。架构决定着网络中包含多少基本单元，以及这些基本单元之间如何相互连接。几乎所有前馈网络采用的都是链式架构，即前一层的输出是后一层的输入。在这样的链式架构中，层的数目和每一层中神经元的数目就是网络的主要变量。

介绍多层感知器时我曾提到了通用逼近的性质，这个性质的严格形式是通用逼近定理。**通用逼近定理的内容是如果一个前馈网络具有单个隐藏层，这个隐藏层又有足够但是有限数目的神经元，这个神经网络就可以以任意精度逼近任意连续函数**。虽然在这个定理的初始证明中，隐藏神经元的传递函数是具有“挤压”性质的非线性函数，但定理的成立性实际上并不取决于传递函数的性质，而是由网络的前馈架构所决定的。

通用逼近定理是一个存在性定理，它说明需要的神经网络是肯定存在的，却并没有指明具体的构造方法。所以在给定一个目标函数时，我们可以确定单隐藏层的感知器一定能够将它表示出来，却对隐藏层需要多少神经元毫无把握。这个数目很可能是个天文数字，这会让网络结构在计算机上根本无法实现。即使能够设计出这么复杂的算法，要对它进行训练和泛化也近乎天方夜谭。
<div><strong>精选留言（6）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（2） 💬（1）<div>传统的前馈网络没有时间顺序的概念。循环网络中的每个输入样本之间则有时间顺序的概念，循环网络可以沿时间反向传播。

LSTM在时间序列上通过输入们，输出门，遗忘门的组合与控制来让更久远的时间点的信息能更明显地影响之后时间点的输出。</div>2018-01-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（1） 💬（0）<div>学习打卡</div>2023-05-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/d6/46/5eb5261b.jpg" width="30px"><span>Sudouble</span> 👍（1） 💬（0）<div>非常生动的比喻，隐含层的设计主要靠经验，相当于数据的炼金术</div>2022-11-13</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（1） 💬（0）<div>极客时间
21天打卡行动 21&#47;21
&lt;&lt;人工智能基础课23&gt;&gt;深度前馈网络
回答老师问题:
在深度前馈网络中，深度结构是在空间维度上展开的，那么能否在时间维度上引入类似的结构，实现不同时刻之间的信息流动呢？
神经网络模型:1、卷积神经网络（CNN）2、循环神经网络（Recurrent Neural Network，RNN）3、双向长短时记忆循环神经网络详解（Bi-directional LSTM RNN）4、长短期记忆模型（LSTM）5、MLP（多层神经网络）
,其中,RNN,LSTM RNN,LSTM,应该是时间维度上的,而今天学的深度前馈网络应用模型应该是CNN
[资料来源:https:&#47;&#47;www.jianshu.com&#47;p&#47;c8a46f00b06d]
今日所学:
1,深度前馈网络（Deep Feedforward Network）是具有深度结构的前馈神经网络，可以看成是进化版的多层感知器。与只有一个或两个隐藏层的浅层网络相比，深度前馈网络具有更多的隐藏层数目，从而具备了更强的特征提取能力;
2,网络架构的建立、损失函数的选择、输出单元和隐藏单元的设计、训练误差的处理等问题就成为深度前馈网络设计中的一系列核心问题。
3,在深度前馈网络的设计中，确定架构是首要考虑的关键问题。架构决定着网络中包含多少基本单元，以及这些基本单元之间如何相互连接。几乎所有前馈网络采用的都是链式架构，即前一层的输出是后一层的输入。在这样的链式架构中，层的数目和每一层中神经元的数目就是网络的主要变量。
4,通用逼近定理的内容是如果一个前馈网络具有单个隐藏层，这个隐藏层又有足够但是有限数目的神经元，这个神经网络就可以以任意精度逼近任意连续函数;
5,深度前馈网络的出现克服的正是单隐藏层带来的复杂性问题：使用深度架构的模型既能减少表示目标函数时所需要的单元数量，也能有效降低泛化误差，在一定程度上抑制过拟合的发生。
6,待学习的复杂函数可以视为若干简单函数的层次化结合;
7,任何机器学习算法都可以看成是对某个预设函数的最优化方法，深度前馈网络也不例外;
8,在学习中，损失函数的选择是深度神经网络设计中另一个重要环节。深度前馈网络选择损失函数的准则与其他机器学习算法并无二致：回归问题的损失函数通常是最小均方误差，而分类问题的损失函数通常是交叉熵（Cross-Entropy）;
9,其实无论是最小均方误差还是交叉熵，体现的都是概率论中最大似然估计的原理;
10,损失函数的表示与输出单元的选择密切相关，输出单元的传递函数决定了交叉熵的具体表达式。输出层的作用是对隐藏层提取出的特征施加额外的变换以得到输出，变换的形式则有多种选择。最简单的变换形式就是线性变换，它将隐藏特征的线性组合作为输出，简单而实用。
11,而在深层前馈网络的设计中，一个独有的问题就是隐藏单元的设计，也就是隐藏神经元的传递函数如何选择。
关键字:梯度信息,反向传播方法,随机梯度下降法,对数几率函数,softmax 函数(柔性最大值函数),整流线性单元,渗漏整流单元（Leaky ReLU）,指数整流单元,对数几率函数,双曲正切函数

总结重点:
1,深度前馈网络利用深度架构实现工程上可实现的对任意函数的通用逼近；
2,深度前馈网络使用梯度下降的方法进行学习；
3,深度前馈网络的损失函数通常是交叉熵或最小均方误差；
4,深度前馈网络的隐藏神经元通常使用整流线性单元作为传递函数。</div>2020-01-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/80/04/e6989d2a.jpg" width="30px"><span>极客时间攻城狮。</span> 👍（0） 💬（0）<div>学习了</div>2018-01-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/80/04/e6989d2a.jpg" width="30px"><span>极客时间攻城狮。</span> 👍（0） 💬（0）<div>好</div>2018-01-30</li><br/>
</ul>