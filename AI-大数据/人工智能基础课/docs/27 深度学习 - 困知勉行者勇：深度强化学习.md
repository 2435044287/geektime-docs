在2017年新鲜出炉的《麻省理工科技评论》十大突破性技术中，“强化学习”榜上有名。如果把时钟调回到一年多之前的围棋人机大战，彼时的深度强化学习在AlphaGo对李世乭的横扫中就已经初露峥嵘。而在进化版AlphaGo Zero中，深度强化学习更是大放异彩，AlphaGo Zero之所以能够摆脱对人类棋谱的依赖，其原因就在于使用纯粹的深度强化学习进行端到端的自我对弈，从而超越了人类的围棋水平。

要介绍深度强化学习就不得不先说一说强化学习的故事。相比于纯人造的监督学习和无监督学习，强化学习的思想根源来自于认知科学。20世纪初，美国心理学家爱德华·桑代克在对教育过程的研究中提出了强化学习的原始理论，而作为人工智能方法的强化学习则力图使计算机在没有明确指导的情况下实现自主学习，完成从数据到决策的转变。

**强化学习（reinforcement learning）实质上是智能系统从环境到行为的学习过程，智能体通过与环境的互动来改善自身的行为，改善准则是使某个累积奖励函数最大化**。具体来说，强化学习是基于环境反馈实现决策制定的通用框架，根据不断试错得到来自环境的奖励或者惩罚，从而实现对趋利决策信念的不断增强。它强调在与环境的交互过程中实现学习，产生能获得最大利益的习惯性行为。

强化学习的特点在于由环境提供的强化信号只是对智能体所产生动作的好坏作一种评价，和监督学习中清晰明确的判定结果相比，环境的反馈只能提供很少的信息。所以强化学习需要在探索未知领域和遵从已有经验之间找到平衡。一方面，智能体要在陌生的环境中不断摸着石头过河，来探索新行为带来的奖励；另一方面，智能体也要避免在探索中玩儿脱，不能放弃根据已有经验来踏踏实实地获得最大收益的策略。

**描述强化学习最常用的模式是马尔可夫决策过程**（Markov decision process）。马尔可夫决策过程是由离散时间随机控制的过程，可以用以下的四元组来定义

- $S$：由智能体和环境所处的所有可能状态构成的有限集合
- $A$：由智能体的所有可能动作构成的有限集合
- $P\_a(s, s') = \\text{Pr}(s\_{t + 1} = s' | s\_t = s, a\_t = a)$：智能体在$t$时刻做出的动作$a$使马尔可夫过程的状态从$t$时刻的$s$转移为$t + 1$时刻的$s'$的概率
- $R\_a(s, s')$：智能体通过动作$a$使状态从$s$转移到$s'$得到的实时奖励
<div><strong>精选留言（6）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（3） 💬（1）<div>据说AlphaGo Zero是将策略网络和价值网络合并成一个神经网络。</div>2018-02-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/f7/77/0b774a80.jpg" width="30px"><span>大聪小才</span> 👍（2） 💬（1）<div>突破奇点后，比人还聪明的agent，一定掌握了上文中的招数。如果我们想让一个agent降低一点&quot;智商&quot;，引出一个问题:上文中的招数可逆吗？</div>2018-02-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/63/2c/2750bc59.jpg" width="30px"><span>历尽千帆</span> 👍（0） 💬（1）<div>经验回放能够克服数据之间的相关性，避免网络收敛到局部极小值。
为什么经验回放能够做到这些呢？希望老师解答</div>2019-02-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/71/45/abb7bfe3.jpg" width="30px"><span>Andy</span> 👍（0） 💬（1）<div>王老师您好，上述强化学习中的Pa 是智能体在 t时刻做出的动作 a 使马尔可夫过程的状态从 t时刻的 ss 转移为 t+1 时刻的 s′的概率

请问这个概率是否包含智能体选择动作a的概率呢？还是说每次选择的都是特定的a?</div>2018-04-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（1） 💬（0）<div>学习打卡</div>2023-05-12</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（1） 💬（0）<div>极客时间
21天打卡行动 25&#47;21
&lt;&lt;人工智能基础课27&gt;&gt; 困知勉行者勇：深度强化学习
回答老师问题:
深度强化学习的三种实现方式各具特色，各有千秋，那么能不能将它们优势互补，从而发挥更大的作用呢？
人工智能领域大多应用的应该是深度强化学习吧;
今日所学:
1,强化学习（reinforcement learning）实质上是智能系统从环境到行为的学习过程，智能体通过与环境的互动来改善自身的行为，改善准则是使某个累积奖励函数最大化
2,强化学习的特点在于由环境提供的强化信号只是对智能体所产生动作的好坏作一种评价，和监督学习中清晰明确的判定结果相比，环境的反馈只能提供很少的信息。所以强化学习需要在探索未知领域和遵从已有经验之间找到平衡。一方面，智能体要在陌生的环境中不断摸着石头过河，来探索新行为带来的奖励；另一方面，智能体也要避免在探索中玩儿脱，不能放弃根据已有经验来踏踏实实地获得最大收益的策略;
3,描述强化学习最常用的模式是马尔可夫决策过程（Markov decision process）。马尔可夫决策过程是由离散时间随机控制的过程;
4,深度强化学习（deep reinforcement learning）是深度学习和强化学习的结合，它将深度学习的感知能力和强化学习的决策能力熔于一炉，用深度学习的运行机制达到强化学习的优化目标，从而向通用人工智能迈进;
5,深度强化学习方法可以分成三类，分别是基于价值、基于策略和基于模型的深度强化学习。
6,基于价值（value-based）的深度强化学习的基本思路是建立一个价值函数的表示;
7,在没有“深度”的强化学习中，使用价值函数的算法叫做 Q 学习算法（Q-learning）;
8,基于策略（strategy-based）的深度强化学习的基本思路就是直接搜索能够使未来奖励最大化的最优策略;
9,策略梯度方法的思想是直接使用逼近函数来近似表示和优化策略，通过增加总奖励较高情况的出现概率来逼近最优策略。其运算方式和深度学习中的随机梯度下降法类似，都是在负梯度的方向上寻找最值，以优化深度网络的参数。
10,一种实用的策略梯度方法是无监督强化辅助学习（UNsupervised REinforcement and Auxiliary Learning），简称UNREAL 算法。UNREAL 算法的核心是行动者 - 评论家（actor-critic）机制，两者分别代表两个不同的网络。
11,基于模型（model-based）的深度强化学习的基本思路是构造关于环境的模型，再用这个模型来指导决策。
重点提纯:
1,深度强化学习是深度学习和强化学习的结合，有望成为实现通用人工智能的关键技术；2,基于价值的深度强化学习的基本思路是建立价值函数的表示，通过优化价值函数得到最优策略；
3,基于策略的深度强化学习的基本思路是直接搜索能够使未来奖励最大化的最优策略；
4,基于模型的深度强化学习的基本思路是构造关于环境的转移概率模型，再用这个模型指导策略。</div>2020-01-12</li><br/>
</ul>