数学中的线性模型可谓“简约而不简单”：它既能体现出重要的基本思想，又能构造出功能更加强大的非线性模型。在机器学习领域，线性回归就是这样一类基本的任务，它应用了一系列影响深远的数学工具。

在数理统计中，回归分析是确定多种变量间相互依赖的定量关系的方法。**线性回归假设输出变量是若干输入变量的线性组合，并根据这一关系求解线性组合中的最优系数**。在众多回归分析的方法里，线性回归模型最易于拟合，其估计结果的统计特性也更容易确定，因而得到广泛应用。而在机器学习中，回归问题隐含了输入变量和输出变量均可连续取值的前提，因而利用线性回归模型可以对任意输入给出对输出的估计。

1875年，从事遗传问题研究的英国统计学家弗朗西斯·高尔顿正在寻找父代与子代身高之间的关系。在分析了1078对父子的身高数据后，他发现这些数据的散点图大致呈直线状态，即父亲的身高和儿子的身高呈正相关关系。而在正相关关系背后还隐藏着另外一个现象：矮个子父亲的儿子更可能比父亲高；而高个子父亲的儿子更可能比父亲矮。

受表哥查尔斯·达尔文的影响，高尔顿将这种现象称为“**回归效应**”，即大自然将人类身高的分布约束在相对稳定而不产生两极分化的整体水平，并给出了历史上第一个线性回归的表达式：y = 0.516x + 33.73，式中的y和x分别代表以英寸为单位的子代和父代的身高。

高尔顿的思想在今天的机器学习中依然保持着旺盛的生命力。假定一个实例可以用列向量${\\bf x} = (x\_1; x\_2; \\cdots, x\_n)$表示，每个$x\_i$代表了实例在第i个属性上的取值，线性回归的作用就是习得一组参数$w\_i, i = 0, 1, \\cdots, n$，使预测输出可以表示为以这组参数为权重的实例属性的线性组合。如果引入常量$x\_0 = 1$，线性回归试图学习的模型就是

$$f({\\bf x}) = {\\bf w} ^ T {\\bf x} = \\sum\\limits\_{i = 0}^{n} w\_i \\cdot x\_i $$

当实例只有一个属性时，输入和输出之间的关系就是二维平面上的一条直线；当实例的属性数目较多时，线性回归得到的就是n+1维空间上的一个超平面，对应一个维度等于n的线性子空间。

在训练集上确定系数$w\_i$时，预测输出$f({\\bf x})$和真实输出$y$之间的误差是关注的核心指标。在线性回归中，这一误差是以**均方误差**来定义的。当线性回归的模型为二维平面上的直线时，均方误差就是预测输出和真实输出之间的**欧几里得距离**，也就是两点间向量的$L ^ 2$范数。而以使均方误差取得最小值为目标的模型求解方法就是**最小二乘法**，其表达式可以写成

$${\\mathbf{w}}^* = \\mathop {\\arg \\min }\\limits\_{\\mathbf{w}} \\sum\\limits\_{k = 1} {{{({{\\mathbf{w}}^T}{{\\mathbf{x}}\_k} - {y\_k})}^2}} $$

$$= \\mathop {\\arg \\min }\\limits\_{\\mathbf{w}} \\sum\\limits\_{k = 1} || y\_k - \\mathbf{w}^T \\mathbf{x}\_k ||^2 $$

式中每个${\\bf x}\_k$代表训练集中的一个样本。**在单变量线性回归任务中，最小二乘法的作用就是找到一条直线，使所有样本到直线的欧式距离之和最小**。

说到这里，问题就来了：凭什么使均方误差最小化的参数就是和训练样本匹配的最优模型呢？
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/5e/90/5f79859b.jpg" width="30px"><span>Maiza</span> 👍（23） 💬（1）<div>老师 每次看到公式的地方就跪了...
能麻烦给每个公式标明下出处，方便理解吗？
老师认为理所当然的事情对小白来说就是天书啊 。。。。。😂😂😂
</div>2018-01-06</li><br/><li><img src="" width="30px"><span>wdf</span> 👍（5） 💬（1）<div> 为什么说ringe对应的是正态，lasso是拉普拉斯分布</div>2018-07-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/42/af/8c37ca95.jpg" width="30px"><span>haiker</span> 👍（3） 💬（2）<div>LASSO 回归感觉也是在做特征降维，会不会和做完特征降维之后再做线性回归效果差不多呢？</div>2018-10-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/08/ae/e3649ed6.jpg" width="30px"><span>Haley_Hu</span> 👍（2） 💬（1）<div>2范数是不是就指L2正则 1范数就指L1正则</div>2018-02-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/42/af/8c37ca95.jpg" width="30px"><span>haiker</span> 👍（1） 💬（1）<div>引入抑制过拟合现象，以训练误差的上升为代价，换取测试误差的下降，训练误差和测试误差有时候是不是鱼和熊掌，不可兼得，训练误差太低可能就过拟合了，在测试集上效果就不好了。有些学者建议在训练集上训练的时候要等到稍微过拟合了再结束，因为提前结束的话，可能模型还没训练到位。</div>2018-10-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/be/02/43202976.jpg" width="30px"><span>w 🍍</span> 👍（0） 💬（1）<div>应该是”线性回归得到的就是 n+1 维空间上的一个超平面，对应一个维度等于 n 的线性子空间。“吧，还是我理解不对...
</div>2019-08-11</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/AoXC8Dwxrs9SKJicxdoiav3pYibibjJeFm2dwDiaK4ly7Jk7ZtPh9J8JzyBibR3xBohvz4NvKlhibn8DlV0Grwv8FNhqg/132" width="30px"><span>全全</span> 👍（0） 💬（1）<div>天一老师，讲的真好！有些内容您可以加上一些图片帮助理解，比如讲特征值分解那里，变换是由矩阵引起的，特征值和特征向量，您用文字解释的非常明白，我又看维基百科里给了一个动图，觉得豁然开朗。还有这次课里那几个范数应用在回归正则化里，也有图片可以帮助理解。有时候学习这个东西，在我完全不懂的时候，解释的多明白都是天书的感觉。只有懂了，再看的时候就有共鸣，看出您文字里背后的意思。这是我的体会。
那么天一老师，我的困惑在于，用1范数可以做稀疏，同样是有棱有角的无穷范数，是不是也可以作为正则化的约束条件来实现稀疏呢？
可以么？用无穷范数来实现稀疏？直观上看是可以的，如果不可以是因为什么呢？希望老师指点
另外如果可以的话，那无穷范数实现的稀疏和1范数所实现的稀疏比较而言，有什么优缺点呢？
跟您学，有收获，真心求教！</div>2019-03-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/ac/5f/894761f8.jpg" width="30px"><span>十八哥</span> 👍（0） 💬（1）<div>假定给出美女的标准，数据化。问题提出如何确定美女在泳池中出现的概率？模型输入，地区、区域房价、游泳单价、时间维度、年龄参数等。此时我们可以用线性回归找到这些模型中那个变量是最优的，并且能给出排序。我的理解对吗？</div>2018-12-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/42/af/8c37ca95.jpg" width="30px"><span>haiker</span> 👍（0） 💬（1）<div>季霍诺夫矩阵是超参数要提前指定，还是参数，在训练过程中获得呢？</div>2018-10-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/57/6e/f7fa7278.jpg" width="30px"><span>Howard.Wundt</span> 👍（0） 💬（1）<div>老师的数学公式是用什么工具写的，可以具体分享一下吗？</div>2018-09-22</li><br/><li><img src="" width="30px"><span>wdf</span> 👍（0） 💬（1）<div>请问老师，如果是多元回归，假定噪声服从高斯分布极大似然估计和最小二乘法等价吗</div>2018-07-07</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/mticjmrhDj380Cb21EEz89wa04HF5ia9ze1icZIKGM4JBgkXrVosAHrXFNeGKickHGXAiaF3PzPqwHiaaggsjokTDh3A/132" width="30px"><span>duchao_hit</span> 👍（0） 💬（1）<div>老师，对误差的概率就等于在参数w下样本的条件概率觉得不是很理解</div>2018-05-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/b0/4e/937ee2ca.jpg" width="30px"><span>刘滨</span> 👍（0） 💬（1）<div>老师，请问最小二乘法跟梯度下降方法有什么区别？这里可以用梯度下降方法吗</div>2018-01-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/58/f7/22ea9761.jpg" width="30px"><span>wolfog</span> 👍（0） 💬（1）<div>天一老师有段，开头是“LASSO回归的全称最小绝对缩减和选择算子”这一段的倒数第二行的2泛数项和一泛数项写法是否应该统一，要么数字都在右下角，要么都在右上角。今天后面这个惩罚项目不太了解为什么惩罚项目一加，就可以降低了他的过拟合。</div>2018-01-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/71/45/abb7bfe3.jpg" width="30px"><span>Andy</span> 👍（0） 💬（2）<div>最小二乘的形式，为何跟极大似然估计后的形式一致呢？是巧合吗？</div>2017-12-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg" width="30px"><span>Simon</span> 👍（7） 💬（0）<div>“从几何意义出发的最小二乘法与从概率意义出发的最大似然估计是等价的。” 这是新收获</div>2020-03-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/ff/79/3b38c9e1.jpg" width="30px"><span>nil</span> 👍（4） 💬（0）<div>概率论真是个厉害的工具，从另一个方面提供了看问题的视角，真所谓横看成林侧成峰，殊途同归。</div>2020-04-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/a7/ea/75b48440.jpg" width="30px"><span>Jin</span> 👍（3） 💬（1）<div>老师，“回归”这个词的来历讲解有些不准确，我之前也不明白为啥叫回归，后来看了aurelien 的hands-on machine learning 这本书才知道，来源于统计学，就是您说的那个例子，但是解释不是您说的那样，而是因为高个子人的孩子个子没父母高，我查了词典regression就是有退化的含义。这样我记住了为啥叫回归。谢谢！</div>2020-02-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/52/95/abb7bfe3.jpg" width="30px"><span>秦龙君</span> 👍（2） 💬（0）<div>学习了。</div>2017-12-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/27/41/0d/99312186.jpg" width="30px"><span>小龚小龚 马到成功 🔥</span> 👍（0） 💬（0）<div>我直接看的人过去了</div>2023-10-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-04-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/27/f0/f2/d8624dce.jpg" width="30px"><span>早八很难不迟到</span> 👍（0） 💬（0）<div>1. 最小二乘法在误差遵循正态分布的情况下能求得最优模型
2. 正则化有两种方式，二范数和一范数
3. 一范数正则化的特点能够有效减少参数数量，减少无用特征，保留主要特征</div>2022-08-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/fe/2d/e23fc6ee.jpg" width="30px"><span>深水蓝</span> 👍（0） 💬（1）<div>请问老师 LASSO算子的公式末尾，λ∣∣w∣∣1​ 的这个下标 1 是什么意思啊？</div>2021-10-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/2c/cd/b88a0922.jpg" width="30px"><span>王  明</span> 👍（0） 💬（0）<div>“单个样本 xk​ 出现的概率实际上就是噪声等于 yk​−f(xk​) 的概率”
请问老师这句怎么理解啊？</div>2021-02-24</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/B9vSOjMc2a86kYA8R5yDkVdFiaj2JeBZ1PuI9oUKhbnvuZwuibdUam6FTcGzDaiaFdk2GWJveUGhfCVpv4KaOdicoQ/132" width="30px"><span>帝江</span> 👍（0） 💬（0）<div>唉...听不懂...听不懂...学习这个章节需要什么基础知识啊?</div>2021-02-23</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（0） 💬（0）<div>岭回归和 LASSO 回归分别通过引入二范数惩罚项和一范数惩罚项抑制过拟合。</div>2019-12-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/c7/d9/9fc367b5.jpg" width="30px"><span>Snail@AI_ML</span> 👍（0） 💬（0）<div>和学习的课程相互印证之后，发现居然更糊涂了:这是上面的章节没有的现象，总结来说，本文更详细并做了一些拓展，比如线性回归的来历，正则化的应用等，课程只是告诉我们正则化的特点和应用，简单粗暴呢</div>2019-01-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/63/2c/2750bc59.jpg" width="30px"><span>历尽千帆</span> 👍（0） 💬（1）<div>老师~我不太明白，为什么说LASSO回归的特点在于稀疏性的引入？我不太懂这里说的稀疏性是指的什么~</div>2018-12-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/63/2c/2750bc59.jpg" width="30px"><span>历尽千帆</span> 👍（0） 💬（1）<div>您好，我没有明白，为什么引入常量 x0=1，后面的y=wx才成立呢？</div>2018-12-26</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/mticjmrhDj380Cb21EEz89wa04HF5ia9ze1icZIKGM4JBgkXrVosAHrXFNeGKickHGXAiaF3PzPqwHiaaggsjokTDh3A/132" width="30px"><span>duchao_hit</span> 👍（0） 💬（0）<div>老师，关于样本x的概率就等于误差的概率不是很理解。疑惑是y-wx=e，只能说p(y-wx)=p(e)，但不能说p(y-wx)=p(x|w)吧？</div>2018-05-25</li><br/>
</ul>