周二我和你分享了机器学习中的线性回归算法，这一算法解决的是从连续取值的输入映射为连续取值的输出的回归问题。今天我分享的算法则用于解决分类问题，即将连续取值的输入映射为离散取值的输出，算法的名字叫作“**朴素贝叶斯方法**”。

解决分类问题的依据是数据的属性。朴素贝叶斯分类器假定样本的不同属性满足条件独立性假设，并在此基础上应用贝叶斯定理执行分类任务。**其基本思想在于分析待分类样本出现在每个输出类别中的后验概率，并以取得最大后验概率的类别作为分类的输出**。

假设训练数据的属性由n维随机向量$\\bf x$表示，其分类结果用随机变量y表示，那么x和y的统计规律就可以用联合概率分布$P(X, Y)$描述，每一个具体的样本$(x\_i, y\_i)$都可以通过$P(X, Y)$独立同分布地产生。

朴素贝叶斯分类器的出发点就是这个联合概率分布，根据条件概率的性质可以得到

$$ P(X, Y) = P(Y) \\cdot P(X|Y)$$

$$= P(X) \\cdot P(Y|X) $$

在上式中，P(Y)代表着每个类别出现的概率，也就是**类先验概率**；P(X|Y)代表着在给定的类别下不同属性出现的概率，也就是**类似然概率**。

先验概率容易根据训练数据计算出来，只需要统计不同类别样本的数目即可。而似然概率受属性取值数目的影响，其估计较为困难。

如果每个样本包含100个属性，每个属性的取值都可能有100种，那么对分类的每个结果，要计算的条件概率数目就是$100 ^ 2 = 10000$。在这么多参数的情况下，对似然概率的精确估计就需要庞大的数据量。

要解决似然概率难以估计的问题，就需要“条件独立性假设”登台亮相。**条件独立性假设保证了所有属性相互独立，互不影响，每个属性独立地对分类结果发生作用**。**这样类条件概率就变成了属性条件概率的乘积**，在数学公式上可以体现为

$$ P(X = {\\bf x}|Y = c) = $$

$$P(X^{(1)} = x^{(1)}, X^{(2)} = x^{(2)}, \\cdots, X^{(n)} = x^{(n)}|Y = c)$$

$$= \\Pi\_{j = 1}^n P(X^{(j)} = x^{(j)}|Y = c) $$

**这正是朴素贝叶斯方法的“朴素”之处，通过必要的假设来简化计算，并回归问题的本质**。
<div><strong>精选留言（18）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/fb/a0/8594f5ed.jpg" width="30px"><span>王杰</span> 👍（10） 💬（2）<div>讲的简洁易懂，回家途中看完了！一个问题：如果样本先降维去除属性相关性再用朴素贝叶斯分类，效果是不是就很好？</div>2018-01-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/22/6c/90527684.jpg" width="30px"><span>杜浩</span> 👍（8） 💬（1）<div>朴素贝叶斯为什么是期望风险最小化的 这点还是不太理解</div>2018-02-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/4e/8e/f4297447.jpg" width="30px"><span>吴文敏</span> 👍（4） 💬（3）<div>如果每个样本包含 100 个属性，每个属性的取值都可能有 100 种，那么对分类的每个结果，要计算的条件概率数目就是 100^2=10000 感觉这里应该是100^100
</div>2018-02-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/52/95/abb7bfe3.jpg" width="30px"><span>秦龙君</span> 👍（3） 💬（3）<div>学习了。我想问老师一个问题，所有文章更新完了，有集结出书的计划吗？我感觉平时看完后，再用书重新看一遍，效果更好。</div>2017-12-29</li><br/><li><img src="" width="30px"><span>隔壁老任</span> 👍（0） 💬（2）<div>老师你好，请教俩问题，1.第一段有句话有点懵，朴素贝叶斯是 将连续输入转化为离散输出么？我目前简单的，感觉都是离散输入到离散输出，属性的取值也多是离散的，如果是连续的，数量就太大了
2.同最后一个问题，因为朴素贝叶斯是用的后验概率相乘，貌似训练一次后，参数就不会变了？后续的的参数更新一般用什么方法呢？
谢谢</div>2018-11-08</li><br/><li><img src="" width="30px"><span>wdf</span> 👍（0） 💬（2）<div>老师如果朴素贝叶斯算法，只在乎分类是否正确。是否他给出的概率值就参考意义不大？如果给出是正立的，赵军，只有一个是0.9，一个是0.6是不是很难说，是有区别的。</div>2018-09-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/2a/dd/aee4d7de.jpg" width="30px"><span>夏震华(围巾)</span> 👍（0） 💬（1）<div>在使用高维数据集时，每个样本都会包含大量的属性，这时属性条件概率连乘的结果会非常接近于零，导致下溢的发生。如何防止因概率过小造成的下溢呢？
都乘个100，放大了，然后到了后面在统一除去?如何</div>2018-03-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/71/45/abb7bfe3.jpg" width="30px"><span>Andy</span> 👍（0） 💬（1）<div>王老师您好，感觉朴素贝叶斯不像逻辑回归那样有个loss func 可以做权重的学习，那么朴素贝叶斯训练好的模型怎么才能持久化呢？</div>2018-01-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/bc/a0/97c7679b.jpg" width="30px"><span></span> 👍（11） 💬（0）<div>大佬，能有简单的项目或习题，让我们实践下不？</div>2017-12-29</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（3） 💬（1）<div>打卡第七天(7&#47;21):
&lt;&lt;人工智能基础课09&gt;&gt;朴素贝叶斯方法
回答老师问题:
在使用高维数据集时，每个样本都会包含大量的属性，这时属性条件概率连乘的结果会非常接近于零，导致下溢的发生。如何防止因概率过小造成的下溢呢？
这样想到,得到&lt;&lt;吴军数学课&gt;&gt;42讲中古德-图灵折扣估计,预防黑天鹅事件.吴军老师讲古德用每一次的概率加一除总数,这样的结果不至于出现为0的情况,但最后估值肯定不准确,用朴素贝叶斯方法做分类应该没问题,我特意重读了吴军老师的课,老师还讲了吴军老师导师创的差值法,这个更接近,更精确,最后还提到备用法;等这些方法,都能最大程度防止因概率过小造成的下溢;我个人看法;
今日所学:朴素贝叶斯方法,将连续取值的输入映射为离散取值的输出，算法的名字叫作“朴素贝叶斯方法”。
其基本思想在于分析待分类样本出现在每个输出类别中的后验概率，并以取得最大后验概率的类别作为分类的输出.
名词:类先验概率,类似然概率(条件独立性假设保证了所有属性相互独立，互不影响，每个属性独立地对分类结果发生作用),这正是朴素贝叶斯方法的“朴素”之处，通过必要的假设来简化计算，并回归问题的本质;
1,从模型最优化的角度观察，朴素贝叶斯分类器是平均意义上预测能力最优的模型，也就是使期望风险最小化;
2,影响朴素贝叶斯的分类的是所有属性之间的依赖关系在不同类别上的分布，而不仅仅是依赖关系本身;
3,半朴素贝叶斯分类器考虑了部分属性之间的依赖关系，既保留了属性之间较强的相关性，又不需要完全计算复杂的联合概率分布。常用的方法是建立独依赖关系：假设每个属性除了类别之外，最多只依赖一个其他属性。由此，根据属性间依赖关系确定方式的不同，便衍生出了多种独依赖分类器。
4,朴素贝叶斯分类器的应用场景非常广泛。它可以根据关键词执行对一封邮件是否是垃圾邮件的二元分类，也可以用来判断社交网络上的账号到底是活跃用户还是僵尸粉。在信息检索领域，这种分类方法尤为实用。总结起来，以朴素贝叶斯分类器为代表的贝叶斯分类方法的策略是：根据训练数据计算后验概率，基于后验概率选择最佳决策。
总结:
1,朴素贝叶斯方法利用后验概率选择最佳分类，后验概率可以通过贝叶斯定理求解；
2,朴素贝叶斯方法假定所有属性相互独立，基于这一假设将类条件概率转化为属性条件概率的乘积；
3,朴素贝叶斯方法可以使期望风险最小化；
4,影响朴素贝叶斯分类的是所有属性之间的依赖关系在不同类别上的分布。</div>2019-12-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/ec/3e/885ec1d2.jpg" width="30px"><span>宋不肥</span> 👍（3） 💬（0）<div>概率取对数，把连乘变成连加</div>2019-07-04</li><br/><li><img src="" width="30px"><span>七月</span> 👍（3） 💬（0）<div>可以用对数吗</div>2017-12-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-04-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/6b/80/dd2b3b3f.jpg" width="30px"><span>冯思鸣</span> 👍（0） 💬（0）<div>音量有时会很小…</div>2022-08-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/44/a4/7a45d979.jpg" width="30px"><span>IT蜗壳-Tango</span> 👍（0） 💬（0）<div>太棒了，如果有py代码就更棒啦。学习到了。</div>2020-11-07</li><br/><li><img src="" width="30px"><span>oci</span> 👍（0） 💬（0）<div>谢谢。理论很好，我还得多多实践</div>2020-03-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/3a/54/72402617.jpg" width="30px"><span>上善若水</span> 👍（0） 💬（1）<div>条件独立性，本身就是思维模式的问题，可能常识并不正确</div>2019-10-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/f2/af/db328094.jpg" width="30px"><span>啊哈哈</span> 👍（0） 💬（0）<div>根据训练数据计算后验概率，基于后验概率选择最佳决策。</div>2018-03-02</li><br/>
</ul>