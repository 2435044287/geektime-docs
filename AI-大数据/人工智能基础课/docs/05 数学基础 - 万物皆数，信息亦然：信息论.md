近年来的科学研究不断证实，不确定性才是客观世界的本质属性。换句话说，上帝还真就掷骰子。不确定性的世界只能使用概率模型来描述，正是对概率的刻画促成了信息论的诞生。

1948年，供职于美国贝尔实验室的物理学家克劳德·香农发表了著名论文《通信的数学理论》（A Mathematical Theory of Communication），给出了对信息这一定性概念的定量分析方法，标志着信息论作为一门学科的正式诞生。

香农在《通信的数学理论》中开宗明义：“通信的基本问题是在一点精确地或近似地复现在另一点所选取的消息。消息通常有意义，即根据某种体系，消息本身指向或关联着物理上或概念上的特定实体。但消息的语义含义与工程问题无关，重要的问题是一条消息来自于一个所有可能的消息的集合。”

这样一来，所有类型的信息都被抽象为逻辑符号，这拓展了通信任务的范畴与信息论的适用性，也将信息的传播和处理完全剥离。

**信息论使用“信息熵”的概念，对单个信源的信息量和通信中传递信息的数量与效率等问题做出了解释，并在世界的不确定性和信息的可测量性之间搭建起一座桥梁**。
<div><strong>精选留言（24）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/10/23/0a/39524860.jpg" width="30px"><span>Mr.Button</span> 👍（12） 💬（2）<div>为什么log以2为底的函数这么常见...这里为什么取2</div>2018-08-13</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKmyUF0MWNDUWibma6ia6t6xrZDv93hiaNONp9FG4yyjz1JBCe5dmJ4zjOI3hZiazK17PMCKwIE69gacA/132" width="30px"><span>井中月</span> 👍（6） 💬（1）<div>王老师，感谢您的回复。但是我还有点疑惑，X表示的是训练集的某个特征，Y相当于是训练集中需要被分类的变量，那么这样的话H(Y)就是一个定值，用它做分母和直接使用信息增益进行特征选择不就是一样的吗？</div>2018-03-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/2b/f9/203b0173.jpg" width="30px"><span>夜星辰</span> 👍（4） 💬（1）<div>有一点理解上的困惑希望王老师帮忙解答下 

1. 熵表示的是信息量大小，从公式中知道随着概率增大，熵会变小。而机器学习中常用交叉熵作为目标函数，学习的过程是不断求取最小熵，也就是求取概率最大的参数，等价于极大似然估计法进行参数估计。
2. 但是我无法上述理解1和最大熵原理联系起来，请老师佐证下问题</div>2018-03-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/de/50/317159be.jpg" width="30px"><span>Naraka，</span> 👍（3） 💬（1）<div>老师，不知道现在提问还会不会回答，
“从这个角度看，最大熵原理的本质在于在推断未知分布时不引入任何多余的约束和假设，因而可以得到最不确定的结果，预测的风险也就最小。”
这一段没有看懂，为什么得到最不确定的结果，预测风险会最小？最不确定的，可能性很多，预测的结果不也更吗？</div>2019-03-25</li><br/><li><img src="" width="30px"><span>水木竹水</span> 👍（3） 💬（1）<div>首先感谢老师讲的非常好。有个疑惑问下老师，信息增益是H(Y)-H(Y|X)，后者是已知X情况下Y的不确定性，信息增益就是X对Y的确定性消除。H(Y|X)越小，说明X对Y的分类效果越好，为何决策树不直接用H(Y|X)选取主要特征，而用信息增益，H(Y)是变化的吗？</div>2018-07-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/55/44/024204e2.jpg" width="30px"><span>星运里的错</span> 👍（3） 💬（1）<div>信息增益表示的就是特征 X带来的对训练集 Y 分类不确定性的减少程度，也就是特征 X 对训练集 YY的区分度。
这句话意思是  总体熵-某个特征下的熵  ＝去除某个特征影响的熵   老师。这个公式对么？
我的理解是   熵对应着信息量的多少。熵大，意味着信息量大，信息混杂，也就是不确定性大。
当用到决策树中时，要保证分支所考虑的不确定性最小，也就是可用信息量纯净（少），所以我们要用使 某个特征影响的熵 最小的那个特征进行分支，也就是信息增益越大。
我感觉。。。我理解的好乱。求老师解惑下</div>2018-05-19</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKmyUF0MWNDUWibma6ia6t6xrZDv93hiaNONp9FG4yyjz1JBCe5dmJ4zjOI3hZiazK17PMCKwIE69gacA/132" width="30px"><span>井中月</span> 👍（1） 💬（1）<div>王老师，您好，我有个疑问，信息增益比里面的分母是不是应该是H(X)？</div>2018-03-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/6e/84/45a909a6.jpg" width="30px"><span>卡斯瓦德</span> 👍（1） 💬（1）<div>看完这篇，突然觉得所谓的奇迹，其实就是信息熵不对等的结果，从某个面如何环境，物质看概率为百万分之一，从另一个面如自主意念等，概率可能就是十分之一，那么事件成就的结果其实就是KL后，不同的结果，饿可能总结有点问题，但是有那么个方向的感觉</div>2018-02-01</li><br/><li><img src="" width="30px"><span>chucklau</span> 👍（1） 💬（1）<div>嗯，这篇的内容很难理解，希望有其它更多的相关资料，谢谢老师。</div>2017-12-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/63/85/1dc41622.jpg" width="30px"><span>姑射仙人</span> 👍（10） 💬（2）<div>可以参考数学之美第二版，第六章 - 信息的度量和作用</div>2019-01-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/52/95/abb7bfe3.jpg" width="30px"><span>秦龙君</span> 👍（6） 💬（0）<div>学习了。这篇很难，后半部分暂时还看不懂。</div>2017-12-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/8e/8b/38b93ca0.jpg" width="30px"><span>听天由己</span> 👍（5） 💬（0）<div>看完之后，我努力应用如下：

消息是今天我学会了专栏的信息论部分，因为可能性较低，因而信息量较大，信息熵也就越大。

机器学习中的分类问题，可能还是缺乏较好的类比方法，初入门道就有些迷糊了。

看了些其他资料，这句话写得很妙，“学习就是一个熵减的过程”，学习的过程也就是使信息的不确定度下降的过程，这似乎就是机器学习的方向，然后再把《信息论、推理与学习算法》下载了，继续学习中。</div>2018-01-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/58/f7/22ea9761.jpg" width="30px"><span>wolfog</span> 👍（4） 💬（0）<div>这个推荐大家可以看看吴军老师的数学之美其中就有关于最大熵和互信息等的介绍，讲的更加详细和通俗一些</div>2018-01-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg" width="30px"><span>Simon</span> 👍（2） 💬（0）<div>Kullback-Leibler 散度，也叫库尔贝勒交叉熵</div>2020-03-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/b4/94/2796de72.jpg" width="30px"><span>追风筝的人</span> 👍（1） 💬（0）<div>信息是为了消除不确定性，量化信息的单位是比特</div>2021-04-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/36/88/20b6a6ee.jpg" width="30px"><span>Simon</span> 👍（1） 💬（0）<div>互信息是一个随机变量包含另一个随机变量信息量的度量。</div>2020-03-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/3a/54/72402617.jpg" width="30px"><span>上善若水</span> 👍（1） 💬（0）<div>局部信息增益</div>2019-10-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/3a/54/72402617.jpg" width="30px"><span>上善若水</span> 👍（1） 💬（0）<div>概率只能在条件确定性的环境使用啊</div>2019-10-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/c7/d9/9fc367b5.jpg" width="30px"><span>Snail@AI_ML</span> 👍（1） 💬（0）<div>非常棒，深入浅出，对照了培训课程之后，有一个更清晰的思路，虽然理解程度可能不够深，但觉得目前够用了，安利一波😄</div>2019-01-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/63/8d/bddef9bf.jpg" width="30px"><span>Geek_4b73dd</span> 👍（1） 💬（0）<div>老师你好，虽然留言里面提到了，但有一个问题还是不太明白，关于最大熵模型和交叉熵的。在网上看max最大熵模型的解时发现，其实max最大熵模型，就是max模型的最大似然估计，也就是说如果以logistic regression为例的化，max最大熵模型和max logistic regression的最大似然估计是一样的，而max logistic regression的最大似然估计其实就是minimize对应的cross entropy，所以其实最大熵模型和最小化cross entropy是不是其实是一回事？还是我理解的有些不对？谢谢老师啦！</div>2018-12-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/58/f7/22ea9761.jpg" width="30px"><span>wolfog</span> 👍（1） 💬（1）<div>之前看过吴军老师的《数学之美》，这一张还听得有点眉目，加油。</div>2018-01-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-04-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/22/57/77/e094e9a9.jpg" width="30px"><span>Geek_HanX2</span> 👍（0） 💬（0）<div>老师，信息增益比那里是不是写错了？分母应该是H(X)</div>2022-11-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/07/36/d677e741.jpg" width="30px"><span>黑山老妖</span> 👍（0） 💬（0）<div>感觉信息论就是机器学习在信息处理上的应用</div>2021-08-01</li><br/>
</ul>