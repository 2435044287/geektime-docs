除了正则化之外，优化也是深度学习需要解决的一个核心问题。由于深度神经网络中的隐藏层数目较多，因而将整个网络作为一个整体进行优化是非常困难的事情，需要花费大量的时间和计算力。出于效率和精确性的考虑，在深度学习的优化上需要使用专门的技术。

出于可解性的考虑，传统机器学习算法往往会小心翼翼地选择代价函数和优化条件，将待优化问题转化为容易求解的**凸优化问题**。但在神经网络，尤其是在深度神经网络中，更一般的非凸情况是不可避免的，这就给深度学习中的优化带来很多额外的挑战。

当待优化的代价函数的输入是$n$维向量时，其二阶导数就包含针对不同变量分别求偏导所得到的很多分量，将这些分量按顺序排列可以得到**Hessian矩阵**。而在神经网络的训练中，Hessian矩阵的病态问题非常常见，甚至无论优化问题是否具有凸优化的形式，病态的Hessian矩阵都会出现。

在线性方程$\\mathbf{A} \\mathbf{x} = \\mathbf{b}$中，当系数矩阵$\\mathbf{A}$的微小扰动会给解集$\\mathbf{x}$带来较大幅度的波动时，这样的矩阵就被称为**病态矩阵**（ill-conditioned matrix）。

病态矩阵是科学计算不愿打交道的对象，因为数值精度导致的不可避免的舍入误差可能会给输出带来巨大的偏离，正所谓“差之毫厘，谬以千里”。在神经网络的训练中，病态矩阵的影响体现在梯度下降的不稳定性上。当应用随机梯度下降解决优化问题时，病态矩阵对输入的敏感性会导致很小的更新步长也会增加代价函数，使学习的速度变得异常缓慢。

**深度神经网络面临的另一个挑战是局部极小值的问题**。凸优化问题的数学特性保证了局部极小值和全局最小值之间的等价关系。因而在优化一个凸问题时，任何形式的临界点都可以看成是可行解。而在神经网络，尤其是深度模型中，代价函数甚至会具有不可列无限多个局部极小值，这显然会妨碍对全局最小值的寻找，导致搜索陷入局部最优的陷阱中。
<div><strong>精选留言（7）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/8d/c0/7d3ce41b.jpg" width="30px"><span>徐凌</span> 👍（12） 💬（1）<div>我大学本科学的是文科，之后做了十几年的市场销售方面工作。现在已经决心工作后的业余时间学人工智能，不知道还有没有戏，会不会起步太晚了？


我毕业太久了，之前中学数学都快忘光了。之前看吴恩达的课对他上面提到的数学都不太看得懂。

所以2年前决定学人工智能后。我前年把中学数学给学了。去年我自学了单元和多元微积分，统计学，学了一些基础的线性代数和微分方程，离散数学。编程方面我去年从hello world 开始学了python 和matlab，还学了点sql语言。
到现在为止我一共花了2000小时学数学和编程。

接下来今年可能会重点把线性代数基础打好。再学一学数学分析，概率学。我觉得现在我应该具备学机器学习的基础了，下个月准备开始学系统学习一下机器学习。下半年想看看有没有能力学一下随机过程，听说随机过程对于人工智能比较重要。

明年我希望能学一些优化论，和泛函分析。看看自己到时候有没有水平可以看一些国外论文。现在看了一下都基本看不懂。 不知道集合论和图论是否也应该学呢？另外数论对人工智能是不是不太重要。


王老师觉得我这个计划是否可行，有没有什么建议呢？另外到后面我发现能自学的网络素材变少了。没有找到什么公开课教pde之类的数学的，应该怎么学比较好呢？</div>2018-02-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（3） 💬（1）<div>谢谢分享。

请问RMSprop，Adadelta主要使用的优化方法是不是文中提及的梯度聚合？

从找到的1年多前的文章来看，如果我上面的问题的答案是肯定的，梯度聚合和融合了Momentum的梯度聚合Adam及其一些变体(如融合进Nesterov Accelerated方法)的优化方法对于解决文中提到的问题的效果不少情况下更好。</div>2018-02-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-05-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-05-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/44/a4/7a45d979.jpg" width="30px"><span>IT蜗壳-Tango</span> 👍（0） 💬（0）<div>果然还是需要代码才能更好的理解</div>2020-11-08</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（0） 💬（0）<div>极客时间
21天打卡行动 23&#47;21
&lt;&lt;人工智能基础课25&gt;&gt;深度学习中的优化
回答老师问题:
既然优化方法包含不同的切入角度，那么你觉得哪一种优化方法可能具有更好的效果呢?
如何选择优化算法
1 对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值。
2 SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下（很多论文都用SGD），结果更可靠。
3 如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。
4 Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多 。Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，随着梯度变得稀疏，Adam 比 RMSprop 效果会好。整体来讲，Adam 是最好的选择。
[来源:https:&#47;&#47;www.cnblogs.com&#47;zingp&#47;p&#47;11352012.html]
今日所学:
1,病态矩阵是科学计算不愿打交道的对象，因为数值精度导致的不可避免的舍入误差可能会给输出带来巨大的偏离，正所谓“差之毫厘，谬以千里”。在神经网络的训练中，病态矩阵的影响体现在梯度下降的不稳定性上。当应用随机梯度下降解决优化问题时，病态矩阵对输入的敏感性会导致很小的更新步长也会增加代价函数，使学习的速度变得异常缓慢;
2,深度神经网络面临的另一个挑战是局部极小值的问题;
3,点是梯度为 0 的临界点，但它既不是极大值也不是极小值。从函数图像上看，多变量函数的鞍点在一个方向上向上弯曲，在另一个方向上则向下弯曲，从而形成了类似马鞍的形状。由于牛顿法的目标是寻找梯度为零的临界点，因而会受鞍点的影响较大，高维空间中鞍点数目的激增就会严重限制牛顿法的性能。
4,。随机梯度下降法（stochastic gradient descent）就是在传统机器学习和深度神经网络中都能发挥作用的经典算法。
5,随机梯度下降法是原始梯度下降法的一种改良,
6,在随机梯度下降法的基础上进行改进可以得到其他的优化方式，改进的手段主要有两种：一种是随机降低噪声，另一种是使用二阶导数近似。
7,降噪方法正是为了抑制噪声的影响应运而生，降噪的方式既包括提升单次梯度估计的精度，也包括提升迭代过程的精度，常用的算法包括动态采样、梯度聚合和迭代平均三类
8,动态采样和梯度聚合两类方法是通过使用固定的步长来获得线性的收敛速度，进而实现降噪;
9,迭代平均方法不是通过对梯度估计求平均，而是对每次迭代得到的参数结果求平均来实现降噪。
10,要提升随机梯度下降法的性能，还可以通过使用二阶导数近似的信息来抑制高度非线性和病态目标函数的不利影响。
11,其他随机剃度下降算法:典型的例子包括动量方法（momentum）、加速下降方法（accelerated gradient descent）和坐标下降方法（coordinate descent）。
关键字:凸优化问题,Hessian 矩阵,病态矩阵
总结重点:
1,深度学习中的优化需要解决病态矩阵、局部极小值和鞍点等问题；
2,深度学习优化中的降噪方法包括动态采样、梯度聚合和迭代平均；
3,深度学习优化中的二阶导数近似方法是对原始牛顿法的各种改进；
4,其他优化方法包括动量方法、加速下降方法和坐标下降方法。</div>2020-01-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/e6/f7/f8ed07ab.jpg" width="30px"><span>明High</span> 👍（0） 💬（0）<div>要多点图就好了</div>2018-02-25</li><br/>
</ul>