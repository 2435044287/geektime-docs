周四我和你分享了机器学习中的朴素贝叶斯分类算法，这一算法解决的是将连续取值的输入映射为离散取值的输出的分类问题。朴素贝叶斯分类器是一类**生成模型**，通过构造联合概率分布$P(X, Y)$实现分类。如果换一种思路，转而用**判别模型**解决分类问题的话，得到的算法就是“**逻辑回归**”。

**虽然顶着“回归”的名号，但逻辑回归解决的却是实打实的分类问题**。之所以取了这个名字，原因在于它来源于对线性回归算法的改进。通过引入单调可微函数$g(\\cdot)$，线性回归模型就可以推广为$y = g ^ {-1} (\\mathbf{w} ^ T \\mathbf{x})$，进而将线性回归模型的连续预测值与分类任务的离散标记联系起来。当$g(\\cdot)$取成对数函数的形式时，线性回归就演变为了逻辑回归。

在最简单的二分类问题中，分类的标记可以抽象为0和1，因而线性回归中的实值输出需要映射为二进制的结果。逻辑...
<div><strong>精选留言（16）</strong></div><ul>
<li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLoNzO1UdKU3MT4iciaUjgcsr1KPuOcVUJzWk0oS8EYdzzYRt6q1cdXw87PjnIAWNH3nfgNFOeeSE7g/132" width="30px"><span>dianxin556</span> 👍（22） 💬（4）<div>王老师，请问似然概率和后验概率的区别和联系？能否举例说明？谢谢！</div>2018-01-24</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKmyUF0MWNDUWibma6ia6t6xrZDv93hiaNONp9FG4yyjz1JBCe5dmJ4zjOI3hZiazK17PMCKwIE69gacA/132" width="30px"><span>井中月</span> 👍（4） 💬（1）<div>王老师，按文章最后一段所说，当一个样本同时属于多个类别时，是不是有几个类别就建立几个二分类模型，这样效率比较高？但是我曾经遇到过一个类似的问题，当时没有解决，想跟您请教。每个样本属于多个类别，这些类别加起来一共有将近400种，而且绝大多数类别都是严重不平衡的。这种情况下如果一个一个的建立模型是不是效率很低？这是一个文本分类的问题，文本是餐厅评论数据，类别是人工标注的评论主题。这些类别其实是可以合并的，但是合并之后意义不大。您怎么看这种情况？</div>2018-03-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/4f/fe/cd166eb4.jpg" width="30px"><span>叶秋</span> 👍（2） 💬（1）<div>可否推导一下逻辑回归的条件概率的推导过程</div>2018-03-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/91/f5/abb7bfe3.jpg" width="30px"><span>Jean</span> 👍（2） 💬（2）<div>问下逻辑回归中的“逻辑”是什么意思，为什么叫逻辑，是怎么来的？</div>2018-03-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/bc/66/117bdb99.jpg" width="30px"><span>MJ小朋友</span> 👍（2） 💬（1）<div>我好像发现了两个无伤大雅的错误，
S型函数b好像漏了，后面推导有了，又没了
对数似然函数应该取自然对数，不然没底数不行</div>2018-01-05</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKmyUF0MWNDUWibma6ia6t6xrZDv93hiaNONp9FG4yyjz1JBCe5dmJ4zjOI3hZiazK17PMCKwIE69gacA/132" width="30px"><span>井中月</span> 👍（1） 💬（1）<div>谢谢您的回复。我当时设想的是第二种思路，每个类和其他所有类配对。当时的数据量是50多万条。按您的经验来说，一般做分类问题，类别控制在多少个容易取得较好的效果呢？</div>2018-03-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/55/44/024204e2.jpg" width="30px"><span>星运里的错</span> 👍（0） 💬（1）<div>老师。我发现很多概念当时明白，过后就忘了，您是推荐从实战例子中去加深理解，还是反复的去看概念，知道看懂</div>2018-05-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/71/45/abb7bfe3.jpg" width="30px"><span>Andy</span> 👍（0） 💬（1）<div>王老师您好，在逻辑回归中，代价函数为什么选用交叉熵代价函数，而不是选用最小二乘代价函数呢？</div>2018-01-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/7a/42/9681595e.jpg" width="30px"><span>阿里-赤壁（羊宏飞）</span> 👍（4） 💬（0）<div>王老师这个课 我认为是 机器学习入门者 非常好的 图谱 ，其中一些知识点还是要结合书本和实战。问题的讨论 知乎上回答的也非常详尽</div>2018-06-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/2f/f4/2dede51a.jpg" width="30px"><span>小老鼠</span> 👍（1） 💬（1）<div>各位是不是都是数学系毕业的，好难懂</div>2019-01-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-05-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/a0/3f/06b690ba.jpg" width="30px"><span>刘桢</span> 👍（0） 💬（0）<div>打卡打卡</div>2020-03-22</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（0） 💬（0）<div>极客时间
21天打卡行动 8&#47;21
&lt;&lt;人工智能基础课10&gt;&gt;逻辑回归
回答老师问题:
前文对逻辑回归的分析都是在概率理论的基础上完成的。但在二分类任务中，逻辑回归的作用可以视为是在平面直角坐标系上划定一条数据分类的判定边界。那么逻辑回归的作用能不能从几何角度理解，并推广到高维空间呢？
这个问题有点深奥,根据我自身的学习知识,在逻辑回归算法中,如果用几何去表达,显然平面几何是做不到的,以前我老师讲过一个升维的算法,在平面几何不能表达的回归图形中,假设这个图形不是二维的,是多维度的,在多维度的空间中,是能计算和描绘的;
今日所学:Logistic回归(逻辑回归),这个算法本身是解决分类问题的,逻辑回归中，实现这一映射是对数几率函数，也叫 Sigmoid 函数;(Sigmoid函数搜索较大的值,所有值将介于0和1之间),而寻找以上函数的最大值就是以对数似然函数为目标函数的最优化问题，通常通过“梯度下降法”或拟“牛顿法”求解,另外对数似然函数的最大化可以等效为待求模型与最大熵模型之间 KL 散度的最小化。这意味着最优的估计对参数做出的额外假设是最少的，这无疑与最大熵原理不谋而合。
逻辑回归与朴素贝叶斯区别:
1,逻辑回归与线性回归的关系称得上系出同门，与朴素贝叶斯分类的关系则是殊途同归。两者虽然都可以利用条件概率 P(Y|X) 完成分类任务，实现的路径却截然不同。
2,朴素贝叶斯分类器是生成模型的代表，其思想是先由训练数据集估计出输入和输出的联合概率分布，再根据联合概率分布来生成符合条件的输出，P(Y|X) 以后验概率的形式出现。
3,两者的区别在于当朴素贝叶斯分类的模型假设不成立时，逻辑回归和朴素贝叶斯方法通常会学习到不同的结果
4,两者的区别还在于收敛速度的不同;
5,朴素贝叶斯基于少量数据集更有优势
逻辑回归的改进:一种改进方式是通过多次二分类实现多个类别的标记;另一种多分类的方式通过直接修改逻辑回归输出的似然概率，使之适应多分类问题，得到的模型就是 Softmax 回归,Softmax 回归模型的训练与逻辑回归模型类似，都可以转化为通过梯度下降法或者拟牛顿法解决最优化问题。
说明:虽然都能实现多分类的任务，但两种方式的适用范围略有区别。当分类问题的所有类别之间明显互斥，即输出结果只能属于一个类别时，Softmax 分类器是更加高效的选择；当所有类别之间不存在互斥关系，可能有交叉的情况出现时，多个二分类逻辑回归模型就能够得到多个类别的标记。
总结:a,逻辑回归模型是对线性回归的改进，用于解决分类问题；b,逻辑回归输出的是实例属于每个类别的似然概率，似然概率最大的类别就是分类结果；c,在一定条件下，逻辑回归模型与朴素贝叶斯分类器是等价的；d,多分类问题时可以通过多次使用二分类逻辑回归或者使用 Softmax 回归解决。
</div>2019-12-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/63/2c/2750bc59.jpg" width="30px"><span>历尽千帆</span> 👍（0） 💬（0）<div>王老师，您好。我没有明白“对数似然函数的最大化可以等效为待求模型与最大熵模型之间 KL散度的最小化。这意味着最优的估计对参数做出的额外假设是最少的”这句话，您可否再解释下？</div>2018-12-29</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/EvqrQ1wMs4SzC8dSBjAfVdEZ3yDT0bDUMicfRLq6BOSzjGFhCownt3S5MERXLpOpLmyJXCuyWbeOQG3ibzy0b4ibA/132" width="30px"><span>jkhcw</span> 👍（0） 💬（0）<div>逻辑回归在高纬模型下就是超平面</div>2018-12-16</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKmyUF0MWNDUWibma6ia6t6xrZDv93hiaNONp9FG4yyjz1JBCe5dmJ4zjOI3hZiazK17PMCKwIE69gacA/132" width="30px"><span>井中月</span> 👍（0） 💬（0）<div>好的，谢谢老师</div>2018-03-15</li><br/>
</ul>