决策树算法是解决分类问题的另一种方法。与基于概率推断的朴素贝叶斯分类器和逻辑回归模型不同，**决策树算法采用树形结构，使用层层推理来实现最终的分类**。与贝叶斯分类器相比，决策树的优势在于构造过程无需使用任何先验条件，因而适用于探索式的知识发现。

决策树的分类方法更接近人类的判断机制，这可以通过买房的实例说明。

面对眼花缭乱的房源，普通人优先考虑的都是每平方米的价格因素，价格不贵就买，价格贵了就不买。在价格合适的前提下，面积就是下一个待确定的问题，面积不小就买，面积小了就不买。如果面积合适，位置也是不容忽视的因素，单身业主会考虑房源离工作地点的远近，离单位近就买，离单位远就不买；为人父母的则要斟酌是不是学区房，是学区房就买，不是学区房就不买。如果位置同样称心，就可以再根据交通是否便捷、物业是否良好、价格是否有优惠等条件进一步筛选，确定最后的购买对象。

前面的例子模拟了一套购房策略。在这套策略中，业主对每个可选房源都要做出“买”与“不买”的决策结果，而“每平米价格”、“房屋面积”、“学区房”等因素共同构成了决策的判断条件，在每个判断条件下的选择表示的是不同情况下的决策路径，而每个“买”或是“不买”的决定背后都包含一系列完整的决策过程。决策树就是将以上过程形式化、并引入量化指标后形成的分类算法。

**决策树是一个包含根节点、内部节点和叶节点的树结构，其根节点包含样本全集，内部节点对应特征属性测试，叶节点则代表决策结果**。从根节点到每个叶节点的每条路径都对应着一个从数据到决策的判定流程。使用决策树进行决策的过程就是从根节点开始，测试待分类项的特征属性，并按照其值选择输出的内部节点。当选择过程持续到到达某个叶节点时，就将该叶节点存放的类别作为决策结果。

由于决策树是基于特征对实例进行分类的，因而其学习的本质是从训练数据集中归纳出一组用于分类的“如果......那么......”规则。在学习的过程中，这组规则集合既要在训练数据上有较高的符合度，也要具备良好的泛化能力。**决策树模型的学习过程包括三个步骤：特征选择、决策树生成和决策树剪枝**。
<div><strong>精选留言（6）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/8e/8b/38b93ca0.jpg" width="30px"><span>听天由己</span> 👍（9） 💬（1）<div>看完这几篇机器学习的方法与模型，深深感叹数学抽象思维的重要性，概念和原理是我们必须要掌握的知识。感谢老师的普及，功夫在课外，给自己压力去多看看其他视频课程，阅读相关资料来学习，要不然很容易迷失。

今天的思考题很有意思，从概念来看，决策树(Decision Tree）是在已知各种情况发生概率的基础上，通过构成决策树来求取净现值的期望值大于等于零的概率，评价项目风险，判断其可行性的决策分析方法，是直观运用概率分析的一种图解法。其实，我们更关心的先验概率，将所有可能发生的结果进行评估。</div>2018-01-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/58/f7/22ea9761.jpg" width="30px"><span>wolfog</span> 👍（2） 💬（0）<div>前几天一直在忙其他事，今天又回来了，加油。</div>2018-02-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/a5/98/a65ff31a.jpg" width="30px"><span>djfhchdh</span> 👍（1） 💬（0）<div>ID3算法生成决策树时，每个子节点都递归调用算法生成新的子节点，这里，节点在计算时使用的样本数据是全部的数据，还是经过上面节点分类后的样本数据呢？</div>2020-11-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/ac/5f/894761f8.jpg" width="30px"><span>十八哥</span> 👍（1） 💬（0）<div>信息增益就是业务信息最终要的字段</div>2019-01-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-05-02</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（0） 💬（0）<div>21天打卡行动 9&#47;21
&lt;&lt;人工智能基础课11&gt;&gt;决策树
回答老师问题:
在对决策树算法的分析中，自始至终都没有涉及似然概率和后验概率这些之前频繁出现的概念，但这并不意味着决策树算法与概率论完全无关。如何从概率角度看待决策树呢？
查资料总结,找到很多类似的,把树模型建立好后，把训练数据在模型里跑一下，看最终各类别所在比例，这个比例就成了最后所需的概率了。但这样的答案好像不对,感觉一楼思考的更有深度;
今日所学:
1,开篇老师先解释了决策树算法与素贝叶斯,逻辑回归的不同:
决策树算法是解决分类问题的另一种方法。与基于概率推断的朴素贝叶斯分类器和逻辑回归模型不同，决策树算法采用树形结构，使用层层推理来实现最终的分类。与贝叶斯分类器相比，决策树的优势在于构造过程无需使用任何先验条件，因而适用于探索式的知识发现。
2,决策树是一个包含根节点、内部节点和叶节点的树结构，其根节点包含样本全集，内部节点对应特征属性测试，叶节点则代表决策结果;
3,决策树模型的学习过程包括三个步骤：特征选择、决策树生成和决策树剪枝
4,特征选择决定了使用哪些特征来划分特征空间
5,在特征选择中通常使用的准则是信息增益。机器学习中的信息增益就是通信理论中的互信息，是信息论的核心概念之一;
6,决策树算法——ID3 算法, C4.5 算法,CART 算法
7,无论是 ID3 算法还是 C4.5 算法，都是基于信息论中熵模型的指标实现特征选择，因而涉及大量的对数计算。另一种主要的决策树算法 CART 算法则用基尼系数取代了熵模型
8,CART 分类树算法每次只对某个特征的值进行二分而非多分，最终生成的就是二叉树模型
9,过拟合解决方法:同其他机器学习算法一样，决策树也难以克服过拟合的问题，“剪枝”是决策树对抗过拟合的主要手段,决策树剪枝则是通过主动去掉分支以降低过拟合的风险，提升模型的泛化性能
10,定义决策树整体的损失函数并使之极小化，这等价于使用正则化的最大似然估计进行模型选择
11,决策树的剪枝策略可以分为预剪枝和后剪枝
12,依赖多个特征进行分类决策的就是多变量决策树,在特征空间上，单变量决策树得到的分类边界是与坐标轴平行的分段，多变量决策树的分类边界则是斜线的形式。
名词:决策树,特征选择,决策树生成,决策树剪枝,信息增益,决策树算法,基尼系数,二叉树模型,多变量决策树
总结:
1,决策树是包含根节点、内部节点和叶节点的树结构，通过判定不同属性的特征来解决分类问题；
2,决策树的学习过程包括特征选择、决策树生成、决策树剪枝三个步骤；
3,决策树生成的基础是特征选择，特征选择的指标包括信息增益、信息增益比和基尼系数；
4,决策树的剪枝策略包括预剪枝和后剪枝。
</div>2019-12-27</li><br/>
</ul>