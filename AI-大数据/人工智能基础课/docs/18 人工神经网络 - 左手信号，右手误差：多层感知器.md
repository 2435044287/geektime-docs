虽然异或问题成为感知器和早期神经网络的阿喀琉斯之踵，但它并非无解的问题。**恰恰相反，解决它的思路相当简单，就是将单层感知器变成多层感知器**。下图就是一个多层感知器的实例，这个包含单个隐藏层的神经网络能够完美地解决异或问题。

![](https://static001.geekbang.org/resource/image/ca/43/ca2dac3a4fd898ca87ff59b2be01aa43.png?wh=519%2A390) （图片来自*Machine Learning: An Algorithmic Perspective*, 2nd Edition, Figure 4.2）

假定两个输入节点A和B的二进制输入分别为1和0，则根据图中的权重系数可以计算出神经元C的输入为0.5，而神经元D的输入为0。在由C和D构成的隐藏层中，由于C的输入大于0，因而符号函数使其输出为1；由于D的输入等于0，符号函数则使其输出为0。在输出节点的神经元E上，各路输入线性组合的结果为0.5，因而E的输出，也是神经网络整体的输出，为1，与两个输入的异或相等。在此基础上可以进一步证明，这个神经网络的运算规则就是异或操作的运算规则。

**多层感知器（multilayer perceptron）包含一个或多个在输入节点和输出节点之间的隐藏层（hidden layer），除了输入节点外，每个节点都是使用非线性激活函数的神经元**。而在不同层之间，多层感知器具有全连接性，即任意层中的每个神经元都与它前一层中的所有神经元或者节点相连接，连接的强度由网络中的权重系数决定。**多层感知器是一类前馈人工神经网络**（feedforward neural network）。网络中每一层神经元的输出都指向输出方向，也就是向前馈送到下一层，直到获得整个网络的输出为止。

多层感知器的训练包括以下步骤：首先确定给定输入和当前权重下的输出，再将输出和真实值相减得到误差函数，最后根据误差函数更新权重。在训练过程中，虽然信号的流向是输出方向，但计算出的误差函数和信号传播的方向相反，也就是向输入方向传播的，正因如此，这种学习方式得名**反向传播**（backpropagation）。**反向传播算法通过求解误差函数关于每个权重系数的偏导数，以此使误差最小化来训练整个网络**。

**在反向传播算法中，首先要明确误差函数的形式**。当多层感知器具有多个输出时，每个分类结果$y\_j$与真实结果$d\_j$之间都会存在误差。在单层感知器中，误差直接被定义为两者之间的差值。但在多个输出的情形下，如果第一个输出神经元的误差大于零，第二个输出神经元的误差小于零，这两部分误差就可能部分甚至完全抵消，造成分类结果准确无误的假象。

如何避免这个问题呢？
<div><strong>精选留言（7）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg" width="30px"><span>JustDoDT</span> 👍（4） 💬（2）<div>https:&#47;&#47;google-developers.appspot.com&#47;machine-learning&#47;crash-course&#47;backprop-scroll&#47;
谷歌的一个反向传播演示页面，很有意思 。</div>2019-09-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/58/93/05ebd2aa.jpg" width="30px"><span>Geek_xlcfr2</span> 👍（11） 💬（0）<div>一个学习的感受，如果没有相关基础多说内容看一遍是没有什么感觉的。必须看完之后再去翻相关的资料和解释，然后再回过头来看文章，才会有有所感悟。</div>2018-02-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/a1/36/14b9dcb8.jpg" width="30px"><span>陈吉米</span> 👍（5） 💬（1）<div>要是配上点图就更会明白了</div>2018-02-08</li><br/><li><img src="" width="30px"><span>Geek_de85cb</span> 👍（4） 💬（0）<div>这个图解释的不好，只是拿过来这张图，并没有解释上边两个点的含义，对于其中的-0.5，-1，含义都没有解释，也没举例说明：（0，0）（1，0）（0，1）（1，1）四种情况得到的结果为什么是异或的效果，对于新人理解困难较大。</div>2021-08-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/f5/73/f7d3a996.jpg" width="30px"><span>！null</span> 👍（1） 💬（0）<div>开始的图的，不太清楚各个节点的运算法则。第一句：“ C 的输入为 0.5，而神经元 D 的输入为 0”这个结论是咋出来的都不太清楚。</div>2021-09-08</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（1） 💬（0）<div>极客时间
21天打卡行动 16&#47;21
&lt;&lt;人工智能基础课18&gt;&gt;多层感知器
回答老师问题:
反向传播算法是在 1986 年由乔弗雷·辛顿提出的，可今天，作为反向传播之父的辛顿却要大义灭亲。辛顿的观点是没有目标函数就无法进行反向传播，而如果数据没有标签自然就没有目标函数了。因此，要实现无监督学习就必须告别反向传播算法。那么应该如何看待辛顿的观点呢？
前面老师也讲了,无监督算法:聚类(1,k-均值聚类,2,层次聚类,3,基于密度聚类,4高斯混合模型,5,);降维;这类正与监督学习的目标相反,我觉得是对的;
今日所学:
1,多层感知器（multilayer perceptron）包含一个或多个在输入节点和输出节点之间的隐藏层（hidden layer），除了输入节点外，每个节点都是使用非线性激活函数的神经元;
2,多层感知器是一类前馈人工神经网络;
3,多层感知器的训练包括以下步骤：首先确定给定输入和当前权重下的输出，再将输出和真实值相减得到误差函数，最后根据误差函数更新权重。在训练过程中，虽然信号的流向是输出方向，但计算出的误差函数和信号传播的方向相反，也就是向输入方向传播的，正因如此，这种学习方式得名反向传播（backpropagation）。
4,反向传播算法通过求解误差函数关于每个权重系数的偏导数，以此使误差最小化来训练整个网络
5,在反向传播算法中，首先要明确误差函数的形式;
6,在反向传播算法中，每个输出神经元的误差都被写成平方项的形式，整个神经网络的误差则是所有输出神经元的误差之和;
7,明确定义了误差函数后，就要想方设法让它取得最小值。影响误差函数的因素无外乎三个：输入信号、传递函数和权重系数;
8,多层感知器采用对数几率函数作为传递函数;
9,求解误差函数的最小值就要找到误差函数的梯度，再根据梯度调整权重系数，使误差函数最小化;
10,链式法则是个非常有用的数学工具，它的思想是求解从权重系数到误差函数这个链条上每一环的作用，再将每一环的作用相乘，得到的就是链条整体的效果;
11,多层感知器的核心结构就是隐藏层，之所以被称为隐藏层是因为这些神经元并不属于网络的输入或输出。
12,在多层神经网络中，隐藏神经元的作用在于特征检测。随着学习过程的不断进行，隐藏神经元将训练数据变换到新的特征空间之上，并逐渐识别出训练数据的突出特征。
13,一个经验法则是训练样本数目应该是权重系数数目的 10 倍，这显然对计算能力提出了较高的要求;
14,多层感知器的训练要需要多次遍历整个数据集，因而迭代次数就成为另一个重要的问题。预先设定迭代次数无法保证训练效果，预先设定误差阈值则可能导致算法无法终止。因而常用的办法是：一旦误差函数停止减小，就终止学习算法。
15,多层感知器的训练要需要多次遍历整个数据集，因而迭代次数就成为另一个重要的问题。预先设定迭代次数无法保证训练效果，预先设定误差阈值则可能导致算法无法终止。因而常用的办法是：一旦误差函数停止减小，就终止学习算法。
总结:
讲课重点:
1,在感知器的输入层和输出层之间添加隐藏层，就可以得到多层感知器；
2,多层感知器是一类前馈神经网络，采用的是反向传播的学习方式；
3,反向传播算法要根据误差函数的梯度来调整权重系数，需要应用求导的链式法则；
4,单个隐藏层就能使多层感知器以任意精度逼近任意复杂度的连续函数。</div>2020-01-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg" width="30px"><span>JustDoDT</span> 👍（1） 💬（0）<div>链式法则：
https:&#47;&#47;zh.wikipedia.org&#47;wiki&#47;%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99</div>2019-09-17</li><br/>
</ul>