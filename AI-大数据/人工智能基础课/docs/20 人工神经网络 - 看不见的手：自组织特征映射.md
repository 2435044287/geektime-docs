无论是全局逼近的多层感知器，还是局部逼近的径向基网络，在训练中用到的都是监督学习的方法。**如果将无监督学习引入神经网络中，对应的结构就是自组织特征映射**（Self-Organizing Map），这是芬兰赫尔辛基大学的泰乌沃·柯霍宁于1981年提出的一类神经网络。

相比于前面介绍的神经网络，自组织映射有两个明显的不同。

**第一，它能够将高维的输入数据映射到低维空间之上（通常是二维空间），因而起到降维的作用**。在降维的同时，自组织映射妙就妙在还能维持数据在高维空间上的原始拓扑，将高维空间中相似的样本点映射到网络输出层的邻近神经元上，从而保留输入数据的结构化特征。

**第二，自组织映射采用的是竞争性学习而非传统的纠错学习**。在竞争性学习中，对输入样本产生响应的权利并不取决于预设的权重系数，而是由各个神经元相互竞争得到的。不断竞争的过程就是网络中不同神经元的作用不断专门化的过程。

竞争性学习的理念来自于神经科学的研究。在生物的神经系统中存在着一种名叫“**侧向抑制**”的效应，它描述的是兴奋的神经元会降低相邻神经元活性的现象。侧向抑制能够阻止从侧向刺激兴奋神经元到邻近神经元的动作电位的传播。什么意思呢？当某个神经元受到刺激而产生兴奋时，再刺激相近的神经元，则后者的兴奋对前者就会产生抑制作用。这种抑制作用会使神经元之间出现竞争，在竞争中胜出的神经元就可以“胜者通吃”，将竞争失败的神经元全部抑制掉。

自组织映射中的竞争性学习模拟的就是上述的侧向抑制机制。自组织映射的拓扑结构并非如多层感知器般的层次结构，而是一张一维或者二维的网格，网格中的每个节点都代表一个神经元，神经元的权重系数则是和输入数据的维度相同的向量。在拓扑结构中，每个神经元的位置都不是随意选取的，而是和功能有着直接的关系。距离较近的神经元能够处理模式相似的数据，距离较远的神经元处理对象的差异也会很大。

由于神经元在网格中的位置至关重要，因而训练过程就是在空间上对神经元进行有序排列的过程。自组织映射为神经元建立起一个坐标系，由于每个网格神经元对应一类特定的输入模式，输入模式的内在统计特征就是通过神经元的坐标来表示的。

因此，**自组织映射的主要任务就是将任意维度的输入模式转换为一维或二维的离散映射，并以拓扑有序的方式自适应地实现这个映射**。在训练过程中，自组织映射中每个神经元的权重系数首先要初始化，初始化的方式通常是将其赋值为较小的随机数，这可以保证不引入无关的先验信息。当初始化完成后，网络的训练就包括以下三个主要过程。
<div><strong>精选留言（7）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（1） 💬（1）<div>请问拓扑邻域函数有什么不错的参考资料除了文中的原理阐述，能看到数学公式，代码实现(最好是基于Python)或伪代码实现吗？能有实例更好。谢谢。</div>2018-01-23</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（2） 💬（0）<div>极客时间
21天打卡行动 18&#47;21
&lt;&lt;人工智能基础课20&gt;&gt;自组织特征映射
回答老师问题:
自组织映射具有通过时间上的演化在无序中产生有序的内在能力，这与目前同样火热的非线性科学与复杂系统的研究有几分神似。那么关于复杂性的研究能否应用于人工智能之中呢？
查资料后,汇总自己的语言:是可以的,例如当今的围棋alpha zero,还有医学领域等等;

今日所学:
1,将无监督学习引入神经网络中，对应的结构就是自组织特征映射（Self-Organizing Map);
2,自组织映射有两个明显的不同:第一，它能够将高维的输入数据映射到低维空间之上（通常是二维空间），因而起到降维的作用。第二，自组织映射采用的是竞争性学习而非传统的纠错学习。
3,侧向抑制”的效应，它描述的是兴奋的神经元会降低相邻神经元活性的现象。侧向抑制能够阻止从侧向刺激兴奋神经元到邻近神经元的动作电位的传播;
4,自组织映射的主要任务就是将任意维度的输入模式转换为一维或二维的离散映射，并以拓扑有序的方式自适应地实现这个映射;

5,网络的训练就包括以下三个主要过程:a,竞争过程,b,合作过程,c,自适应过程;
6,竞争过程的实质是找到输入模式和神经元之间的最佳匹配;
7,两个向量的内积越大，它们之间的欧氏距离就越小，因而内积最大化的匹配准则等效于欧氏距离最小化。从这个角度看，获胜神经元就是对输入模式的最佳匹配。
8,竞争过程确定了合作神经元的拓扑邻域的中心，合作过程就要界定中心之外的拓扑邻域;
9,自适应过程可以分为两个阶段。第一阶段是排序阶段，权重系数的拓扑排序在这个阶段形成；第二阶段是收敛阶段，通过微调特征映射实现对输入模式的精确描述。只要算法的参数没有问题，自组织映射就能将完全无序的初始状态按照输入模式以有组织的方式重构，这也是“自组织”的含义。
10,从输入模式到神经元的映射关系被称为特征映射;
11,自组织映射可以看成是一个编码器 - 解_码器模型：寻找最佳匹配神经元就是对输入模式进行编码，确定权重系数则是对编码结果进行解码，邻域函数则可以表示对编解码过程造成干扰的噪声的概率密度。自组织映射的这个性质与信息论中用于数据压缩的向量量化方法不谋而合。
总结:
重点:
1,自组织映射是一类无监督学习的神经网络，模拟了生物神经系统的竞争性学习机制；
2,自组织映射能将任意维度的输入模式转换为一维或二维的离散映射，得到的特征映射是拓扑有序的；
3,在拓扑映射中，输出神经元的空间位置对应了输入数据的模式或特征；
4,自组织映射网络的训练包括竞争过程、合作过程和自适应过程等几个主要步骤。</div>2020-01-05</li><br/><li><img src="" width="30px"><span>Geek_7389a6</span> 👍（1） 💬（0）<div>关于复杂性的研究，应该是可以用于人工智能的。虽然大都倾向于以简化繁，但繁琐复杂是研究中必然会逐渐面对的，不然怎么从复杂中抽取出精炼的简单的成分</div>2020-04-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-05-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/22/57/77/e094e9a9.jpg" width="30px"><span>Geek_HanX2</span> 👍（0） 💬（0）<div>一个参考视频：https:&#47;&#47;www.bilibili.com&#47;video&#47;BV1244y1E7PQ</div>2022-11-29</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（0） 💬（0）<div>极客时间
21天打卡行动 18&#47;21
&lt;&lt;人工智能基础课20&gt;&gt;自组织特征映射
回答老师问题:
自组织映射具有通过时间上的演化在无序中产生有序的内在能力，这与目前同样火热的非线性科学与复杂系统的研究有几分神似。那么关于复杂性的研究能否应用于人工智能之中呢？
查资料后,汇总自己的语言:是可以的,例如当今的围棋alpha zero,还有医学领域等等;
</div>2020-01-05</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（0） 💬（0）<div>极客时间
21天打卡行动 18&#47;21
&lt;&lt;人工智能基础课20&gt;&gt;自组织特征映射
</div>2020-01-05</li><br/>
</ul>