你好，我是方远。

在之前的课程里，我们一起学习了深度学习必备的内容，包括构建网络、损失函数、优化方法等，这些环节掌握好了，我们就可以训练很多场景下的模型了。

但是有的时候，我们的模型比较大，或者训练数据比较多，训练起来就会比较慢，该怎么办呢？这时候牛气闪闪的分布式训练登场了，有了它，我们就可以极大地加速我们的训练过程。

这节课我就带你入门分布式训练，让你吃透分布式训练的工作原理，最后我还会结合一个实战项目，带你小试牛刀，让你在动手过程中加深对这部分内容的理解。

## 分布式训练原理

在具体介绍分布式训练之前，我们需要先简要了解一下为什么深度学习要使用GPU。

在我们平时使用计算机的时候，程序都是将进程或者线程的数据资源放在内存中，然后在CPU进行计算。通常的程序中涉及到了大量的if else等分支逻辑操作，这也是CPU所擅长的计算方式。

而在深度学习中，模型的训练与计算过程则没有太多的分支，基本上都是矩阵或者向量的计算，而这种暴力又单纯的计算形式非常适合用GPU处理，GPU 的整个处理过程就是一个流式处理的过程。

但是再好的车子，一个缸的发动机也肯定比不过12个缸的，同理单单靠一个GPU，速度肯定还是不够快，于是就有了多个GPU协同工作的办法，即分布式训练。分布式训练，顾名思义就是训练的过程是分布式的，重点就在于后面这两个问题：
<div><strong>精选留言（12）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/8d/8a/ec29ca4a.jpg" width="30px"><span>马克图布</span> 👍（14） 💬（1）<div>问：在 `torch.distributed.init_process_group(backend=“nccl”)` 函数中，backend 参数可选哪些后端，它们分别有什么区别？

答：根据文档，可选后端有 `gloo`, `nccl` 和 `mpi`。区别如下。

1. GLOO 支持绝大部分 CPU 上对于 tensor 的 communication function，但在 GPU 上只支持 broadcast 以及 all_reduced 函数；
2. NCCL 支持大部分 GPU上对于 tensor 的 communication function，但不支持 CPU；
3. PyTorch 自带 NCCL 和 GLOO，但要使用 MPI 后端则需要在安装了 MPI 的 host 上从源码构建 PyTorch；
3. MPI 支持绝大部分 CPU 和 GPU 上的函数，但对于 CUDA 的支持必须是这个 host 上构建的 PyTorch 支持 CUDA 才可以。

关于使用哪个后端的经验法则：
- 使用 NCCL 进行分布式GPU训练
- 使用 GLOO 进行分布式CPU训练

参考自 PyTorch 文档：https:&#47;&#47;pytorch.org&#47;docs&#47;stable&#47;distributed.html</div>2021-11-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1c/6a/7f/791d0f5e.jpg" width="30px"><span>李翔</span> 👍（9） 💬（1）<div>你好，我看到官方的 ImageNet 的示例中299行有如下代码：
# compute gradient and do SGD step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

此处使用的是optimizer.zero_grad(),跟之前课程中使用的model.zero_grad()有些许不同。请问这里应该使用哪个</div>2021-11-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1b/7f/d1/15a39351.jpg" width="30px"><span>小门神</span> 👍（6） 💬（1）<div>老师，您好，请问利用pytorch对模型进行分布式训练后，在对模型进行推理时的处理和单机单卡有区别吗</div>2021-11-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/8d/8a/ec29ca4a.jpg" width="30px"><span>马克图布</span> 👍（1） 💬（1）<div>问题：

train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
data_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)

在使用 DistributedSampler 时，DataLoader 中的 dataset 参数是否应该与 DistributedSampler 的 `train_dataset` 一致呀？</div>2021-11-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/4a/35/66caeed9.jpg" width="30px"><span>完美坚持</span> 👍（0） 💬（1）<div>args 是一个模块吗？加载不了，我用的是python3 加载就说是 NameError: name &#39;basestring&#39; is not defined</div>2023-02-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2c/8d/70/b0047299.jpg" width="30px"><span>Zeurd</span> 👍（0） 💬（1）<div>老师，使用gup的时候会报错，Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)为什么会同时推到cup和gup呢，我输入的就是to（device）啊</div>2022-07-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/02/2a/90e38b94.jpg" width="30px"><span>John(易筋)</span> 👍（1） 💬（0）<div>过去，我们经常被问到：“我应该使用哪个后端？”。

# 1. 经验法则

* 使用 NCCL 后端进行分布式 GPU 训练

* 使用 Gloo 后端进行分布式 CPU 训练。

# 2. 具有 InfiniBand 互连的 GPU 主机

* 使用 NCCL，因为它是当前唯一支持 InfiniBand 和 GPUDirect 的后端。

# 3. 具有以太网互连的 GPU 主机

* 使用 NCCL，因为它目前提供了最好的分布式 GPU 训练性能，特别是对于多进程单节点或多节点分布式训练。 如果您在使用 NCCL 时遇到任何问题，请使用 Gloo 作为备用选项。 （请注意，对于 GPU，Gloo 目前的运行速度比 NCCL 慢。）

# 4. 具有 InfiniBand 互连的 CPU 主机

* 如果您的 InfiniBand 启用了 IP over IB，请使用 Gloo，否则，请改用 MPI。 我们计划在即将发布的版本中添加对 Gloo 的 InfiniBand 支持。

# 5. 具有以太网互连的 CPU 主机

* 使用 Gloo，除非您有特定的理由使用 MPI。
官网： https:&#47;&#47;pytorch.org&#47;docs&#47;stable&#47;distributed.html</div>2022-08-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/81/e9/d131dd81.jpg" width="30px"><span>Mamba</span> 👍（0） 💬（0）<div>因为设备没法运行demo了,实验室有多个机器,但每个机器只有单个GPU,难顶,以后有条件再回头看看</div>2024-05-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1e/b0/02/98f8b0ee.jpg" width="30px"><span>方华Elton</span> 👍（0） 💬（0）<div>讨论：DP部分ASimpleNet定义：

源代码：
class ASimpleNet(nn.Module):    def __init__(self, layers=3):        super(ASimpleNet, self).__init__()        self.linears = nn.ModuleList([nn.Linear(3, 3, bias=False) for i in range(layers)])    def forward(self, x):        print(&quot;forward batchsize is: {}&quot;.format(x.size()[0]))        x = self.linears(x)        x = torch.relu(x)        return x

提示修改后：
class ASimpleNet(nn.Module):
    def __init__(self, layers=3):
        super(ASimpleNet, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(3, 3, bias=False) for i in range(layers)])
    def forward(self, x):
        print(&quot;forward batchsize is : {}&quot;.format(x.size()[0]))
        for linear in self.linears:
            x = linear(x)
            x = torch.relu(x)
        return x

修改原因：
这个 NotImplementedError 的报错指出了问题所在：在 forward 方法中，尝试使用 self.linears(x) 对一个 ModuleList 直接进行调用。这是不被支持的，因为 ModuleList 不能像单个模块那样直接应用于输入数据。你需要遍历 ModuleList 中的每个模块，并逐个对输入数据 x 应用这些模块。

修正这一点的方法是在 forward 方法中遍历 self.linears 中的每个线性层（nn.Linear），并逐个应用它们。</div>2023-12-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-12-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/4a/35/66caeed9.jpg" width="30px"><span>完美坚持</span> 👍（0） 💬（0）<div>小试牛刀那一节看不懂</div>2023-02-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg" width="30px"><span>亚林</span> 👍（0） 💬（1）<div>感觉现在购买一个mac炼丹炉，好贵</div>2022-05-23</li><br/>
</ul>