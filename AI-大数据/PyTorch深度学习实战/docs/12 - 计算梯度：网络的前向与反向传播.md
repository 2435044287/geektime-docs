你好，我是方远。

在上节课，我们一同学习了损失函数的概念以及一些常用的损失函数。你还记得我们当时说的么：模型有了损失函数，才能够进行学习。那么问题来了，模型是如何通过损失函数进行学习的呢？

在接下来的两节课中，我们将会学习前馈网络、导数与链式法则、反向传播、优化方法等内容，掌握了这些内容，我们就可以将模型学习的过程串起来作为一个整体，彻底搞清楚怎样通过损失函数训练模型。

下面我们先来看看最简单的前馈网络。

## 前馈网络

前馈网络，也称为前馈神经网络。顾名思义，是一种“往前走”的神经网络。它是最简单的神经网络，其典型特征是一个单向的多层结构。简化的结构如下图：

![图片](https://static001.geekbang.org/resource/image/2c/27/2c89c49e8a4d46724113874c2e8d8f27.jpg?wh=1241x790)

结合上面的示意图，我带你具体看看前馈网络的结构。这个图中，你会看到最左侧的绿色的一个个神经元，它们相当于第0层，一般适用于接收输入数据的层，所以我们把它们叫做**输入层**。

比如我们要训练一个y=f(x)函数的神经网络，x作为一个向量，就需要通过这个绿色的输入层进入模型。那么在这个网络中，输入层有5个神经元，这意味着它可以接收一个5维长度的向量。

结合图解，我们继续往下看，网络的中间有一层红色的神经元，它们相当于模型的“内部”，一般来说对外不可见，或者使用者并不关心的非结果部分，我们称之为**隐藏层**。在实际的网络模型中，隐藏层会有非常多的层数，它们是网络最为关键的内部核心，也是模型能够学习知识的关键部分。
<div><strong>精选留言（11）</strong></div><ul>
<li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/j24oyxHcpB5AMR9pMO6fITqnOFVOncnk2T1vdu1rYLfq1cN6Sj7xVrBVbCvHXUad2MpfyBcE4neBguxmjIxyiaQ/132" width="30px"><span>vcjmhg</span> 👍（30） 💬（4）<div>不全是基于反向传播的，正常来说训练权重的方法有两种：
第一种是基于数值微分（如梯度下降算法），这种方式的优点就是实现简单便于编写代码不容易出错，但缺点在于时间复杂高，计算比较耗时
第二种是基于反向传播，它优点是效率高计算速度快，但缺点在于实现起来比较复杂，容易出错。
因此在实际工程应用中，会比较数值微分和反向传播的结果（两者的的结果应该非常接近），以确定我们书写的反向传播逻辑的正确性，这样的操作也被称为梯度确认。</div>2021-11-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/e3/d7/d7b3505f.jpg" width="30px"><span>官</span> 👍（9） 💬（1）<div>个人觉得不全是，比如在验证集和测试集测试模型不存在训练的过程，应该也就不存在反向传播吧？</div>2021-11-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/55/99/4bdadfd3.jpg" width="30px"><span>Chloe</span> 👍（2） 💬（1）<div>赞这个总结“模型通过梯度下降的方式，在梯度方向的反方向上不断减小损失函数值，从而进行学习。”</div>2022-01-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/b0/2f/e2096905.jpg" width="30px"><span>马成</span> 👍（0） 💬（1）<div>关于梯度计算，有个困惑：
如果采用GD算法，一次性把整个测试集都参与计算，loss函数是一个关于参数的多元函数，那么：
1）数据集可以流式加载，不断更新这个loss函数，其实不会消耗多大内存，就可以得到最终的内涵了全部数据集的loss函数；
2）有了loss函数之后，只需要求导一次，理论上以后每次迭代计算梯度的时候，直接基于loss函数计算就行，不需要重新计算一遍整个数据集；
所以总体上来看，GD应该是非常快的呀，为什么说他一次性加载太多数据，占用太多内存，每次梯度计算都重来一遍呢？</div>2023-11-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg" width="30px"><span>piboye</span> 👍（0） 💬（1）<div>这个前馈网络就是MLP 吧?</div>2023-06-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/68/2f/7bbf8e72.jpg" width="30px"><span>小平的IO</span> 👍（0） 💬（1）<div>这个图的输入层是4维的吧</div>2022-08-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/02/2a/90e38b94.jpg" width="30px"><span>John(易筋)</span> 👍（1） 💬（0）<div>方老师强调的要点摘录：
模型就是通过不断地减小损失函数值的方式来进行学习的。让损失函数最小化，通常就要采用梯度下降的方式，即：每一次给模型的权重进行更新的时候，都要按照梯度的反方向进行。

模型通过梯度下降的方式，在梯度方向的反方向上不断减小损失函数值，从而进行学习。</div>2022-08-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/27/e0/b1/78663847.jpg" width="30px"><span>北方剑圣</span> 👍（1） 💬（0）<div>链式法则部分似乎有个公式每显示出来，损失函数H的表示那里</div>2022-02-23</li><br/><li><img src="" width="30px"><span>Geek_9f1b33</span> 👍（1） 💬（0）<div>不太理解，深度学习不都是反向传播吗？ 通过误差反向传播迭代更新</div>2021-12-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-11-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg" width="30px"><span>亚林</span> 👍（0） 💬（0）<div>不是。参考一下百科：
反向传播算法和梯度下降法由于其实现简单，与其他方法相比能够收敛到更好的局部最优值而成为神经网络训练的通行方法。但是，这些方法的计算代价很高，尤其是在训练深度神经网络时，因为深度神经网络的规模（即层数和每层的节点数）、学习率、初始权重等众多参数都需要考虑。扫描所有参数由于时间代价的原因并不可行，因而小批量训练（mini-batching），即将多个训练样本组合进行训练而不是每次只使用一个样本进行训练，被用于加速模型训练[49]。而最显著地速度提升来自GPU，因为矩阵和向量计算非常适合使用GPU实现。但使用大规模集群进行深度神经网络训练仍然存在困难，因而深度神经网络在训练并行化方面仍有提升的空间。</div>2022-05-18</li><br/>
</ul>