你好，我是方远。

现在刷脸支付的场景越来越多，相信人脸识别你一定不陌生，你有没有想过，在计算机识别人脸之前，我们人类是如何判断一个人是谁的呢？

我们眼睛看到人脸的时候，会先将人脸的一些粗粒度特征提取出来，例如人脸的轮廓、头发的颜色、头发长短等。然后这些信息会一层一层地传入到某一些神经元当中，每经过一层神经元就相当于特征提取。我们大脑最终会将最后的特征进行汇总，类似汇总成一张具体的人脸，用这张人脸去大脑的某一个地方与存好的人名进行匹配。

那落实到我们计算机呢？其实这个过程是一样的，在计算机中进行特征提取的功能，就离不开我们今天要讲的卷积。

可以说，没有卷积的话，深度学习在图像领域不可能取得今天的成就。 那么，就让我们来看看什么是卷积，还有它在PyTorch中的实现吧。

## 卷积

在使用卷积之前，人们尝试了很多人工神经网络来处理图像问题，但是人工神经网络的参数量非常大，从而导致非常难训练，所以计算机视觉的研究一直停滞不前，难以突破。

直到卷积神经网络的出现，它的两个优秀特点：稀疏连接与平移不变性，这让计算机视觉的研究取得了长足的进步。什么是稀疏连接与平移不变性呢？简单来说，就是稀疏连接可以让学习的参数变得很少，而平移不变性则不关心物体出现在图像中什么位置。
<div><strong>精选留言（20）</strong></div><ul>
<li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/j24oyxHcpB5AMR9pMO6fITqnOFVOncnk2T1vdu1rYLfq1cN6Sj7xVrBVbCvHXUad2MpfyBcE4neBguxmjIxyiaQ/132" width="30px"><span>vcjmhg</span> 👍（16） 💬（2）<div>不可以，会出现ValueError: padding=&#39;same&#39; is not supported for strided convolutions。
原因分析：
1. 引入padding = &#39;same&#39;的目的实际上就是为了让输入特征保留更多的信息。而将stride设置成2，则是为了压缩特征，这和padding=&#39;same&#39; 的作用刚好相反
2. 假定允许在padding = &#39;same&#39;的情况下，将stride设置成2，则计算出来特征图，左侧和右侧的部分都会是0，提取到的这份部分特征是没有意义的。

此外我们在使用padding =  &#39;same&#39;时，一定要保证pytorch的版本大于等于1.9否则会出现RuntimeError: argument 1 (padding) must be tuple..这样的错误</div>2021-10-31</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q3auHgzwzM5RJskfTOqjCBNJ7dH0Th9L4yPsC8bXSpEpnJTtIfIMx0ia6icOt6UluD0rxBh2vF4xibQKfxRGfr9EA/132" width="30px"><span>Geek_8a391d</span> 👍（8） 💬（1）<div>老师您好，因为都是迁移学习从来没有思考过卷积的原理知识，一直有一个疑问，卷积核里面的值究竟是如何确定的呢？不同的卷积核会对卷积结果起到一个什么样的影响呢？我看很多模型会使用[-1 0 1]这样的形式，这样是不是对图像做了一个水平的分割呢？您在文中提到的这一个卷积核又起到什么作用？我们是通过训练得到的卷积核还是通过经验提前确定的呢？</div>2021-10-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/55/99/4bdadfd3.jpg" width="30px"><span>Chloe</span> 👍（2） 💬（1）<div>老师讲的很好呀，kernel的概念用二维图像一说，马上就明白了。谢谢老师解惑</div>2022-01-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1f/0b/f8/a9f695dc.jpg" width="30px"><span>🚶</span> 👍（2） 💬（1）<div>是不是可以理解为每次训练都是为了确定合适的卷积核参数呢？</div>2021-10-30</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKyEJx6dG2dMuMz6swdfjNuw3HMoEAbtxprfdBUAyRpLFayxmwEiaYLs224LuAdwWu55ENLgsI8U4w/132" width="30px"><span>lwg0452</span> 👍（1） 💬（1）<div>padding 为’same’时，stride 不可以为 1 以外的数值（会报错）。
个人理解：假如可以的话，用文中的例子计算输出特征图只有左上部分不是0，图像边缘信息还是丢失了，没有意义。</div>2021-10-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/be/6e/46a5da10.jpg" width="30px"><span>Hong</span> 👍（0） 💬（1）<div>padding为’same’时，stride不能大于1，简单理解，当大于1时，相当于通过stride进行降级采样，输入特征图和输出特征图尺寸不可能相等了~
老师，这么理解对吗？</div>2023-02-20</li><br/><li><img src="" width="30px"><span>chenyuping</span> 👍（0） 💬（1）<div>老师，您好，请问padding=1和same的效果是一样的吗？ 
对这个我有一点疑惑，padding=1是补一圈0，然后same的时候我看的例子是在右边和下边补0。感觉两者效果不一样吧，因为padding=1的时候，算下来会比原来的长宽，各多出一个像素吧？

另外，为什么valid 和same 的时候 stride只能是1，这个没有想明白。
</div>2022-10-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/02/2a/90e38b94.jpg" width="30px"><span>John(易筋)</span> 👍（0） 💬（1）<div>平移不变性（translation invariant）指的是CNN对于同一张图及其平移后的版本，都能输出同样的结果。这对于图像分类（image classification）问题来说肯定是最理想的，因为对于一个物体的平移并不应该改变它的类别。而对于其它问题，比如物体检测（detection）、物体分割（segmentation）来说，这个性质则不应该有，原因是当输入发生平移时，输出也应该相应地进行平移。这种性质又称为平移等价性（translation equivalence）。
稀疏连接学生认为是stride，可以跳过一些计算，池化层可以缩小图片大小。

参数的学习需要数据，由于数据中平移的分布一般都比较不均匀，引入平移的数据增强（augmentation）肯定是必要的。其实裁切（crop）就是一种平移的数据增强方式，因为不同裁切方式对应的patch之间的变换就是平移。而且这种方式相比于平移更加自然，没有周围的黑边padding，因此更加常用。总结起来，就是CNN的平移不变性主要是通过数据学习来的，结构只能带来非常弱的平移不变性，而学习又依赖于数据增强中的裁切，裁切相当于一种更好的图像平移。

希望老师点评，wx: zgpeace
参考：https:&#47;&#47;www.zhihu.com&#47;question&#47;301522740&#47;answer&#47;531606623</div>2022-07-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1f/10/99/40e067d4.jpg" width="30px"><span>梁</span> 👍（0） 💬（1）<div>你好，老师，请问有无这些文章中的完整案例，直接看只言片语感觉比较吃力</div>2022-06-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg" width="30px"><span>亚林</span> 👍（0） 💬（1）<div>不能。
ValueError: padding=&#39;same&#39; is not supported for strided convolutions</div>2022-05-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2c/8d/70/b0047299.jpg" width="30px"><span>Zeurd</span> 👍（0） 💬（1）<div>跑的时候会报Using padding=&#39;same&#39; with even kernel lengths and odd dilation may require a zero-padded copy of the input be created的错，是新版本需要先创副本么？要怎么操作呢？ </div>2022-04-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/5a/56/115c6433.jpg" width="30px"><span>jssfy</span> 👍（0） 💬（1）<div>卷积核是什么呢？其实就是我们卷积层要学习到的参数，就像下图中红色的示例，下图中的卷积核是最简单的情况，只有一个通道。

请问
1. 这里的通道只有一个怎么理解，玕没太明白?</div>2022-03-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/29/5d/3f/ad1fed4a.jpg" width="30px"><span>黑暗骑士</span> 👍（0） 💬（1）<div>老师好，
请问padding=&#39;same&#39;时，第一行和第一列都只进行了一次卷积运算，而其他部分都进行了两次，如何确保这一部分信息不被丢失呢？</div>2022-03-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/29/5d/3f/ad1fed4a.jpg" width="30px"><span>黑暗骑士</span> 👍（0） 💬（2）<div>老师，最后结果出不来，报警告

Using padding=&#39;same&#39; with even kernel lengths and odd dilation may require a zero-padded copy of the input be created</div>2022-02-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/0f/53/92a50f01.jpg" width="30px"><span>徐洲更</span> 👍（0） 💬（1）<div>卷积核让我想到了传统图像处理中的HORRIS这些方法，有着类似的卷积运算。只不过，卷积神经网络是随机初始化卷积核里的数值，通过不断的训练，得到比较合适的参数，而传统的方法可能偏向于人工设计一些比较好的卷积核，用于某些特定的任务。不知道，这样子理解是否合适呢？</div>2021-12-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1a/86/23/f1dbd526.jpg" width="30px"><span>Allen</span> 👍（0） 💬（1）<div>nn.Parameter这一行的用法老师能再详细解说一下吗？</div>2021-11-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2b/14/f3/175dc943.jpg" width="30px"><span>咪西小悠</span> 👍（0） 💬（1）<div>padding = &#39;same&#39;&#47;&#39;valid&#39; 是在pytorch 1.9之后的版本才支持的，我用的1.8版本会报错：conv2d(): argument &#39;padding&#39; (position 5) must be tuple of ints, not str</div>2021-11-08</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q3auHgzwzM7uYZvwhrLHsJICstGXaOvUNZnyq0aO7gF0OLicMyZAZFnRiaDyvM1lGxnEDP2VUMV3m6UjiazMmSNGQ/132" width="30px"><span>Geek_8cfc4c</span> 👍（0） 💬（1）<div>老师你好：

文中这句没看懂……
&quot;这样计算后会生成 m 个特征图，然后将这 m 个特征图按对应位置求和即可，
求和后 m 个特征图合并为输出特征中一个通道的特征图。&quot;
假设m等于1，那么 带入原文就是
&quot;这样计算后会生成 1 个特征图，然后将这 1 个特征图按对应位置求和即可，
求和后 1 个特征图合并为输出特征中一个通道的特征图。&quot; 中的 &quot;按对应位置求和&quot; 指的是什么呢，
计算后的特征图不就已经是输出特征图了么？

另外，n个卷积核，每个大小都是(m, k, k), 那么输入图的每个通道都要经过n个卷积核计算，生成n个输出图，所以最后的形状是
(n, w&#39;, h&#39;)这么理解对么？</div>2021-11-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2b/5e/f9/96896116.jpg" width="30px"><span>天凉好个秋</span> 👍（1） 💬（2）<div>请问为什么我跑代码会出现相同结果但是会报错呢？
UserWarning: Using padding=&#39;same&#39; with even kernel lengths and odd dilation may require a zero-padded copy of the input be created
dilation默认为1为奇数造成的吗？怎么才可以取消这个报错呢</div>2022-02-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-11-27</li><br/>
</ul>