你好，我是方远。

在上节课中，我们共同了解了前馈网络、导数、梯度、反向传播等概念。但是距离真正完全了解神经网络的学习过程，我们还差一个重要的环节，那就是优化方法。只有搞懂了优化方法，才能做到真的明白反向传播的具体过程。

今天我们就来学习一下优化方法，为了让你建立更深入的理解，后面我还特意为你准备了一个例子，把这三节课的所有内容串联起来。

## 用下山路线规划理解优化方法

深度学习，其实包括了三个最重要的核心过程：模型表示、方法评估、优化方法。我们上节课学习的内容，都是为了优化方法做铺垫。

优化方法，指的是一个过程，这个过程的目的就是，寻找模型在所有可能性中达到评估效果指标最好的那一个。我们举个例子，对于函数f(x)，它包含了一组参数。

这个例子中，优化方法的目的就是**找到能够使得f(x)的值达到最小值**对应的权重。换句话说，优化过程就是找到一个状态，这个状态能够让模型的损失函数最小，而这个状态就是**模型的权重**。

常见的优化方法种类非常多，常见的有梯度下降法、牛顿法、拟牛顿法等，涉及的数学知识也更是不可胜数。同样的，PyTorch也将优化方法进行了封装，我们在实际开发中直接使用即可，节省了大量的时间和劳动。

不过，为了更好地理解深度学习特别是反向传播的过程，我们还是有必要对一些重要的优化方法进行了解。我们这节课要学习的梯度下降法，也是深度学习中使用最为广泛的优化方法。
<div><strong>精选留言（15）</strong></div><ul>
<li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/j24oyxHcpB5AMR9pMO6fITqnOFVOncnk2T1vdu1rYLfq1cN6Sj7xVrBVbCvHXUad2MpfyBcE4neBguxmjIxyiaQ/132" width="30px"><span>vcjmhg</span> 👍（20） 💬（3）<div>不是
1. batch_size越大显存占用会越多，可能会造成内存溢出问题，此外由于一次读取太多的样本，可能会造成迭代速度慢的问题。
2. batch_size较大容易使模型收敛在局部最优点
3. 此外过大的batch_size的可能会导致模型泛化能力较差的问题</div>2021-11-09</li><br/><li><img src="" width="30px"><span>clee</span> 👍（5） 💬（1）<div>你好，最近3节课是理论偏多，实际动手部分偏少，那这部分内容应该怎么实际动手练习才能和理论想结合的更好，这方面有什么建议呢？</div>2021-11-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/8d/8a/ec29ca4a.jpg" width="30px"><span>马克图布</span> 👍（4） 💬（1）<div>文章里说：

&gt; 越小的 batch size 对应的更新速度就越快，反之则越慢，但是更新速度慢就不容易陷入局部最优。

「更新速度慢就不容易陷入局部最优」这不是说「越大的 batch size 越不容易陷入局部最优」么？看了其他人和老师的留言被整晕了……😂</div>2021-11-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/63/cb/7c004188.jpg" width="30px"><span>和你一起搬砖的胡大爷</span> 👍（3） 💬（1）<div>老师，吴恩达老师的ml课上说不管什么gd都没法确定打到全局最优，在高维数据下几乎必然只能到局部最优，所以初始化的技巧就比较重要。
老师您说的就无法得到全局最优我不大理解</div>2021-11-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/e3/d7/d7b3505f.jpg" width="30px"><span>官</span> 👍（3） 💬（1）<div>并不是，batchsize越大更新越慢，而且对显存要求变高，在配置较低的机器会出现显存不足的问题</div>2021-11-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2a/ce/1d/74fc7790.jpg" width="30px"><span>tom</span> 👍（1） 💬（1）<div>batch size过大，有可能会爆显存。。。😂</div>2021-11-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（1）<div>老师我不太明白梯度下降。
梯度下降最快，但是风险最大，所以损失函数最大。反方向就是下降最慢，风险最小，所以损失函数最小吗？
那反方向，听字面意思不是朝山上回去了吗？</div>2023-12-01</li><br/><li><img src="" width="30px"><span>人间失格</span> 👍（0） 💬（1）<div>老师，请问&quot;迭代速度慢&quot;这个问题
1、随机梯度下降和批量梯度下降都是对每个样本求损失，理论上是不是计算次数一样呢，这个慢是指的参数更新慢吗，总体上是不是批量下降更快一点。
2、小批量梯度下降和随机梯度下降的epoch理论上次数差别大吗，随机梯度下降每个样本更新一次，epoch会不会少一点也能达到相同的收敛效果。</div>2021-11-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/27/fc/ca/c1b8d9ca.jpg" width="30px"><span>IUniverse</span> 👍（0） 💬（1）<div>batchsize越大，更新模型的也会越慢。同时太大了也会爆显存</div>2021-11-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/dc/ce/03fdeb60.jpg" width="30px"><span>白色纯度</span> 👍（15） 💬（0）<div>batch_size理解：
1) Batch_size 设为 2 的幂，是为了符合 CPU、GPU 的内存要求，利于并行化处理
2) Batch_size 设置的大（视数据集而定，一般上千就算比较大了），意味着守旧派系
a) 优点是：梯度震荡小，训练结果更稳定，训练时间相对更短。
b) 缺点：泛化能力差，因为相同epoch的情况下，参数更新的次数减少了，增大迭代次数可以适当提高模型表现；内存可能会溢出，训练速度变慢；容易陷入局部最优（毕竟全局最优只有一个，局部最优有很多，收敛太过平稳，一旦陷入某个极小值便很难逃离）
3）batch_size设置的小，意味着创新派
a) 更新频繁，会受到噪声数据的影响，容易逃离局部最优
b) 泛化能力往往更强，在数据标注没有大范围出错的情况下，参数更新次数频繁，更新方向整体都是对的，方向错的次数占比更低，容易被纠正

按照极限法的思路，batch_size为1，代表着参数更新极不稳定；batch_size为全体数据样本量，更新非常稳定。但参数更新本就是个追逐最优的过程，遇到极值前需要稳定更新，遇到极值后需要不稳定的更新逃离马鞍面。所以衍生出 mini-batch_size的办法。权衡之下的取舍吧。</div>2021-11-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/92/71/9fd7cd7a.jpg" width="30px"><span>Daniel</span> 👍（1） 💬（0）<div>发现一个 神经网络的可视化 playground， 可以调节batch_size 等一系列参数，感兴趣的同学可以试试，挺助于前几节课的理解。
http:&#47;&#47;playground.tensorflow.org&#47;#activation=relu&amp;regularization=L1&amp;batchSize=24&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=10&amp;networkShape=2&amp;seed=0.88254&amp;showTestData=false&amp;discretize=true&amp;percTrainData=20&amp;x=false&amp;y=false&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false</div>2022-09-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/81/e9/d131dd81.jpg" width="30px"><span>Mamba</span> 👍（0） 💬（0）<div>必然不是，因为mini_batch小批量梯度下降是在批量梯度下降和随机梯度下降的折中，如果batch size越大，训练的速度就越慢，就会越接近批量梯度下降，从而失去了小批量的优点</div>2024-05-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>Batch size越大，越容易陷入局部最优</div>2023-12-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/02/2a/90e38b94.jpg" width="30px"><span>John(易筋)</span> 👍（0） 💬（0）<div>为了得到最小的损失函数，我们要用梯度下降的方法使其达到最小值。
为了得到最小的损失函数，我们要用梯度下降的方法使其达到最小值。
为了得到最小的损失函数，我们要用梯度下降的方法使其达到最小值。
重要的事情3遍</div>2022-08-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/8c/5c/3f164f66.jpg" width="30px"><span>亚林</span> 👍（0） 💬（0）<div>太大了，硬件受不了</div>2022-05-19</li><br/>
</ul>