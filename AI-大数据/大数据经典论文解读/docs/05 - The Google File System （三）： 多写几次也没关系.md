你好，我是徐文浩。在前面的两讲中，我们一起探讨了GFS系统设计中秉持的两个原则，分别是“保持简单”和“根据硬件特性设计系统”，而今天我们要讨论的GFS的最后一个设计特点，是“**放宽数据一致性的要求**”。

分布式系统的一致性要求是一个很有挑战的话题。如果说分布式系统天生通过更多的服务器提升了性能，是一个天然的优点，那么在这些通过网络连起来的服务器之间保持数据一致，就是一个巨大的挑战。毕竟网络传输总有延时，硬件总会有故障，你的程序稍有不慎，就会遇到甲服务器觉得你的钱转账失败，而乙服务器却觉得钱已经转走了的情况。可以说，**一致性是分布式系统里的一个永恒的话题**。

不过2003年的GFS，对于一致性的要求，是非常宽松的。一方面，这是为了遵循第一个设计原则，就是“保持简单”，简单的设计使得做到很强的一致性变得困难。另一方面，则是要考虑“硬件特性”，GFS希望在机械硬盘上尽量有比较高的写入性能，所以它只对顺序写入考虑了一致性，这就自然带来了宽松的一致性。

![图片](https://static001.geekbang.org/resource/image/22/73/224bffba53c4398bd39cd55342341673.jpg?wh=1920x904)

好，废话不多说，让我们切入正题。

## 随机写入只是“确定”的

通过上一讲的学习，我们知道了在GFS中，客户端是怎么把数据写入到文件系统里的。不过，我们并没有探讨一个非常重要的问题，就是数据写入的一致性（Consistency）问题。这个一致性，也是我们常常听说的CAP理论中的C，即一致性、可用性、分区容错性在分布式系统中，三者不能兼得中的一致性问题。这个C，也正来自于一致性的英文Consitency的首字母。
<div><strong>精选留言（22）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/14/00/4e/be2b206b.jpg" width="30px"><span>吴小智</span> 👍（6） 💬（2）<div>老师能否抽时间讲一下 GFS 中 5.2 Data Integrity 章节关于数据完整性的内容，论文说是不同的 chunkserver 独立检查数据的完整性，不怎么看懂，老师能否解答下。</div>2021-10-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/1d/13/31ea1b0b.jpg" width="30px"><span>峰</span> 👍（6） 💬（2）<div>如果在写入时就保证文件是ok的，那么就是要做个失败重试，即，写入失败就整个放弃重新写一个文件。
另一个思路是，记录下每次发其写入请求是失败还是成功，以及每次写入的大小，也就形成了一个list（成功失败，写入大小）每次读电影文件的时候，就可以根据这个信息，读文件了，比如开始读失败了，所以跳过这次写入大小，读成功了，则读取这次写入大小。 怎么保存这个信息存的是ok的，又可以专门为gfs，设计一种文件格式来保证或者当作文件的元数据信息存入master（当然不是好思路）。

</div>2021-09-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/62/b5/4159fa05.jpg" width="30px"><span>zhanyd</span> 👍（3） 💬（1）<div>如果我们通过 GFS 的客户端要写入一部电影到 GFS，然后过一阵再读出来，我们都可以有哪些方式，来保障这个电影读取之后能够正常播放呢？

可以按照老师说的，给每一条要写入的数据生成一个唯一的ID，并且在里面带上当时的时间戳，读取的时候再按ID的顺序去重读取即可。
</div>2021-09-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/66/8f/02be926d.jpg" width="30px"><span>在路上</span> 👍（1） 💬（2）<div>徐老师好，非常感谢对CAP问题的分享，希望能多分享一些中文资料。回答老师的问题，如果保证一部电影写入GFS之后，读出来能正常播放？可以将电影数据拆分成k份，每一份的大小小于GFS单次追加的数据，在每一份数据前面增加电影编号和数据块编号，以便在读取时重新组装数据。如果数据顺序写入GFS，读取时只需要去重，如果数据并发写入GFS，读取时既需要去重，也需要重排序。考虑到播放电影这个业务场景，可以采用64位数据保存电影id，32位数据保存数据块编号，减掉这12B的数据，剩余的数据（16MB-12B）存储电影数据。电影通常是顺序播放的，所以数据最好顺序写入GFS，便于快速读取电影接下来的内容。</div>2021-09-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/30/c1/2dde6700.jpg" width="30px"><span>密码123456</span> 👍（0） 💬（1）<div>提个问题，文中提到当前chunk写不下填充数据，换下一个chunk。如果遇到多个chunk都写不下当前数据，怎么优化？如果都填充数据，感觉又浪费不少空间。</div>2023-03-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/f9/e6/47742988.jpg" width="30px"><span>webmin</span> 👍（0） 💬（1）<div>一个chunk有64MB，追加一次16MB，这个是不是有点像是一个嵌套数组，第一维定位chunk，第二维定位每次追加写入，管理文件块与管理内存块比较类似。
思考题：
可以参考TCP协议，TCP是流式协议，向网络写入1MB，在传输的过程中，这一1MB会被切分为若干包，在包外套上一层TCP协议的头，其中有Seq等信息，交换机在传输的过程中有可能会通过多条路径来传输数据，数据到达目标机器的先后顺序不一定和发送顺序一至，包到达目标机器后，如果前序还未达到，OS是不会让应用程序读取到这些数据的，只有Seq按序的包到达OS才提供给应用程序，那么同理GFS也可以给16MB的块加上一个协议头其中可以包含序号等信息。</div>2021-10-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/27/f3/51/c8eb2d0c.jpg" width="30px"><span>Amon Tin</span> 👍（6） 💬（0）<div>“客户端可能读出来的数据里，前一小时是《星球大战》，后一小时是《星际迷航》。”
我理解这个问题是因为在并发随机写的场景下，两个客户端从master拿到的写入chunk handle和写入的偏移位置是相同的，虽然主副本在写入缓存区时对多个客户端的写入顺序做了排序，但由于这两个写入操作都指定了是要往同一个偏移位置写，所以不管排序结果是先执行《星球大战》的写入，还是先执行《星际迷航》的写入，他们都一定会写到同一个偏移位置上，导致数据被覆盖。
而追加写入过程中，客户端不需要获取具体的写入偏移位置，所以主副本在对多个写入请求排序后，一定会保证后写入的是追加在先写入的偏移位置之后的，就不会写入覆盖的情况发生。
不知道我的理解正不正确。</div>2022-01-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/83/c9/5d03981a.jpg" width="30px"><span>thomas</span> 👍（6） 💬（2）<div>老师请问： 本节介绍的追加写为什么没有用到上一节课介绍的流水线式的数据存储，而是由主副本节点分别向次副本节点同步？ 而且追加写是GFS保存数据的主要方式，那依然存在网络传输的瓶颈</div>2021-11-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/83/c9/5d03981a.jpg" width="30px"><span>thomas</span> 👍（3） 💬（2）<div>为什么要给空间不足的chunk 填满空数据后，再寻找下一个chunk?   不填充空数据有什么问题？</div>2021-10-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c6/8f/a1504d13.jpg" width="30px"><span>稻草人</span> 👍（2） 💬（0）<div>“至少一次”那里1、产生的脏数据如何处理的呢？ 是否会造成空间浪费；2、另外的两个并发客户端也产生了数据，这部分数据也是没用的，又是怎么处理的呢？</div>2022-06-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/55/99/4bdadfd3.jpg" width="30px"><span>Chloe</span> 👍（2） 💬（0）<div>个人感觉，从这篇开始，需要多读几遍文章，才能消化，大家觉得呢？</div>2022-06-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/18/75/68487e89.jpg" width="30px"><span>川川</span> 👍（2） 💬（0）<div>老师你好，有一点我感觉特别不理解。文章说到GFS采用追加的方式一次最多16MB，但由于最后一个chunk很可能剩余空间小于64M，这意味着某一份数据单个chunk会分布在chunkserver的多个物理的chunk文件上。这里是否应该有两个概念：
1.我们需要存储的数据被拆分成了平均大小64MB的逻辑chunk
2.chunkserver中的chunk文件实际上是每个64MB
结论：既然采用追加的方式是不是能够说明我的疑惑，单个逻辑chunk数据实际上会分布在chunkserver两个连续的chunk文件上。那这样元数据实际上也会存逻辑chunk在指定chunk文件的start和offset</div>2021-10-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/22/f4/9fd6f8f0.jpg" width="30px"><span>核桃</span> 👍（1） 💬（0）<div>这里重复写会带来写放大问题，另外一致性的要求吧，现在很多已经没有要求所有副本都成功才算成功的。像elasticsearch的话，可以只要求主副本成功就算成功了，并且写入是异步的，对一致性要求非常宽松了，但是带来非常多噩梦般的难题，例如主副本写入的时候，恰好发生了leader切换，导致事务校验逻辑就非常复杂了，甚至会出现互相删除对方文件的情况。</div>2021-11-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1e/15/8e/ae3304c2.jpg" width="30px"><span>demo</span> 👍（0） 💬（0）<div>空间不够为什么要填充无效数据啊?</div>2022-11-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1f/94/a0/84342a0d.jpg" width="30px"><span>@</span> 👍（0） 💬（0）<div>老师 请教一下 GFS的chunkserver是如何动态扩展的 新添加一个chunkserver服务器后，之后的写数据操作流程</div>2022-09-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/3e/e9/116f1dee.jpg" width="30px"><span>wy</span> 👍（0） 💬（1）<div>老师，hdfs里面有一个lease（契约）的概念，确保一个文件同一时候只有一个客户端有权限去写，这个算是解决一致性问题的方法吗？</div>2022-03-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1c/bd/27/e653a220.jpg" width="30px"><span>Xiaosong</span> 👍（0） 💬（0）<div>所以gfs的客户端是也在gfs机群里面的吗，不然为什么可以access到集群中每个节点</div>2021-11-19</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJUDU5biaVLrseyIJ4mk4KojBD6N64tqHSX3SazM2eQsfqlic6Ppccgd41eq1eicEmE2WRWPjaHD6S9w/132" width="30px"><span>Geek_025c6d</span> 👍（0） 💬（0）<div>还没看评论，说一下思考题的思路，把要写的电影文件的字节序列拆成多个数据帧，规定每个数据帧包括了写文件时生成的一个全局唯一id，数据段的编号，数据长度和数据，追加写入到文件。读的时候找对应的唯一id，把所有数据段读进来后按数据段号排序即可</div>2021-10-16</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eomCrCRrLAWib0gMI2L2NbicMummlxOY6nVmphsDO0J3xx7OygNd8wJicc88RbNoHrcuXBsKLtCMvgFQ/132" width="30px"><span>zart</span> 👍（0） 💬（0）<div>“这使得客户端读取到的副本中，可能也会存在重复的数据或者空的填充数据，这样的文件系统实在不咋样。”
请问老师这么理解对不对：如果第一次某个副本写失败，主副本告诉客户端重试，但是没有同步master。第二次写入客户端把之前的交互再走一遍。第二次成功后，其实master中记录写了两次。等到客户端读的时候，会把这两次的chunk信息都发给客户端，这个时候客户端就可能读到空数据。</div>2021-10-11</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/QFE00aXGzaS6ibbfJSJsDrpIkqs0OrIYjzZv6L9vZmMhOlut2j24iaeZb0MCQazToE6FRXN960nNiaTrsmw09YjGw/132" width="30px"><span>岁月如歌</span> 👍（0） 💬（0）<div>思考题: 
示例并发写入A-B-C，A写入失败此时客户端将会重试。
方式一： 可在客户端收到重试消息后，向matser发送消息先作废失败元数据，再发请求写入A，保证matser不会保存无效元数据。
方式二： 可在客户端拿到master所有元数据后检查检验和等方式，验证元数据的完整性，不符合要求直接丢弃。</div>2021-10-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/8f/60/be0a8805.jpg" width="30px"><span>陈迪</span> 👍（0） 💬（1）<div>感觉不是很理解这个思考题。写入一部电影，就追加写，然后失败重试，会有啥问题么？

chunk追加写入部分机器失败导致重试，进而导致某些“成功”的机器上存有“重复”的chunk，这个对客户端可见么？应用程序需要处理这个重复块么？我感觉不需要啊，应该是全部写入成功才在Master上面的记录下来这个块位置吧？下次读，虽然这个块物理上存在在机器上，但是客户端通过元数据是看不到这个块的。
</div>2021-10-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/4e/1b/f4b786b9.jpg" width="30px"><span>飞翔</span> 👍（0） 💬（0）<div>master告诉客户端的信息是去哪个机器的哪个chunk读 还是仅仅是哪个机器。让chunkservier决定写在哪个chunk？</div>2021-10-07</li><br/>
</ul>