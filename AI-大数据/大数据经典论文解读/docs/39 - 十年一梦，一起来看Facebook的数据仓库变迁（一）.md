你好，我是徐文浩。

前面两节课，我们是从方法论和具体的实践这两方面，一起了解了Twitter的大数据系统是怎么样的。而在过去的整个课程里，我们也看过大量的来自Google的论文。

发表了大量论文的Google，是开创整个大数据时代的引领者，我们有太多可以学习他们的地方，却很难去模仿他们。那么，团队规模比起来小很多的Twitter，则是一个我们可以直接“抄袭”的对象，我们不仅可以借鉴他们的直接经验，在选择开发什么系统上，也可以模仿他们的整体思路。

而有一家公司，则介于他们两者之间，那就是今天我们要介绍的**Facebook**。比起Twitter，随着逐渐发展壮大的过程，Facebook为开源社区提供了更多广泛使用的开源项目。比起Google，Facebook在大数据系统的开创性工作上却做得不多。

而如果你所在的团队，正好要从各种实践方法的优化，进一步提升到**通过开发新系统来让大数据体系更加易用**，那么从Facebook的数据基建的发展历程中，你可以学到很多东西。

所以今天，就请你和我一起来读一读发表在2010年的《[Data Warehousing and Analytics Infrastructure at Facebook](https://cs.stanford.edu/~matei/courses/2015/6.S897/readings/facebook-warehouse.pdf)》这篇论文。在这篇论文里，Facebook还处于自己发展的早期阶段，而在下节课里，我们还会看到Facebook的数据基建是如何进一步进化的。
<div><strong>精选留言（3）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/15/22/f4/9fd6f8f0.jpg" width="30px"><span>核桃</span> 👍（3） 💬（0）<div>Erasure code的就是EC卷，在hdfs里面现在已经支持了，同时存储的glusterfs这些也支持。所谓的EC卷，和副本卷最大的不同是，节点中存放的副本都是一个完整的，而EC则是分片的，是以前数据分片的进一步优化过来的。因为数据节省空间，可以把一份数据切分成等分的十个分片，但是丢失了则无法找回，因此才有了EC编码。 而常见的是2+1, 4+2,8+2这样的方式其中加号后面的，则是冗余，也就是说，一份完整的数据，会切分成m分，然后计算校验码n份，一个完整的数据就有m+n分，最多允许丢失n份。

那么这里的原理也不难，其实就是一个数据矩阵是否可逆的问题，因为根据大学线性代数的基础，可逆矩阵总是有解的。而为了求解是否可逆，一般使用的是柯西矩阵或者范德蒙矩阵，当然，在工业界上使用的是伽罗瓦域去做的，这样求解的速度会快很多。具体的这些，大家可以去自行搜索纠删码，或者里的所罗门编码。

那么对于EC卷和副本卷，抛开原来来说，最大的问题就是对一致性要求不一样，副本卷极端情况下，只要有一个完整副本，那么都可以恢复回来的，但是EC卷不行，因为是对数据分片了，最多只能损坏n个分片。因此每次写入的时候，至少要保证m个数据分片是完整，否则直接无法恢复了。

另外在工程实现上，目前对于EC卷的实现有两种方式，一种是实时EC，也就是每次写入一堆数据(例如要求满4k，不满的则补齐)，然后对其进行分片并且计算冗余码，然后下发到节点。另外一种则是先写入副本，然后把完整的副本转为EC编码，然后再删掉副本数据。</div>2022-03-23</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJoiar0OoeEdc1l1UiaKKLjKblibqda3fxzibXibiahMqsvAanS3Gzu1CF4xupc6wPzmbpQqr2MMWkGeXeA/132" width="30px"><span>vkingnew</span> 👍（1） 💬（1）<div>有机会请详细的讲解下Erasure code的原理和应用？有些数据不用是否可以销毁？应该有销毁的策略</div>2022-01-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/a6/3d/e44a1395.jpg" width="30px"><span>尚春</span> 👍（0） 💬（0）<div>我们厂目前一些分层的思路：
- 数据湖：主要存放一些对于读取性能要求不那么高的数据（Delta 格式，全量长周期数据），以及 ML 中用到的半结构化数据
- 数仓：主要存放一些需要快速读取的数据用于分析（AWS Redshift，只有近期详细数据和周期性汇总数据）</div>2022-02-05</li><br/>
</ul>