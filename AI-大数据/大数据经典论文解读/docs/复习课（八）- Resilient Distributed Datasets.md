你好，我是黄金。今天我们要来一起回顾复习的论文内容，是开源系统Spark的这篇引入了RDD概念的论文。

## RDD介绍

RDD的全称是弹性分布式数据集，它允许开发人员在大规模集群上，以容错的方式执行内存计算。而Spark就是实现了RDD的分布式计算框架。

在Spark出现之前，并没有通用的分布式计算框架，可以高效地运行迭代算法。MapReduce是通用的分布式计算框架，但不管是Mapper任务还是Reducer任务，它们的执行结果都需要写入硬盘。这样一来，由多个MapReduce组合而成的迭代算法程序，运行起来就不够高效。因此，**如何有效地利用分布式内存**，就成为了研究的重点。

而在MapReduce之后，也出现了一些可以利用分布式内存的计算框架，它们把运算的中间结果保存在内存当中。这些计算框架确实提升了执行效率，但是不够通用，只能服务于特定的算法。

**直到Spark的出现，才有了既高效又通用的分布式内存计算框架。**

## 容错的分布式内存数据集

在设计RDD的时候，主要的挑战就是**如何定义编程接口**，才能让RDD具备有效的故障恢复能力。

我们先来看看MapReduce的结果集是如何容错的。Mapper任务把执行结果写入本地文件，服务器即使宕机，重启后依然可以读取结果。对于不能恢复的服务器，只需要把它负责的任务交给其他服务器，重新执行一遍即可。而Reducer任务是把执行结果写入HDFS，由HDFS提供容错支持。