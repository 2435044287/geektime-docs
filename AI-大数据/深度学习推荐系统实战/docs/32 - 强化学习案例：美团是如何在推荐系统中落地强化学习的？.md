你好，我是王喆。今天我们来聊一聊美团的强化学习落地案例。

我们在[第22讲](https://time.geekbang.org/column/article/315254)中学过强化学习的基本原理、优点，以及微软的强化学习模型DRN，但我们也说了强化学习在推荐系统中落地会有一个难点：因为强化学习涉及模型训练、线上服务、数据收集、实时模型更新等几乎推荐系统的所有工程环节，所以强化学习整个落地过程的工程量非常大，需要工程和研究部门通力合作才能实现。

即使很难，但业界依然有成功落地强化学习的案例，典型的就是美团在“猜你喜欢”功能中的应用。美团曾在官方博客中详细介绍过这一[落地方案](https://tech.meituan.com/2018/11/15/reinforcement-learning-in-mt-recommend-system.html)的技术细节，我们这节课就借助这个方案，来好好学习一下强化学习的实践。我也希望你能通过这个案例，串联起我们学过的所有推荐系统知识。

## 美团的强化学习应用场景

“猜你喜欢”是美团这个团购App中流量最大的推荐展位，产品形态是信息流。从图1的App截图中你可以看到，“猜你喜欢”列表中推荐了用户可能喜欢的餐厅，用户可以通过下滑和翻页的方式实现与App的多轮交互，在这个过程中，用户还可能发生点击、购买等多种行为。

![](https://static001.geekbang.org/resource/image/ce/dc/cef09f967937d6d0d7aeab3f26c8a0dc.jpg?wh=750%2A1334 "图1 美团首页“猜你喜欢”场景")

强化学习的应用场景就藏在用户和美团App的多轮交互之中。如果推荐系统能够在用户每次翻页的时候都考虑到用户刚刚发生的行为，就能够提供实时性更强的推荐体验。图2是美团的同学统计的用户翻页次数的分布图，我们可以看到，多轮交互确实是美团App中非常常见的用户场景，这也说明强化学习还是非常有用武之地的。
<div><strong>精选留言（8）</strong></div><ul>
<li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83epWOEWQYu9icQR8iaiayXyeJpzzrZIF6S4NdkrAGYELyrpnh4GxOicjcj6ZG9PnuuYfzEwMMGB0J1z9Tg/132" width="30px"><span>Geek_e642b8</span> 👍（10） 💬（1）<div>课后思考题：能否用flink直接将梯度下降的算法写到数据流中？不过因为flink是对一个一个数据进行处理，可能梯度下降法就只能用mini-batch=1了</div>2021-01-07</li><br/><li><img src="" width="30px"><span>Liam</span> 👍（4） 💬（1）<div>一直徘徊在 AI 的大门外，看完老师的教程，才真正入门了。作为后端开发者，终于知道怎么在工作中把AI加进来。</div>2020-12-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/ec/64/7403c694.jpg" width="30px"><span>ALAN</span> 👍（2） 💬（3）<div>老师，你好，Advantage 和 Value 函数相关的参数，这部分并不是在 TensorFlow 中训练的，而是利用实时数据流中的样本进行在线学习，并实时调整相关参数值。
请问这个具体是哪种方式实现的了？</div>2020-12-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/27/f8/05/079444c2.jpg" width="30px"><span>创之巅</span> 👍（1） 💬（1）<div>推荐系统太复杂了，知识点太多了。比nlp难多了</div>2021-05-13</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/fcftgBsticCicEEkuzB0GTkHIocX62YVTSvnhR1c94sccj42lVaYXrmcZyhzUI3l9NcvuN1rXLhXt2eBrZZ0Tw7A/132" width="30px"><span>idiot</span> 👍（1） 💬（1）<div>没有mcts的部分？如果只是value net或者q函数的预测，那不是和一般的回归一样吗，只是实时更新模型而已？</div>2021-05-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/63/dd/49123a85.jpg" width="30px"><span>一轩明月</span> 👍（1） 💬（1）<div>1. 借鉴预训练的思路。先全量计算一个 base model，实时数据进来后，在近线层组装样本，用小规模的新样本训练模型进行 fine-tune。数据量小 SGD，mini-batch 都可以用
2. 除了 RL，FTRL 在在线学习也很强</div>2021-01-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/1c/b4/eb71e0f5.jpg" width="30px"><span>李平</span> 👍（0） 💬（0）<div>请问老师，state网络为什么不用在线实时训练呢？每次预测action的时候不是要读取用户状态吗？这个状态又是历史行为，我理解这个历史行为应该是实时的才能更好表征用户兴趣</div>2023-03-07</li><br/><li><img src="" width="30px"><span>Geek_732522</span> 👍（0） 💬（0）<div>美团的这个强化模型的架构和DDPG网络类似，是一个AC架构，Advantage网络类似DDPG网咯中的critic网络，输入是预判的action和当前的state，Value网络类似DDPG网络中的Actor网络，只输入当前的state。DDPG模型就是一种在线更新的深度学习模型，在线训练使用的是mini-batch的梯度下降。</div>2022-11-10</li><br/>
</ul>