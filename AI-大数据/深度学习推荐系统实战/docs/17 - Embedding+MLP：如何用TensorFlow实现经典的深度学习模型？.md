你好，我是王喆。

今天我们正式进入深度学习模型的实践环节，来一起学习并实现一种最经典的模型结构，Embedding+MLP。它不仅经典，还是我们后续实现其他深度学习模型的基础，所以你一定要掌握好。

这里面的Embedding我们已经很熟悉了，那什么叫做MLP呢？它其实是Multilayer perceptron，多层感知机的缩写。感知机是神经元的另外一种叫法，所以多层感知机就是多层神经网络。

讲到这里啊，我想你脑海中已经有这个模型结构的大致图像了。今天，我就以微软著名的深度学习模型Deep Crossing为例，来给你详细讲一讲Embedding+MLP模型的结构和实现方法。

## Embedding+MLP模型的结构

图1 展示的就是微软在2016年提出的深度学习模型Deep Crossing，微软把它用于广告推荐这个业务场景上。它是一个经典的Embedding+MLP模型结构，我们可以看到，Deep Crossing从下到上可以分为5层，分别是Feature层、Embedding层、Stacking层、MLP层和Scoring层。

接下来，我就从下到上来给你讲讲每一层的功能是什么，以及它们的技术细节分别是什么样的。
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/10/5f/e5/54325854.jpg" width="30px"><span>范闲</span> 👍（27） 💬（4）<div>无法直接计算相似度. user embedding 和 item embedding 虽然输入数据来源自同一个数据集，但是本身并不在一个向量空间内.</div>2020-12-02</li><br/><li><img src="" width="30px"><span>Geek_f9ea8a</span> 👍（19） 💬（4）<div>请问老师：

1.看了您实现的MLPRecModel, 由于对tf2.x 版本不是很熟悉，没有看出来 是否 针对 发布时间、电影阅读数等等统计字段 进行了 标准化，
我当时用keras 实现了一下，没有对统计特征进行标准化，效果很差，只有56%左右，损失一直不降，然后最那些统计指标进行标准化话，训练完第一轮，测试集就能达到73%。
2. 您这次实现的MLPRecModel，针对用户、电影，先直接给出的初始化Embedding，然后训练对应的Embedding 权重，
在训练集中，假如 一个用户 有10条样本数据集，那么模型训练该用户Embedding 是根据这10条数据最终训练成能表示该用户Embedding吗？
数据集特征中， 针对一条样本，没有特征表示： 用户历史观看电影 【电影1，电影2 电影3。。。】（按照时间排序） 这样一条特征，对训练Embedding有影响吗，我总感觉好像丢了这部分信息。</div>2020-11-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/28/ac/f1/42508146.jpg" width="30px"><span>月臻</span> 👍（18） 💬（3）<div>老师，您好，我使用Pytorch实现了项目中用到的模型：https:&#47;&#47;github.com&#47;hillup&#47;recommend</div>2021-06-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/25/7c/d4/e9cae41d.jpg" width="30px"><span>科西嘉的怪物</span> 👍（17） 💬（2）<div>如果这个模型的结构变一下，把拼接操作变成user emb点乘item emb直接得到预测评分，那训练出来user emb和item emb就在一个向量空间了吧</div>2021-03-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg" width="30px"><span>浣熊当家</span> 👍（13） 💬（1）<div>想请教下老师，关于embedding训练的原理， 比如图二中Word2Vec的训练我们有输入值X（one-hot向量）， 和输出Y（作为label， multi-hot向量），所以我们可以训练我们的的Word2Vec的词向量，使其得到最贴近训练样本标签的结果。但是我们这节课讲的embedding层只有前一部分，并没有Y（label）这部分，embedding是通过什么原理训练出来的呢？embedding本质是个unsupervised learning吗？</div>2020-11-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1e/c8/d7/909d815d.jpg" width="30px"><span>张程</span> 👍（9） 💬（1）<div>如果是电商推荐商品。没有 0和1 的label, 只有buy还是click的区别，这样的话请问应该如何处理？lable列全部为1吗？谢谢！</div>2020-12-03</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/uzvibJnyK9kXJC0p6hv1tyDk5naXzNwOrjibfMAKAb4dadJ608Gd3FW5PCqh0YCXQxMIRkmP6mpEzvraTCJzGvhA/132" width="30px"><span>Geek_b4af04</span> 👍（7） 💬（3）<div>王喆老师，你好，我最近准备吧这几个模型改写为pytorch版本的代码，在进行category features的embedding的时候，是需要先将这些features变为one hot向量，再将这些one hot的向量变成embedding向量吗？但这样的话有些问题，这样数据大小就爆炸了，比如movie id 的features（19000个数据），从(19000,) -&gt; (19000, 30001) -&gt; (19000, 30001, 10)</div>2020-12-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg" width="30px"><span>那时刻</span> 👍（7） 💬（3）<div>课后思考题，我的想法是在Stacking 层把不同的 Embedding 特征和数值型特征拼接在一起，即把电影和用户的 Embedding 向量拼接起来，形成新的包含全部特征的特征向量，该特征向量保存了用户和电影之间的相似性关系，应该可以直接用来计算用户和物品之间的相似度。</div>2020-11-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg" width="30px"><span>那时刻</span> 👍（5） 💬（1）<div>请问老师，对于数值型特征都有均值和标准差，比如电影评分均值和电影评分标准差两个特征。您在实际工作中，对于数值型特征都会额外加上均值和标准差两个特征吗？</div>2020-11-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/a9/05/6822b8a5.jpg" width="30px"><span>灯灯灯</span> 👍（4） 💬（1）<div>老师你好， 我在运行时会出现以下的waring。请问是我设置的原因还是什么其他原因呢？

WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a &lt;class &#39;collections.OrderedDict&#39;&gt; input: OrderedDict([(&#39;movieId&#39;, &lt;tf.Tensor &#39;ExpandDims_4:0&#39; shape=(None, 1) dtype=int32&gt;), (&#39;userId&#39;, &lt;tf.Tensor &#39;ExpandDims_17:0&#39; shape=(None, 1) dtype=int32&gt;), (&#39;rating&#39;, &lt;tf.Tensor &#39;ExpandDims_7:0&#39; shape=(None, 1) dtype=float32&gt;), (&#39;timestamp&#39;, &lt;tf.Tensor &#39;ExpandDims_9:0&#39; shape=(None, 1) dtype=int32&gt;), (&#39;releaseYear&#39;, &lt;tf.Tensor &#39;ExpandDims_8:0&#39; shape=(None, 1) dtype=int32&gt;), (&#39;movieGenre1&#39;, &lt;tf.Tensor &#39;ExpandDims_1:0&#39; shape=(None, 1) dtype=string&gt;), (&#39;movieGenre2&#39;, &lt;tf.Tensor &#39;ExpandDims_2:0&#39; shape=(None, 1) dtype=string&gt;), (&#39;movieGenre3&#39;, &lt;tf.Tensor &#39;ExpandDims_3:0&#39; shape=(None, 1) dtype=string&gt;), (&#39;movieRatingCount&#39;, &lt;tf.Tensor &#39;ExpandDims_5:0&#39; shape=(None, 1) dtype=int32&gt;), (&#39;movieAvgRating&#39;, &lt;tf.Tensor &#39;ExpandDims:0&#39; shape=(None, 1) dtype=float32&gt;), (&#39;movieRatingStddev&#39;, &lt;tf.Tensor &#39;ExpandDims_6:0&#39; shape=(None, 1) dtype=float32&gt;), (&#39;userRatedMovie1&#39;, &lt;tf.Tensor &#39;ExpandDims_18:0&#39; shape=(None, 1) dtype=int32&gt;), 
...
(&#39;userGenre5&#39;, &lt;tf.Tensor &#39;ExpandDims_16:0&#39; shape=(None, 1) dtype=string&gt;)])
Consider rewriting this model with the Functional API.</div>2021-01-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/23/28/50/c8cb0c3b.jpg" width="30px"><span>。LEAF</span> 👍（4） 💬（1）<div>老师好，想问一个问题，分布式训练模型的时候，loss是直接做sum好，还是求mean呢？</div>2020-11-24</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/55lYKUdcPFgUHibRYmaRiaBdrsmnLGOHdPp4OicjBh197X0vyGa9qAwruEqicAPuUgibXO4Lz5jLudlcbtsqq2p3CpA/132" width="30px"><span>Sebastian</span> 👍（4） 💬（3）<div>老师，在实战中由于数据量庞大，用tf搭建的模型是否需要进行分布式训练？一般是如何分布式训练？之后会有类似的章节讲到吗？</div>2020-11-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/2d/bb/8bd1b6e1.jpg" width="30px"><span>挖掘机</span> 👍（3） 💬（1）<div>老师好，最近正好在做dssm，其中的好几个有疑惑的点一起讨论
1. 当id类特征，比如movieId和userId数量很大时，目前userId大概每天有1000w级别。我选择了先使用hash_bucket来做散列，然后再压缩到300维的embedding。但是我对这种方法有怀疑，就是这么大的数量，会不会出现大量的冲突。这样对准确性是不是有影响。
2. mlp的每层的维度如何确定，是越大越好，还是越小越好。
3. 模型特征可选的目前大概有几千个，如何从中选出重要性最大的特征呢？</div>2021-06-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/97/c4/6c92c78a.jpg" width="30px"><span>小强</span> 👍（3） 💬（1）<div>通过学习code，推荐模型篇的Embedding都是通过tensorflow直接生成。请问在实际应用中，深度学习的模型的Embedding和线上服务篇生成的Embedding (Item2Vec, Graph Embedding)一般都是独立的吗？线上服务篇生成的用于Recall？模型篇生成的用于Sorting?</div>2021-04-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/f3/d1/0663e55c.jpg" width="30px"><span>FayeChen</span> 👍（3） 💬（1）<div>对于multi_hot(比如说历史的观影序列)的特征应该怎么生成embedding呢，感觉还是需要建立字典通过embedding_lookup查找pooling，或者矩阵相乘的方法么</div>2021-03-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/29/9f/1d/049d2002.jpg" width="30px"><span>潘</span> 👍（2） 💬（2）<div>老师您好，userGenre1-userGenre5，movieGenre1-movieGenre3 里面的数据是一致的，one-hot大小是1 *19 ，对应的embedding参数矩阵大小就是19*10 ，那么这八列数据对应要训练8个 19*10的矩阵参数，但是他们的one hot是一样的，请问可否可以共享一个19*10的参数矩阵</div>2021-08-09</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/55lYKUdcPFgUHibRYmaRiaBdrsmnLGOHdPp4OicjBh197X0vyGa9qAwruEqicAPuUgibXO4Lz5jLudlcbtsqq2p3CpA/132" width="30px"><span>Sebastian</span> 👍（2） 💬（8）<div>老师，想问下在划分训练集测试集，怎么就直接使用
test_dataset = raw_samples_data.take(1000)train_dataset = raw_samples_data.skip(1000)
这里的take和skip的意思是选1000条样本作为训练，然后选1000条样本作为测试集吗？
难道不需要按照时间划分吗？</div>2020-12-07</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83epzA7E8kS3g2qezPWGxB4Gb6g6ApXaoyxeibbOV5XlTkDWgg3yHRYtYxlh9GoJcGH8yeSmOz8BKzbA/132" width="30px"><span>Geek_59735b</span> 👍（1） 💬（1）<div>请问下对于 Word2Vec 这样的任务，我们可以清楚的知道 embedding 后的 vector 之间的距离有其现实意义，因为 loss 函数的设计会使得在数据集中常常相邻的词距离相近，但是请问对于 这种 End2End 的应用，embedding 出来的 vector 有具体的现实意义吗 ？ 看 loss 好像看不出其有何具体的意义；</div>2021-09-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/27/54/3d/366462d0.jpg" width="30px"><span>Yvonne</span> 👍（1） 💬（1）<div>老师，请问为什么不同的genres要分开embedding呢，他们可以用shared_embedding_columns吗？如果ta们是一个embedding universe的话</div>2021-06-20</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erPuYd30G4OpyHBJ0pkYPyGhUVyvd5lwzkCow1lvBAINazF8yE957liaGkbVnOJ5oZEHOlRHp9bvGQ/132" width="30px"><span>Geek_cabb26</span> 👍（1） 💬（2）<div>这里训练的不在一个向量空间，如果想直接拿user，item的，可以构图做一个metapath2vec，同时产出两个向量～</div>2021-03-18</li><br/><li><img src="https://wx.qlogo.cn/mmopen/vi_32/TYeIuNZlibjr0eCvnCCTkYnFEgc8t7BialET3Bnsrbv9micpGIvbhwQrw7Zvt9BicThAEPPXojibVteAvQLb0eTO3DA/132" width="30px"><span>cymx66688</span> 👍（1） 💬（1）<div>构造出的这些特征，在模型训练前不用标准化吗</div>2021-03-03</li><br/><li><img src="" width="30px"><span>Sanders</span> 👍（1） 💬（1）<div>课程中对Deep Crossing的实现没有用到Residual Units，Stacking后直接用MLP，是为了简便吗？</div>2021-02-05</li><br/><li><img src="" width="30px"><span>Geek_1e3d35</span> 👍（1） 💬（1）<div>老师好，关于维度问题，模型拼接的输入维度是任意的吗，那么这种情况，在矩阵分解是拼接进辅助信息的向量是否可行？模型的输出维度也是可以作为优化参数进行调整的吗？</div>2021-01-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/a9/05/6822b8a5.jpg" width="30px"><span>灯灯灯</span> 👍（1） 💬（2）<div>老师请问这里从embedding层开始都是线上运行的吗？</div>2021-01-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/f7/d3/2bbc62b2.jpg" width="30px"><span>alexliu</span> 👍（1） 💬（5）<div>老师您好，问个小白的问题，userGenre1-userGenre5，movieGenre1-movieGenre3中的数据都是一样的：
1、为什么设置用户类型为5个，电影类型为3个？
2、为什么这些类型中的词表内容都是一样的？
谢谢老师~</div>2020-12-10</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/qw7rRHUPRzhibxXWLG7kc3zkhZwBn4JZaryzko2eWOjSxDlRvUathHugrIVKhcCqxhtsANUTq0140AlbDkLZmcw/132" width="30px"><span>shenhuaze</span> 👍（1） 💬（4）<div>老师，如果要实现原版Deep Crossing的多层残差网络，是将MLP里的激活函数替换一下就可以了，还是需要做一下结构上的改动？</div>2020-11-22</li><br/><li><img src="" width="30px"><span>myrfy</span> 👍（1） 💬（1）<div>请问老师，这里的embedding训练是框架自动完成的吗？如果使用离线的预训练embedding和这种框架顺带训练的方式相比，哪种会更好呢？</div>2020-11-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2b/2a/f8/1af3bba6.jpg" width="30px"><span>m-rui</span> 👍（0） 💬（1）<div>老师请教下，可能有点小白的问题，在这里userID这种id类的embedding，我的理解训练时候其实和线性层一样，那这样的话，对于线上来说新用户要怎样处理呢？

nlp中通常直接用一个固定的OOV代替所有新的词汇，但是推荐以及其他与用户相关的应用似乎都不能这么做，请问解决方案是什么呢？

即使是您前面提到的聚类用相似用户的embedding做用户冷启动，是不是也仍然存在需要定期重新训练全量ID类embedding的问题？那这样的话成本会不会很高，因为每天都有新用户？</div>2021-11-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/da/4a/b71d3f27.jpg" width="30px"><span>Cjw</span> 👍（0） 💬（2）<div>请问老师，one-hot + embedding 是否可以用labelencode + embedding来实现？ </div>2021-09-06</li><br/><li><img src="" width="30px"><span>Yang Hong</span> 👍（0） 💬（2）<div>老师，想跟您确认一下tensorflow的api：embedding_column的训练样本是类似word2vec的skip-gram模型根据item的时序生成的吗？我们在代码中对movieid，userid和genre都做了embedding，是否可以理解成tensorflow在内部根据timestamp column分别对它们排序好然后切割生成训练样本的呢？</div>2021-07-25</li><br/>
</ul>