你好，我是王喆。

在深度学习推荐系统中，我们经常采用Embedding召回这一准确又便捷的方法。但是，在面对百万甚至更高量级的候选集时，线性地逐一计算Embedding间的相似度，往往会造成极大的服务延迟。

这个时候，我们要解决的问题就是，**如何快速找到与一个Embedding最相似的Embedding？**这直接决定了召回层的执行速度，进而会影响推荐服务器的响应延迟。

今天，我们就一起来学习一下业界解决近似Embedding搜索的主要方法，局部敏感哈希。

## 推荐系统中的“快速”Embedding最近邻搜索问题

在深度学习推荐系统中，我们经常会使用Embedding方法对物品和用户进行向量化。在训练物品和用户的Embedding向量时，如果二者的Embedding在同一个向量空间内（如图1），我们就可以通过内积、余弦、欧式距离等相似度计算方法，来计算它们之间的相似度，从而通过用户-物品相似度进行个性化推荐，或者通过物品-物品相似度进行相似物品查找。

![](https://static001.geekbang.org/resource/image/7f/54/7f7f9647565848d0d530d27d96927654.jpeg?wh=1920%2A1080 "图1 用户和电影的Embedding向量空间")

假设，用户和物品的Embeding都在一个$k$维的Embedding空间中，物品总数为$n$，那么遍历计算一个用户和所有物品向量相似度的时间复杂度是多少呢？不难算出是$O(k×n)$。虽然这一复杂度是线性的，但物品总数$n$达到百万甚至千万量级时，线性的时间复杂度也是线上服务不能承受的。
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/21/8c/74/2bbd132d.jpg" width="30px"><span>Dikiwi</span> 👍（64） 💬（1）<div>b 是 0 到 w 间的一个均匀分布随机变量，避免分桶边界固化。这是什么意思呢？是说可以通过调整b来形成另外一个一个hash函数？</div>2020-11-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/20/46/f4/93b1275b.jpg" width="30px"><span>Alan</span> 👍（40） 💬（4）<div>悄悄告诉大家：embedding层K值的初始判断，有个经验公式:K= Embedding维数开4次方 ,x=初始的维度数；
后续，K值调参按照2的倍数进行调整，例如：2，4，8，16；</div>2021-03-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/e0/14/72a484d8.jpg" width="30px"><span>kaijien</span> 👍（40） 💬（2）<div>老师您好，您提到点数越多越应该增加桶的个数，还有Embedding维度越大越应该增加哈希函数并多用且的方式，那从您的经验上:
1 每个桶维持多少个点比较好？
2 Embedding一般多少算大？比如768维是否应该用且的方式？应该用多少哈希函数比较好？</div>2020-10-31</li><br/><li><img src="" width="30px"><span>Alr</span> 👍（26） 💬（1）<div>课后思考问题：以item_id作为key， item_id对应的BucketId作为value存储在redis， 再以每个BucketId作为key， item_id作为value存储在redis， 在召回的时候遍历item_id的所有BucketId，获取BucketId对应的item_id就是需要召回的item， 请问老师这个思路对吗 </div>2020-12-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg" width="30px"><span>浣熊当家</span> 👍（17） 💬（2）<div>请问老师关于这句话 “在训练物品和用户的 Embedding 向量时，如果二者的 Embedding 在同一个向量空间内”， 我们在之前6-7节embedding的中，讲了怎么把物品序列信息转化为embedding， 想知道，用户的embedding是怎么生成的呢？ 然后，物品和用户在同一个向量空间，这个是怎么得到的呢？</div>2020-10-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/5f/e5/54325854.jpg" width="30px"><span>范闲</span> 👍（12） 💬（1）<div>LSH也有自己的问题。数据量太大的时候，hash的个数不好选择，另外存在hash冲突，容易降低召回率。

同基于树的，基于量化的，基于的图的方法来比，在召回率，速度和内存使用上都不占优势。</div>2020-12-02</li><br/><li><img src="" width="30px"><span>haydenlo</span> 👍（9） 💬（1）<div>请问对于计算距离，欧几里得距离和余弦距离等应该怎么选择？</div>2020-11-07</li><br/><li><img src="" width="30px"><span>Infp</span> 👍（8） 💬（1）<div>本人用过faiss，LSH无论是召回率还是速度方面都不是很好。基于图的HNSW或者HNSWSQ是比较好的索引方式，当然缺点是会占用较大的存储空间，还有很多其他索引方式，可参考faiss的GitHub介绍。另外，faiss的wiki里面有关于如何选择索引的指南，有需要的同学可以去了解一下：https:&#47;&#47;github.com&#47;facebookresearch&#47;faiss&#47;wiki&#47;Guidelines-to-choose-an-index。</div>2021-07-26</li><br/><li><img src="" width="30px"><span>Yang Hong</span> 👍（6） 💬（1）<div>课后思考：
离线训练：LSH model为每个item embedding生成m个分桶，同时为每个user embedding生成m个分桶。

离线存储：1）在redis中存储item的分桶结果，key为item_id， value为item对应的BucketId；建立倒排索引，再以每个BucketId作为key， value为对应的item_id；2）在redis中存储user的分桶结果，key为user_id， value为user对应的BucketId；

在线召回：1）取出目标user的user embedding，和user对应的BucketId；2）查询redis分别获取m个BucketId对应的item_id，用且&#47;或的多桶策略找到需要召回的item。

不知道这个思路对不对。</div>2021-07-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg" width="30px"><span>JustDoDT</span> 👍（6） 💬（1）<div>如何精确的控制每个桶内的点的规模是 C？</div>2020-11-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/4a/4a/fdae1e16.jpg" width="30px"><span>梁栋💝</span> 👍（4） 💬（3）<div>课后思考：
1）首先user embedding是基于历史浏览item embedding平均后生成的，这个一般是online实时计算的。
2）当拿到user embedding后，我们需要离线训练好的LSH model对这个emb向量求出3个分桶。
3）拿到3个分桶，我们需要召回3个桶内的item embedding到内存，再进行Online计算求最近距离。

那么难点在于：
1）冷启动用户没有历史行为，无法用。
2）LSH model怎么导出到online服务使用。</div>2021-01-06</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erUKWZy1fBBcJncWRNh9M3TkjThqgsIIpmGOTCyg2IN80IDf3COkeWyTLHliczAppIkfBgCJTsUn1g/132" width="30px"><span>马龙流</span> 👍（4） 💬（1）<div>Embedding 向量的维度越大，我们越应该增加哈希函数的数量，尽量采用且的方式作为多桶策略；这话怎么理解呢？还有就是faiss这种，里面用到的就是局部哈希原理?</div>2020-11-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg" width="30px"><span>那时刻</span> 👍（4） 💬（2）<div>请问老师局部敏感哈希里的minhash和simhash是否有应用呢？</div>2020-10-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/2d/bb/8bd1b6e1.jpg" width="30px"><span>挖掘机</span> 👍（3） 💬（1）<div>如何判断用户和物品是否在一个向量空间呢？我看后面双塔的时候又说物品和向量不在一个空间，这是如何判断的呢？</div>2021-06-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1f/3a/53/ec2c6c55.jpg" width="30px"><span>杨佳亦</span> 👍（2） 💬（2）<div>请问老师，为什么：

Embedding 向量维度越大，越应增加哈希函数的数量，用“且”分桶；相反，Embedding 向量维度越小，我们越应减少哈希函数的数量，用“或”分桶？

我的理解是，Embedding维度较大，特征密集不好分，采用多个哈希函数做映射再取交的确可以找到相似的Embedding；Embedding维度较小，特征分散，需要的桶不多，用或可以增加结果数量。

但是疑虑在于，Embedding大，需要的桶也多，计算量岂不是会变得非常大？</div>2020-12-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/23/24/13/8fb0424b.jpg" width="30px"><span>follow-fate</span> 👍（2） 💬（2）<div>facebook开源的faiss是不是可以替代LSH了？</div>2020-11-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/23/28/50/c8cb0c3b.jpg" width="30px"><span>。LEAF</span> 👍（2） 💬（1）<div>老师好，最后得到 每个电影的 分桶，比如[-2.0], [14.0], [8.0]]，相当于再做召回的时候，比如使用“或”策略，就直接再剩余所有电影里找到 在[-2.0], [14.0], [8.0] 3个桶里的电影就可以了吧</div>2020-11-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/ec/64/7403c694.jpg" width="30px"><span>ALAN</span> 👍（2） 💬（1）<div>老师，numhashtable为3，是指使用了3个分桶函数吗？</div>2020-10-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/f5/8c/82fb5890.jpg" width="30px"><span>抱小星</span> 👍（1） 💬（1）<div>老师，我想请问一下，假设对item已经做好了分桶，在针对某一用户进行召回时，用户的embedding是根据用户喜爱的多个item的embedding取均值得到。用户embedding不一定会有对应的itemId，那么如何根据用户的信息去取对应item的对应bucket里的其他item呢？是把用户embedding计算过程中的每一个item都拿出来查找bucket再进行召回吗？</div>2021-06-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/15/65/37d05463.jpg" width="30px"><span>pop</span> 👍（1） 💬（1）<div>老师我这样理解对不对：numhashtables是代表分桶个数，分桶越多分桶宽度bucketlength应该要小。然后假设对10万条10维数据召回100条，落在每个桶内数据应该在100到500左右，那么numhashtables应该设为200到1000？（感觉和例子里的4差的好多，我是不是理解错了）</div>2021-03-20</li><br/><li><img src="" width="30px"><span>Geek_91c50b</span> 👍（1） 💬（2）<div>请问，用户的embading和物品embading是通过哪些特征进行处理的，例如性别、年龄？怎样处理成数字的，这一块的代码有在工程里吗？</div>2020-12-02</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Y5U2ADUvruWhziaB4tSyiaAN7h9OcHMGj6X6nAeqJyJvrqWs8JmyO6yOTBziatAEIG6gHRic0jvT3d0hxNhiaAUVYkw/132" width="30px"><span>傻</span> 👍（1） 💬（3）<div>想问下老师，关于“在训练物品和用户的向量时，如果二者在同一个向量空间内”，那如果用户的向量空间（比如用户id的embedding向量）和物品的向量不在同一个向量空间该如何处理呢，谢谢老师～</div>2020-11-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/21/8c/be/f3661bb0.jpg" width="30px"><span>超~~</span> 👍（1） 💬（2）<div>你好老师，我clone下来的代码，环境中没有配置scala 时，可以正常启动，但指定scala2. 11后，启动报错Error:scalac: Token not found: &#47;Users&#47;edz&#47;.idea-build&#47;tokens&#47;3200  谷歌也没有找到答案，谢谢</div>2020-10-30</li><br/><li><img src="" width="30px"><span>Yang Hong</span> 👍（0） 💬（1）<div>老师，你好，我对不同hash函数之间如何确定有点不清楚。我一开始理解的是不同的hash函数是通过随机调整x生成的，通过生成不同的x向量和v之间投影，来捕获点之间在不同投影方向上的距离关系；后来看评论您提到是通过随机调整b来生成多个hash函数。

我疑惑的点是，如果通过随机调整b来生成hash函数，是否hash间共用一个x向量呢？（如果hash函数之间用的一个x向量应该会有问题吧，就像您在课中提到的“原本远离的点会有一定概率变成接近的点”，如果选的这个x向量恰好让原本远离的点投影后距离变近了那么候选集就不准确了）</div>2021-07-19</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIacvl2hoQU11kU97wDuKPKP418MNYrnPU8vA4ibXf2a2wvRh5dkb2h28T5yqW3a05yBSxx75IDhtQ/132" width="30px"><span>Ricardo</span> 👍（0） 💬（1）<div>一个bucketId就是一个分桶结果的索引，只是我比较少看到有负数做索引值的，想问下老师这里的负数没有什么特殊含义吧？也只是代表一个索引而已对吧？</div>2021-06-24</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIacvl2hoQU11kU97wDuKPKP418MNYrnPU8vA4ibXf2a2wvRh5dkb2h28T5yqW3a05yBSxx75IDhtQ/132" width="30px"><span>Ricardo</span> 👍（0） 💬（1）<div>最后得到的bucketId是什么含义呢？为什么还会有负数？</div>2021-06-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/25/72/73/d707c8be.jpg" width="30px"><span>MutouMan</span> 👍（0） 💬（1）<div>桶的宽度和桶数是互相独立吗？我的理解是桶数少，宽度就大。类似quantile transfomer。谢谢！</div>2021-04-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1e/f5/8e/1d68db9a.jpg" width="30px"><span>Geek1591</span> 👍（0） 💬（1）<div>老师您好，为了避免边界固化，采用不同的b，意味着是不同hash函数对么，每个hash函数对应一个分桶，那么实践上说设置三个桶，是优先选择不同的x（投影向量），还是会选择不同的b，或者每个x选择若干个b？</div>2021-04-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/21/9f/36/b534ff31.jpg" width="30px"><span>十年</span> 👍（0） 💬（1）<div>局部敏感哈希看了好多帖子还是没弄明白，老师有没有好的相关资料推介下</div>2021-02-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg" width="30px"><span>浣熊当家</span> 👍（0） 💬（4）<div>还有个关于局部敏感哈希的问题想问老师， 我的理解是LSH也是通过降维的手段来提高搜索top n个点的时间复杂度的。 那具体的降维是应用了那种算法呢？我尝试着搜了下没有找到更细节的资料，也想请老师或者大家推荐好的学习资料。</div>2020-11-01</li><br/>
</ul>