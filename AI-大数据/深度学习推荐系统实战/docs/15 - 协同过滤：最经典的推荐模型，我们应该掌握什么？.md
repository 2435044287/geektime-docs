你好，我是王喆。今天我们要开启推荐模型篇的学习。

推荐模型篇是整个课程中最重要的一个模块，因为推荐模型直接决定了最终物品排序的结果，它的好坏也直接影响着推荐效果的优劣。而且，从某种意义上讲，推荐系统的整体架构都是围绕着推荐模型搭建的，用于支持推荐模型的上线、训练、评估、服务。因此，我一直把**推荐模型称作“推荐系统这个皇冠上的明珠”**。

而提起推荐模型，我们就不能不提协同过滤算法。它可能是推荐系统自诞生以来最经典的算法，且没有之一。虽然我们课程的主题是“深度学习”推荐系统，但协同过滤以及它后续衍生出来的各类模型，都与深度学习推荐模型有着千丝万缕的联系。因此，在进入深度学习模型之前，掌握协同过滤及其衍生模型是非常有必要的。

今天，我就来给你讲讲经典协同过滤和它的衍生模型矩阵分解的原理，以及相关的Spark实现。

## 协同过滤算法的基本原理

我在特征工程篇曾经提到过：**“用户行为数据是推荐系统最常用，也是最关键的数据。用户的潜在兴趣、用户对物品的评价好坏都反映在用户的行为历史中”**。

而协同过滤算法，就是一种完全依赖用户和物品之间行为关系的推荐算法。我们从它的名字“协同过滤”中，也可以窥探到它背后的原理，就是 **“协同大家的反馈、评价和意见一起对海量的信息进行过滤，从中筛选出用户可能感兴趣的信息”**。
<div><strong>精选留言（29）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/21/99/f0/ba3c0208.jpg" width="30px"><span>Geek_63ee39</span> 👍（59） 💬（7）<div>问题2:
可以采用余弦相似度可以消除这种影响，例如:用户甲习惯打高分，对A, B, C三个物品打分为[5, 2, 5]；用户乙习惯打低分，对A, B, C打分为为[3, 1, 3]，虽然这两个评分向量的欧式距离比较远，但它们的余弦相似度比较高，约等于0.96</div>2020-11-08</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/55lYKUdcPFgUHibRYmaRiaBdrsmnLGOHdPp4OicjBh197X0vyGa9qAwruEqicAPuUgibXO4Lz5jLudlcbtsqq2p3CpA/132" width="30px"><span>Sebastian</span> 👍（40） 💬（3）<div>老师好，矩阵分解在工业界落地好像并不常见，从工程实践角度来讲，是有什么特殊的原因吗？</div>2020-11-09</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLaoiaerNMy7eoSA5yfibPNhta51jkhPTTL1dD1HGlnjaGnFQ6Uzbbce82Kpnic3g1JlD7rtm41Y83PA/132" width="30px"><span>Geek_3c29c3</span> 👍（19） 💬（4）<div>老师您好，业务的指标是给不同用户推送可能点击概率比较大的广告，提高不同用户对不同广告的点击率，我这边是利用CTR模型来做的，预测每个用户点击某一个广告的概率，最后发现对于不同的广告，点击概率&gt;0.5的人群重合度很大，目前分析有两个原因，一是测试所用的广告标签类似，导致可能点击用户群体相同；二是最可能的，就是喜欢点击广告的，就是那一波人，另外一波人无论什么广告都没有兴趣点击。老师有遇到过这种情况吗？是怎么解决的？</div>2020-11-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg" width="30px"><span>那时刻</span> 👍（19） 💬（2）<div>请问老师，文中提到的梯度下降方法对共现矩阵进行分解，和传统的SVD矩阵分解，有什么异同么？</div>2020-11-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/50/34/172342fd.jpg" width="30px"><span>吴十一</span> 👍（17） 💬（3）<div>老师，我这边做点击推荐的时候正负样本比例相差很大，除了随机抽样负样本，还有什么比较好的办法呢？</div>2020-11-18</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKHFicKDOJk2zNE09HNL5ykibFV7a9I4r8435Y7P1FJbxzTwTGDDRCfBqYrmQKuHrgLJAV3onrOReTw/132" width="30px"><span>Geek_04634b</span> 👍（16） 💬（1）<div>第一个问题既然已经有物品向量了应该直接求cosine sim取topk就行了，第二个问题，均值方差归一化是最标准的做法，我看评论里有说用cosine的，其实cosine和欧氏距离在l2归一化的条件下在数学上是等价的，本质还是要归一化。</div>2021-02-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/21/93/b4/8571958c.jpg" width="30px"><span>Chaosnow</span> 👍（9） 💬（3）<div>请问老师，MF如何做到迭代增量训练模型呢？每天全量更新做不到的情况下，只针对每天生产出的新数据训练是否会导致效果变差，比如更新了一部分item的向量从而影响到原本的相对距离。</div>2020-11-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/23/13/6e/f1e23980.jpg" width="30px"><span>Macielyoung</span> 👍（6） 💬（1）<div>我觉得消除用户评分偏差可以根据用户的平均评分标准化，即原始向量【x1,x2,x3】变成【x1-xp,x2-xp,x3-xp】，这样有利于弱化个人评分标准不同的影响</div>2021-01-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/e4/a1/2f5b9764.jpg" width="30px"><span>你笑起来真好看</span> 👍（5） 💬（4）<div>如果用户只有隐式因为，那如何构建als模型的数据集呢？</div>2020-11-09</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLaoiaerNMy7eoSA5yfibPNhta51jkhPTTL1dD1HGlnjaGnFQ6Uzbbce82Kpnic3g1JlD7rtm41Y83PA/132" width="30px"><span>Geek_3c29c3</span> 👍（3） 💬（1）<div>老师早，
人口数型特征就是性别年龄、常驻城市、手机型号等呗；
广告的分类结构特征又是什么意思呢？将广告的标签打成泛泛的类别吗？
把特定人群的一些行为泛化出去的意思是让每个广告&gt;0.5的人群覆盖度更广，差别度更大吗？
--------------------------------
之前的问题：
老师您好，业务的指标是给不同用户推送可能点击概率比较大的广告，提高不同用户对不同广告的点击率，我这边是利用CTR模型来做的，预测每个用户点击某一个广告的概率，最后发现对于不同的广告，点击概率&gt;0.5的人群重合度很大，目前分析有两个原因，一是测试所用的广告标签类似，导致可能点击用户群体相同；二是最可能的，就是喜欢点击广告的，就是那一波人，另外一波人无论什么广告都没有兴趣点击。老师有遇到过这种情况吗？是怎么解决的？
---------------------------
作者回复: 这是一个非常好的业界问题。我之前也是做计算广告的，确实有过类似的经历。我感觉从数据上说，第二个原因的可能性非常大，你其实可以分析一下原始的数据，是不是说点击人群的范围确实比较小。

至于解决方法我建议从特征设计的角度入手，看看能不能加入一些能增强模型泛化能力的特征，比如大家都有的一些人口属性特征，广告的分类结构特征之类的，希望能把特定人群的一些行为泛化出去。</div>2020-11-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg" width="30px"><span>JustDoDT</span> 👍（3） 💬（1）<div>常见隐模型矩阵分解有两种
隐语义（隐因子）模型LFM（latent factor model）
LSA(latent semantic analysis)潜在语义分析
SparkMl 的 ALS 实现的是LSA吗？</div>2020-11-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/21/84/fb/b656405c.jpg" width="30px"><span>kim</span> 👍（3） 💬（5）<div>王老师 ： 若采用item2vec的算法 ，输入为用户的观看序列（即观看的电影序列），训练得出一个向量查找表（向量权重），再根据每个观看的电影 embedding的向量与向量权重计算相似度，推荐出相似度比较高的电影，如果我想加入电影的标签（主演，导演）等，应该从那个方面尝试入手？</div>2020-11-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/5f/e5/54325854.jpg" width="30px"><span>范闲</span> 👍（2） 💬（3）<div>问题一:
相似物品推荐可以从item embedding做top n的召回
问题二:
对用用户打分做归一化处理（cur-average)&#47;(max_min)</div>2020-12-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/21/9a/d0/b54a96f2.jpg" width="30px"><span>因子分解机</span> 👍（2） 💬（5）<div>问题2：
可以引入用户和物品偏置项来对用户的打分习惯和物品的被打分情况进行建模。</div>2020-11-10</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLaoiaerNMy7eoSA5yfibPNhta51jkhPTTL1dD1HGlnjaGnFQ6Uzbbce82Kpnic3g1JlD7rtm41Y83PA/132" width="30px"><span>Geek_3c29c3</span> 👍（2） 💬（3）<div>老师您好，我目前做的这个CTR有点偏推荐，不像计算广告，在做CTR预估的时候，发现预测每个广告会点击的用户重叠度很大，也就是说会点广告的永远是那一批人，这个问题一般是怎么解决的啊？？</div>2020-11-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg" width="30px"><span>浣熊当家</span> 👍（2） 💬（1）<div>请问老师，矩阵分解可以算是embedding的一种吗？可以把embedding定义为降维的手段的统称吗？然后我的理解是矩阵分解和item2vec的主要区别有两点：1是输入：矩阵分解用到的是共现矩阵， item2vec用到的是序列矩阵，2是算法：矩阵分解的求解用到了交叉最小二乘，而item2vec用到了神经网络（不清楚神经网络内是不是也有als？），3是结果：矩阵分解直接得到每个用户得电影推荐列表，item2vec得到的是电影的相似度，如果需要进一步得到推荐列表还需要进一步操作。想请教老师我理解的对不对，问题有点多，是我一直都没有弄清楚的，期待老师的指导！</div>2020-11-08</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKHFicKDOJk2zNE09HNL5ykibFV7a9I4r8435Y7P1FJbxzTwTGDDRCfBqYrmQKuHrgLJAV3onrOReTw/132" width="30px"><span>Geek_04634b</span> 👍（1） 💬（1）<div>请问老师如果有上亿user和上亿item也能矩阵分解吗？这需要创建一个1亿*1亿的矩阵。工业界对此是如何处理的。</div>2021-02-19</li><br/><li><img src="" width="30px"><span>haydenlo</span> 👍（1） 💬（1）<div>请问老师，lfm是否也是一种矩阵分解的方法，在实践中感觉计算量很大，在这里用als是否因为基于spark框架且运算量较少？</div>2020-11-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/88/d7/07f8bc6c.jpg" width="30px"><span>sljoai</span> 👍（1） 💬（1）<div>老师能否再解释一下矩阵分解算是embedding的一种？还是不理解</div>2020-11-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1e/c1/31/e991c364.jpg" width="30px"><span>魔法海</span> 👍（0） 💬（1）<div>老师， 我目前在做召回层的工作，在测试als的效果，这个als是不是只能输入3元组（用户id， 搜索词id， 还有喜好程度）？那我如何在als中加入其他相关的信息呢？比如我有2百万的用户，2亿个搜索词，搜索词很可能不会出现重复，喜好程度使用的搜索词在百度上的排名(1-14）,目前生成出来的embeddings出现大量的0，或者很多关键词出现重复，包括用户也是一样的，要想形成更大的差异性，是我自己将其他特征根据权重加成一个数，还是说这个可以喜好程度可以变成一个数组输入？</div>2021-06-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/a8/8e/cb6dd10d.jpg" width="30px"><span>DENNY</span> 👍（0） 💬（3）<div>老师您好，
embedding中用户的embedding是由用户所喜欢的物品的embedding计算获得，所以用户的embedding会和他喜欢的物品embedding相似。
但是矩阵分解的损失函数只保证“用户矩阵和物品矩阵的乘积尽量接近原来的共现矩阵”，那怎么保证分解后的用户隐向量和用户喜欢的物品的隐向量相似呢</div>2021-05-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/42/05/1e32d6f6.jpg" width="30px"><span>鹏程</span> 👍（0） 💬（1）<div>对于问题2，增加一个 bias item 应该也可以建模用户差异吧？</div>2021-02-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/b5/a8/b53b2c91.jpg" width="30px"><span>阿阳</span> 👍（0） 💬（1）<div>矩阵分解也是一种embedding生成的方法，那为什么不将其和Item2vec等embedding方法都放在特征工程篇呢？</div>2021-02-02</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/4icayE3ic5IA7RWwZcrxpMZE4T1WViakEgPsDC3UnhZwU83ad65IjmxPficy0vNZz6Q6vCiclnmyBDc5IYf7soHAXrQ/132" width="30px"><span>Geek_790c43</span> 👍（0） 💬（1）<div>1. 对于隐式行为，如果正反馈为1， 负反馈和无交互都取0会不会有问题？模型如何区分到底是没看过还是不喜欢？
2. 如果数据是视频的播放完成率，能不能把这个隐式行为分箱到1至5的rating再去做als( implicitPrefs=False)</div>2021-01-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/f7/88/da243c77.jpg" width="30px"><span>大魔王</span> 👍（0） 💬（3）<div>问题一:
相似物品推荐不是取topn个相似物品 从 u2i,i2i,或者u2u2i等里计算出就行了吧。
问题二:
for u in sim:
      s = 0
      for v in sim[u][v]:
           s += sim[u][v]
      if s &gt; 0:
         for v in sim[u]:
              sim[u][v] &#47;= s

</div>2020-11-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2c/b9/41/bdcf239f.jpg" width="30px"><span>凌一</span> 👍（0） 💬（0）<div>目前算法应用比较多的是搜广推，目前在网上看到比较多的是推荐算法，NLP，图像识别，想了解下，搜广推的技术原理是否大致一致？召回+排序为主线？算法原理是否相通的？</div>2022-03-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/29/ce/89/72982409.jpg" width="30px"><span>CY</span> 👍（0） 💬（0）<div>王老师您好，请问矩阵分解得到的中间产物user embedding和item embedding，是否算在一个向量空间？我看到您有回复别人说不能保证user embedding和item embedding在同一个向量空间，而是要相乘打分排序。但相乘打分排序其实不就是内积相似度？     另外，在讲deep cross的时候也有回复评论说，如果把user和item embedding从concat变成dot product，就能把user embedding和item embedding放入一个向量空间。此处所谓的dot product，不就是矩阵分解里的计算吗？</div>2022-01-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/23/7a/cf/c42dd74e.jpg" width="30px"><span>sky</span> 👍（0） 💬（0）<div>什么时候回触发推荐呢？感觉推荐没有搜索那么清晰</div>2021-09-12</li><br/><li><img src="" width="30px"><span>Geek_8a732a</span> 👍（0） 💬（0）<div>问题1：从item embedding中找到top n召回
问题2：使用余弦夹角消除这种影响，看了下面说要归一化，学习到了~</div>2021-08-10</li><br/>
</ul>