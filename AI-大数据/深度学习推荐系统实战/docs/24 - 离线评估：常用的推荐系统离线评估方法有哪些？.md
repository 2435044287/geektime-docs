你好，我是王喆。今天我们要进入一个全新的章节，模型评估篇。

在推荐系统这个行业，所有人都在谈效果。就像我们在学习推荐模型篇的时候，你肯定也有过这样的疑问：

- DIEN这个模型的效果到底怎么样啊？
- 我们用深度学习来构建模型到底能让推荐系统效果提高多少啊？
- DeepFM的效果是不是会比Wide&amp;Deep好呢？

**那这个所谓的“效果”到底指的是什么呢？我们一般用什么方法来衡量这个“效果”呢？我们又应该如何根据效果评估的结果来更新模型呢？**这就是模型评估篇要解决的问题。

在所有推荐系统的评估方法中，离线评估是最常用、最基本的。顾名思义，“离线评估”就是我们将模型部署于线上环境之前，在离线环境下进行的评估。由于不用部署到生产环境，“离线评估”没有线上部署的工程风险，也不会浪费宝贵的线上流量资源，而且具有测试时间短，可多组并行，以及能够利用丰富的线下计算资源等诸多优点。

因此，在模型上线之前，进行大量的离线评估是验证模型效果最高效的手段。这节课，我们就来讲讲离线评估的主要方法，以及怎么在Spark平台上实现离线评估。

## 离线评估的主要方法

离线评估的基本原理是在离线环境下，将数据集分为“训练集”和“测试集”两部分，“训练集”用来训练模型，“测试集”用于评估模型。但是如何划分测试集和训练集，其实这里面有很多学问。我总结了一下，常用的离线评估方法主要有五种，分别是：Holdout检验、交叉检验、自助法、时间切割、离线Replay。接下来，我们一一来看。
<div><strong>精选留言（10）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/21/b2/cb/9c6c7bf7.jpg" width="30px"><span>张弛 Conor</span> 👍（100） 💬（3）<div>思考题：离线Replay和RL都是动态更新模型的，都需要不断的测试和再训练模型。增强学习(如DRN)是通过不断接受反馈，在线更新模型的，所以评估方法不能引入未来信息，而简单的时间切割评估方法又不能模拟模型的更新频率，所以离线Replay是增强学习的唯一离线评估方法。</div>2020-12-07</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/dliaGpsxSic6Km3NGL5A3FVOBuQ9qiaUZ1ewCSNPaxxqHBPQ66rc19bRKA9EDy3H1P1wfSMPF4CuTx7X7GPs57CRQ/132" width="30px"><span>Geek_033ad5</span> 👍（4） 💬（1）<div>老师，在交叉检验的例子中，因为是使用的spark，那模型也必须是用spark实现的模型吧？那如果是tf实现的模型，该怎么做交叉检验呢？感谢！</div>2021-01-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/d2/fa/ff691ae0.jpg" width="30px"><span>KongTzeSing</span> 👍（3） 💬（2）<div>老师，我想问问，如果模型用early_stop来调整训练轮数，需要单独拿1天数据当验证集吗，然后测试集是验证集后一天的数据。就是想问上线之后每天跑是否也需要有“验证集”的概念？</div>2020-12-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/14/34/9a96e8d2.jpg" width="30px"><span>浩浩</span> 👍（2） 💬（1）<div>可以用来离线模拟和评估强化学习的在线过程</div>2020-12-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg" width="30px"><span>浣熊当家</span> 👍（2） 💬（7）<div>如果通过划分userID来划分训练集和测试集，是不是也可以避免引入未来信息呢？</div>2020-12-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg" width="30px"><span>那时刻</span> 👍（2） 💬（1）<div>文中提到自助法在 n 次采样之后，将这些没有被抽出的样本作为验证集进行模型验证。如果n次采样之后导致没有被抽出的样本比较多，从而导致验证集比较大，这种情况下，需要抛弃这次采样么？

另外，请问老师一个样本数据有偏斜的问题。比如正例样本有10000例，而反例样本之后100例，采用什么方法对模型进行评估呢？</div>2020-12-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/17/43/928d89a7.jpg" width="30px"><span>Geek_elpkc8</span> 👍（1） 💬（1）<div>老师，有个最近遇到的一个疑问，就是我有30天的数据，我的模型（非rl模型），我现在模型使用数据（时间分割法）是前五天做训练，后一天做测试，以六天为一个窗口进行滑动。但是看到动态replay，想问对于非RL的模型（NLP模型），动态replay评估是否有必要？ 如果采用动态replay那是否需要有一个时间上限？ 比例前15天训练，随后的15天进行replay，一旦完成就完成评估，还是得不断延迟时间观察平均的性能？
不知道描述清楚没有。。。</div>2021-07-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg" width="30px"><span>浣熊当家</span> 👍（1） 💬（3）<div>老师我突然想不清楚了，模型训练中，我们的输入是各种用户 ，产品，场景的特征，然后输出是什么来着？ 
比如说其中一条sample的输入特征会是某个用户A在时间点t (-5) 到 t(0) 的观影序列， t(0) 的 场景特征， t（-5）到 t(0) 时刻的 产品特征， 然后要预测的是t(1) 时刻， 用户A点击（或者评论）的 物品ID这样吗？ 这个物品ID也是个embedding向量吗？</div>2020-12-08</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLaoiaerNMy7eoSA5yfibPNhta51jkhPTTL1dD1HGlnjaGnFQ6Uzbbce82Kpnic3g1JlD7rtm41Y83PA/132" width="30px"><span>Geek_3c29c3</span> 👍（1） 💬（1）<div>老师，书中8.4节有淘宝数据集和亚马逊数据集的AUC对比，请问这些数据源和模型baseline可以在哪里获得啊？</div>2020-12-07</li><br/><li><img src="" width="30px"><span>Geek_13197e</span> 👍（0） 💬（0）<div>请教老师，像传统的FM等模型，输出的数据是Tabular形式的数据，因此这些评估方法都是合理的。但对于DIN这种序列模型，一个正样本对应一个负样本，然后预测Next Item来计算AUC，是不是不用考虑时间切割的问题了。</div>2023-01-16</li><br/>
</ul>