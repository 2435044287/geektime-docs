你好，我是王喆。今天我们来讨论“模型服务”（Model Serving）。

在实验室的环境下，我们经常使用Spark MLlib、TensorFlow、PyTorch这些流行的机器学习库来训练模型，因为不用直接服务用户，所以往往得到一些离线的训练结果就觉得大功告成了。但在业界的生产环境中，模型需要在线上运行，实时地根据用户请求生成模型的预估值。这个把模型部署在线上环境，并实时进行模型推断（Inference）的过程就是模型服务。

模型服务对于推荐系统来说是至关重要的线上服务，缺少了它，离线的模型只能在离线环境里面“干着急”，不能发挥功能。但是，模型服务的方法可谓是五花八门，各个公司为了部署自己的模型也是各显神通。那么，业界主流的模型服务方法都有哪些，我们又该如何选择呢？

今天，我就带你学习主流的模型服务方法，并通过TensorFlow Serving把你的模型部署到线上。

## 业界的主流模型服务方法

由于各个公司技术栈的特殊性，采用不同的机器学习平台，模型服务的方法会截然不同，不仅如此，使用不同的模型结构和模型存储方式，也会让模型服务的方法产生区别。总的来说，那业界主流的模型服务方法有4种，分别是预存推荐结果或Embedding结果、预训练Embedding+轻量级线上模型、PMML模型以及TensorFlow Serving。接下来，我们就详细讲讲这些方法的实现原理，通过对比它们的优缺点，相信你会找到最合适自己业务场景的方法。
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/55lYKUdcPFgUHibRYmaRiaBdrsmnLGOHdPp4OicjBh197X0vyGa9qAwruEqicAPuUgibXO4Lz5jLudlcbtsqq2p3CpA/132" width="30px"><span>Sebastian</span> 👍（15） 💬（3）<div>老师好，关于推荐的机制策略里的疲劳度优化想从工程实践上想再多问一下：内容瀑布流里如何防止重复推荐？最简单的做法是直接过滤用户30分钟内曝光的内容，这种做法过于粗糙，而且忽略的用户的即时兴趣（比如点击过的内容关联的标签）。比较合适的做法是实时采集用户点击过的内容标签，根据标签再进行推荐，但是这种做法如何保证推荐内容不重复呢？从工程上有什么好的方法吗？</div>2020-11-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/23/cf/6a/42ee61a1.jpg" width="30px"><span>找大夫吗</span> 👍（14） 💬（1）<div>老师好 想问下模型部署到线上之后 把flink处理后的特征输入到模型，实时更新用户embedding时，如果需要用到像 ‘年龄’ 这样的基础数据 流处理平台无法提供，是不是以为着依然需要到HDFS去取‘年龄’特征，但是这样是不是会很影响线上服务的效率？ 有什么好的方案呢 ？</div>2021-01-18</li><br/><li><img src="" width="30px"><span>myrfy</span> 👍（13） 💬（1）<div>老师您好，看到上面的介绍，对端到端的理解还不是很清楚。感觉上面介绍的pmml和tf serving都是单向的，从离线模型到线上服务，并没有体现出用端上数据反过来训练模型这个方向。是我对端对端的理解有偏差，还是框架不支持呢</div>2020-11-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/52/d1/a0f5579e.jpg" width="30px"><span>Teddy</span> 👍（12） 💬（3）<div>老师好，模型部署通常需要进行预处理，如果使用tfserving方式部署，由于不是端到端，所以一次推导请求需要进行2次进程间通信，通信开销比较大。因此想了2种方案，1. 自己用c++&#47;go封装预处理,并且在进程中自行loadsavedmodel，这样就把预处理和模型计算放到一个进程处理，减少一次通信。2. 把预处理放入tfserving，运行到gpu机器上，但这种方式又担心浪费有限的gpu资源。并发量大的时候哪种方案好呢？</div>2020-11-19</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/55lYKUdcPFgUHibRYmaRiaBdrsmnLGOHdPp4OicjBh197X0vyGa9qAwruEqicAPuUgibXO4Lz5jLudlcbtsqq2p3CpA/132" width="30px"><span>Sebastian</span> 👍（6） 💬（1）<div>老师好，想问下在线服务是否会涉及一些推荐的机制策略？比如流量控制，多样性，疲劳度优化等等？流量控制一般又有什么手段实现？这方面有什么资料可以推荐吗？谢谢！</div>2020-11-11</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLaoiaerNMy7eoSA5yfibPNhta51jkhPTTL1dD1HGlnjaGnFQ6Uzbbce82Kpnic3g1JlD7rtm41Y83PA/132" width="30px"><span>Geek_3c29c3</span> 👍（5） 💬（1）<div>老师，你好，想问一下如果是sklearn的模型，上线就是PMML最合适不过了吧。sklearn导出到PMML格式的文件我会，后面服务器利用JPMML调用模型文件，编写预测逻辑，生成服务，然后并发调用，这一系列不太会操作，还有服务器怎么选择服务架构，对后端的东西不太熟悉，有相关的资料可以学习吗？</div>2020-12-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/28/a1/fd2bfc25.jpg" width="30px"><span>fsc2016</span> 👍（4） 💬（2）<div>老师，有2个问题请教您。
1，模型的离线训练完，部署到线上，产生推荐结果，根据用户行为反馈数据，然后在更新模型。实际工作上模型更新频率是怎么样，是根据各推荐指标按需进行调整嘛
2，文中说TensorFlow serving部署后，需要考虑性能优化问题，这个主要是指推荐服务器高并发请求下，保证准确，及时产生推荐结果嘛</div>2020-11-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/23/52/4b/7a66598d.jpg" width="30px"><span>嗅嗅的小胖🐷</span> 👍（3） 💬（1）<div>老师你好，如果是高并发大流量的场景下tfserving延迟会不会有问题，和普通的ps架构来讲那个会更好一些</div>2021-01-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/b7/8d/b27b89cc.jpg" width="30px"><span>啊黄黄黄</span> 👍（2） 💬（1）<div>老师好，我现在排序过程中利用tf-serving加载模型，这里耗时很严重有什么好的方法可以解决的嘛？</div>2021-01-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/23/a4/2b/c8f97136.jpg" width="30px"><span>旦旦</span> 👍（2） 💬（4）<div>王喆老师好，想请问下tensorflow训练的深度模型想要离线部署到spark集群有什么好的解决办法吗？</div>2020-11-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1b/6d/c5/c0665034.jpg" width="30px"><span>Wiiki</span> 👍（2） 💬（1）<div>王老师，您好。我按照您说的步骤，在window7上面通过docker toolbox安装好了docker，然后在docker toolbox上pull tensorflow serving镜像。再把tensorflow测试模型文件下载到本地，并配置TESTDATA地址，然后docker run服务。最后报错了：error response form daemon: invalid mode: &#47;models&#47;half_plus_two。不知道是哪里出了问题，麻烦解答一下呀~  谢谢</div>2020-11-03</li><br/><li><img src="" width="30px"><span>Geek_060174</span> 👍（1） 💬（2）<div>老师好。想问下如果既想用模型提供在线服务，又想根据新样本，实时更新这个在线模型。用什么架构比较合适呢？像老师课上讲到的这几种服务方式，比如pmml或者tf server，模型都是离线训练好的，可以实时根据用户动态更新吗。以最简单的lr或者fm模型为例，有没有办法让模型既提供可靠在线服务，又能根据到来的新数据实时更新呢？一般怎么做呢？</div>2021-11-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/67/83/2150e0e0.jpg" width="30px"><span>longhx</span> 👍（1） 💬（2）<div>tfserving方式怎么加载外部预先训练好的的embedding数据呢</div>2021-07-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/17/43/928d89a7.jpg" width="30px"><span>Geek_elpkc8</span> 👍（1） 💬（1）<div>之前使用tfserving来部署模型，但是担心rpc或者http网络传输带来的延时，就选择了用Java load 训练好的tf（仅支持1.15）模型来做推理，然后整个算法服务打包成docker镜像来处理。</div>2021-04-02</li><br/><li><img src="" width="30px"><span>Sanders</span> 👍（1） 💬（1）<div>09#中讲到的推荐服务和模型服务是怎么结合的？是不是两者运行在Jetty + TF Serving一个容器运行时中，他们之间通过localhost的方式进行GRPC调用？感觉这样虽然耦合度高一些，但是会减少网络通信开销。</div>2021-02-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/20/37/c3/090a3519.jpg" width="30px"><span>徐子琨</span> 👍（1） 💬（3）<div>老师好，预存推荐结果的缺点提到了用户规模大消耗存储资源的问题，有点疑惑不是很明白。不考虑多个场景复用特征，离线将所有推荐结果计算完存储在Redis中，假设每个用户返回的列表不是特别长（50左右），N个用户就存了N个长度为50的list。那么如果不用预存推荐结果这种方式，用户特征本身规模也是不小的，将所有用户的特征都存到Redis中的这个存储开销似乎并不比预存推荐结果小。另外我觉得预存推荐结果的缺点还有浪费离线计算资源，毕竟相比于在线inference，大部分推荐结果都是用不到的</div>2021-01-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/b6/88/e8deccbc.jpg" width="30px"><span>romance</span> 👍（1） 💬（1）<div>王老师，请教下，在做企业知识库系统时，想使用推荐系统给用户推荐知识，这个场景可以用“预存推荐结果或 Embedding 结果”这种方案吧？</div>2020-12-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/c4/eb/2285a345.jpg" width="30px"><span>花晨少年</span> 👍（1） 💬（1）<div>有个疑惑，请问tf serving怎么解决预训练embedding+轻量级预估的问题呢，如果用tf serving方案，MIMN不还是割裂的两部分吗，并不是端到端的啊。</div>2020-12-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2b/2a/f8/1af3bba6.jpg" width="30px"><span>m-rui</span> 👍（0） 💬（1）<div>老师请问下，如果是树比如lgbm和pytorch这种是的混合模型，应该怎样部署线上呢，似乎不能直接用tfserving或者pmml了？不是很了解后端…</div>2021-11-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2b/19/54/1547a58a.jpg" width="30px"><span>与光25</span> 👍（0） 💬（1）<div>老师，新手小白 不太懂 TESTDATA不是内部或外部命令,也不是可运行的程序是怎么回事啊？</div>2021-11-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/29/be/56/6a2998ba.jpg" width="30px"><span>คิดถึง </span> 👍（0） 💬（1）<div>老师好，我想问下您在online-&gt;recprocess-&gt;RecForYouProcess中实现的callNeuralCFTFServing方法，该这么调用呢？还有您的ncf模型文件保存在哪里了？</div>2021-09-03</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/FydBiarFhvwAxkA2fL3IpEGcd31xAfo8WicAgxreCSbPgo0IdcHkAwZQDZDiaeAJg7u2UKqSEmzr8Mopf9lFY1IEw/132" width="30px"><span>郭翊麟</span> 👍（0） 💬（1）<div>王老师 您好 JPMML和MLeap 的项目库链接 失效了，能否给一下网址</div>2021-08-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/25/71/8e/31458837.jpg" width="30px"><span>时不时充充电</span> 👍（0） 💬（1）<div>想问一下老师，业界有没有成熟的MLOps解决方案？</div>2021-06-28</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erB5iaXSFdfgPL0uH2teHssfVEjoI7lib9xiao57XpBIjiadlIq2rjh8K2jUmNRnteEpZiaA9IX37csowA/132" width="30px"><span>chouisbo</span> 👍（0） 💬（1）<div>老师您好，期望能讲讲Inference部分模型量化，模型蒸馏方面的内容。</div>2021-04-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/c8/b8/27e45560.jpg" width="30px"><span>芝乎者也</span> 👍（0） 💬（3）<div>成功启动tensorflow servering之后，使用curl 命令来发送 HTTP POST 请求，报了以下错误，至今木有找到解决方案，不知道老师或者哪位大佬能教教我
curl: (3) bad range in URL position 2:
[1.0,</div>2021-01-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/c9/f8/72955ef9.jpg" width="30px"><span>浣熊当家</span> 👍（0） 💬（2）<div>很不好意思问老师一个小白的问题，下面这个TESTDATA这句命令也是直接在command Prompt里直接打么，我试了，得到错误信息“TESTDATA is not recognized as an internal or external command...” 是不是应该在别的地方打这句啊？
TESTDATA=&quot;$(pwd)&#47;serving&#47;tensorflow_serving&#47;servables&#47;tensorflow&#47;testdata&quot;</div>2020-11-13</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/WxqITTQCDjfhkfN1NuekibAQbDW3bEvXey40fNVkvAx877IQMmNYA98FA19iao3hLCfWlSCAkp4AFibAJMsh1n64A/132" width="30px"><span>Geek_ad1a2b</span> 👍（7） 💬（1）<div>win10环境遇到以下问题，应该是由于不同系统间命令的差异导致的
（1）TESTDATA路径变量无法设置，可跳过这一步，在下面对应位置用绝对路径替代即可
（2）docker run命令，下面的可运行命令取自评论区，路径部分内容还要根据你的具体路径修改
docker run -t --rm -p 8501:8501 -v &quot;C:&#47;Users&#47;lenovo&#47;serving&#47;tensorflow_serving&#47;servables&#47;tensorflow&#47;testdata&#47;saved_model_half_plus_two_cpu:&#47;models&#47;half_plus_two&quot; -e MODEL_NAME=half_plus_two tensorflow&#47;serving &#39;&amp;&#39;
（3）crul命令
curl http:&#47;&#47;127.0.0.1:8501&#47;v1&#47;models&#47;half_plus_two:predict -d &quot;{\&quot;instances\&quot;: [1.0, 2.0, 5.0]}&quot;
参考于https:&#47;&#47;blog.csdn.net&#47;xxw52mao1&#47;article&#47;details&#47;98073095</div>2022-03-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/21/96/fa/b8bc5857.jpg" width="30px"><span>Lin D.</span> 👍（7） 💬（1）<div>如果是在windows环境下运行tensorflow serving 建议参考这个视频
https:&#47;&#47;www.youtube.com&#47;watch?v=uabNEQlpGM8</div>2021-02-16</li><br/><li><img src="" width="30px"><span>Geek_209909</span> 👍（7） 💬（3）<div>[已解决，分享一下给可能有需要的人]

我在Windows下的Git命令行里照着输入这些指令碰到了下面的问题，运行docker run那个指令报错：
E tensorflow_serving&#47;sources&#47;storage_path&#47;file_system_storage_path_source.cc:364] FileSystemStoragePathSource encountered a filesystem access error: Could not find base path &#47;models&#47;half_plus_two for servable half_plus_two
报错说找不到模型文件。

原因：$TESTDATA这里的路径前面是&quot;&#47;c:&#47;xxx&#47;&quot;这种，识别不了

解决方法：手动输入路径，把c盘前面的&#39;&#47;&#39;符号去掉，就本来是‘&#47;c:&#47;xxx&#47;xxx’改成‘c:&#47;xxx&#47;xxx’就可以了</div>2021-01-22</li><br/><li><img src="" width="30px"><span>Geek_8a732a</span> 👍（3） 💬（0）<div># 启动TensorFlow Serving容器，在8501端口运行模型服务API
docker run -t --rm  -p 8501:8501 -v &quot;C:\Users\Think\serving\tensorflow_serving\servables\tensorflow\testdata\saved_model_half_plus_two_cpu:&#47;models&#47;half_plus_two&quot; -e MODEL_NAME=half_plus_two tensorflow&#47;serving &#39;&amp;&#39;

# 请求模型服务API
curl -Uri &#39;http:&#47;&#47;localhost:8501&#47;v1&#47;models&#47;half_plus_two:predict&#39; -Body &#39;{&quot;instances&quot;:[1.0, 2.0, 5.0]}&#39; -Method &#39;POST&#39;</div>2021-08-10</li><br/>
</ul>