你好，我是黄鸿波。

上节课，我们使用Scrapy框架已经能够爬取了新浪网的新闻数据，并且，我们也做了相应的翻页爬取功能。

这节课，我们就在上一节课的程序中做一个补充，加入参数传递和数据库存储相关功能（使用MongoDB数据库进行存储）。

## Python中的pymongo库

如果要想在Python中操作MongoDB数据库，首先我们要了解一下pymongo这个库。

pymongo准确来讲是一个连接Python和MongoDB数据库的一个中间的驱动程序，这个程序可以使Python能够非常方便地使用和操作MongoDB数据库。在Python中，我们可以使用pip install pymongo的方式来安装。

接下来，我们就在我们的cmd环境中来安装我们的pymongo库。首先，我们使用下面的命令切换到我们的anaconda环境中。

```
activate scrapy_recommendation
```

如果你是Linux或者Mac用户，则需要把命令改成下面这样。

```
conda activate scrapy_recommendation
```

紧接着，我们使用下面的命令来安装pymongo。

```
pip install pymongo
```

安装完成之后，如下图所示。

![](https://static001.geekbang.org/resource/image/f7/30/f70dd8653c160486e21a8c0ab25d4430.png?wh=1443x417)

接着，我们就可以尝试着在我们的Python环境中使用它。

想要在Python环境中使用pymongo库，我们需要经过下面四个步骤。
<div><strong>精选留言（9）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg" width="30px"><span>peter</span> 👍（0） 💬（1）<div>第07课，创建爬虫程序出错的问题解决了，解决方法如下：
E:\Javaweb\study\recommendsys，这个目录下面有env和project两个子目录，Anaconda安装在env下面，虚拟环境在Anaconda的安装目录下面。爬虫项目在project目录下面。然后执行“scrapy genspider sina_spider sina.com.cn”时候报错了几次。
后来在E盘下创建geekbang，和老师的目录一样，又创建了一个新的虚拟环境，照着老师的流程做下来，就成功创建了爬虫程序。
问题解决了，不知道具体原因，感觉是目录问题。对于这个问题，老师如果有看法就告诉我一下，比如目录位置有一些坑。</div>2023-05-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg" width="30px"><span>peter</span> 👍（0） 💬（1）<div>第07课，创建爬虫程序出错的问题解决了，解决方法如下：
E:\Javaweb\study\recommendsys，这个目录下面有env和project两个子目录，Anaconda安装在env下面，虚拟环境在Anaconda的安装目录下面。爬虫项目在project目录下面。然后执行“scrapy genspider sina_spider sina.com.cn”时候报错了几次。
后来在E盘下创建geekbang，和老师的目录一样，又创建了一个新的虚拟环境，照着老师的流程做下来，就成功创建了爬虫程序。
问题解决了，不知道具体原因，感觉是目录问题。对于这个问题，老师如果有看法就告诉我一下，比如目录位置有一些坑。
运行main.py后，遇到两个问题：
Q1：ROBOTSTXT_OBEY = True时爬取失败
创建main.py后，settings.py中，ROBOTSTXT_OBEY = True。 运行后报告：
.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file。
该错误导致另外一个错误：
robotstxt.py&quot;, line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode(&quot;utf-8&quot;)
UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xc3 in position 93: invalid continuation byte
网上建议ROBOTSTXT_OBEY 改成 False，好像是成功了。
Q2：成功信息和老师的不同，不确定是否成功。
A 专栏上的成功信息很少，我这里的信息非常多，是log设置不同吗？（我用PyCharm4.5）。
B 专栏上的成功信息，有两个链接：
Get https:&#47;&#47;news.sina.com.cn&#47;robots.txt
Get https:&#47;&#47;news.sina.com.cn&#47;china
但我的输出信息中并没有这两个链接，还没有成功吗？
部分信息如下：
2023-05-02 12:56:00 [scrapy.core.engine] INFO: Spider opened
2023-05-02 12:56:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages&#47;min), scraped 0 items (at 0 items&#47;min)
2023-05-02 12:56:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2023-05-02 12:56:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to &lt;GET https:&#47;&#47;www.sina.com.cn&#47;&gt; from &lt;GET http:&#47;&#47;sina.com.cn&#47;&gt;
2023-05-02 [engine] DEBUG: Crawled (200) &lt;GET https:&#47;&#47;www.sina.com.cn&#47;&gt; (referer: None)
2023-05-02 [scrapy.core.engine] INFO: Closing spider (finished)
2023-05-02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:</div>2023-05-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2b/d1/3b/a94459d2.jpg" width="30px"><span>GhostGuest</span> 👍（0） 💬（1）<div>第 23 行中 CONCURRENT_REQUESTS 参数解释有误, 这个参数是设置线程数，并不是多线程的开关，文中描述开启就可以利用多线程进行爬取有点歧义了，默认就是多线程爬取的，这个参数只是设置并发量</div>2023-04-30</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/mhSYbmpwSzVIEDu714dQuicXCf4ssKQ3LictIW6VoCFZ17EdanhRnhHEHmReiatJBrkUsfkXl4FsWU1JkoHqDiaxKA/132" width="30px"><span>19984598515</span> 👍（0） 💬（1）<div>老师你好，什么时候能有完整源码呢</div>2023-04-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg" width="30px"><span>peter</span> 👍（0） 💬（1）<div>调用流程是在哪里定义的？比如，对于pipelines.py文件，框架会自动调用它，假如我改变文件名字，应该就无法正常运行了；如果知道流程调用关系在哪里定义，就可以在那里修改文件名字。 （是settings.py吗？）</div>2023-04-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/23/bb/a1a61f7c.jpg" width="30px"><span>GAC·DU</span> 👍（0） 💬（1）<div>把已经爬过的新闻标题，用Redis set集合存储，作为增量过滤器，如果标题不在集合中则抓取数据并保存数据，同时将新的标题放到集合中。

测试成功，部分代码如下：
redis：
class RedisDB:
    def __init__(self):
        self.host = &quot;ip地址&quot;
        self.port = 6379
        self.passwd = &quot;密码&quot;
        self.db = 2
        self.pool = redis.ConnectionPool(host=self.host, port=self.port, password=self.passwd, db=self.db, decode_responses=True)
        self.client = redis.Redis(connection_pool=self.pool)

    def add(self, key, value):
        return self.client.sadd(key, value)

    def exists(self, key, value):
        return self.client.sismember(key, value)

spider部分:
if self.redis.exists(&quot;sina&quot;, items[&quot;news_title&quot;]):
    continue
self.redis.add(&quot;sina&quot;, items[&quot;news_title&quot;])

那里有问题，请老师指正，谢谢。</div>2023-04-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/3c/9d/cf/96713901.jpg" width="30px"><span>Baird</span> 👍（0） 💬（0）<div>增量爬取：
pipelines.py
新增一个方法获取MongoDB集合中的最新时间
def get_last_crawl_time(self):
    latest_entry = self.collection.find_one(
        sort=[(&quot;times&quot;, -1)],  # 按 times 字段降序排序
        projection={&quot;times&quot;: 1} 
    )
    if latest_entry:
        return latest_entry[&quot;times&quot;]
    return None
sina_spider.py
初始化一个sinaPipeline实例，调用get_last_crawl_time获取上次爬取新闻的最后时间
def __init__(self):
    self.start_urls = [&#39;https:&#47;&#47;news.sina.com.cn&#47;china&#47;&#39;]
    self.option = Options()
    self.option.add_argument(&#39;--no-sandbox&#39;)
    self.option.add_argument(&#39;--blink-settings=imagesEnabled=false&#39;)
    self.pipeline = SinaPipeline()
    self.last_crawl_time = self.pipeline.get_last_crawl_time()
循环中判断时间是否小于上次爬取的新闻日期，是的话跳过这次循环
if &#39;分钟前&#39; in eachtime:
    minute = int(eachtime.split(&#39;分钟前&#39;)[0])
    t = datetime.datetime.now() - datetime.timedelta(minutes=minute)
    t2 = datetime.datetime(year=t.year, month=t.month, day=t.day, hour=t.hour, minute=t.minute)
else:
    if &#39;年&#39; not in eachtime:
        eachtime = str(today.year) + &#39;年&#39; + eachtime
    t1 = re.split(&#39;[年月日:]&#39;, eachtime)
    t2 = datetime.datetime(year=int(t1[0]), month=int(t1[1]), day=int(t1[2]), hour=int(t1[3]),
                           minute=int(t1[4]))

if  t2 &lt;= self.last_crawl_time:
    continue</div>2024-10-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/21/67/fe/5d17661a.jpg" width="30px"><span>悟尘</span> 👍（0） 💬（0）<div>有微信群了吗？我遇到了一些问题，pymongo连不上mongo数据库</div>2023-12-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/25/87/f3a69d1b.jpg" width="30px"><span>peter</span> 👍（0） 💬（0）<div>运行main.py后，遇到两个问题：
Q1：ROBOTSTXT_OBEY = True时爬取失败
创建main.py后，settings.py中，ROBOTSTXT_OBEY = True。 运行后报告：
.robotstxt] WARNING: Failure while parsing robots.txt. File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file。
该错误导致另外一个错误：
robotstxt.py&quot;, line 15, in decode_robotstxt
    robotstxt_body = robotstxt_body.decode(&quot;utf-8&quot;)
UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xc3 in position 93: invalid continuation byte
网上建议ROBOTSTXT_OBEY 改成 False，好像是成功了。
Q2：成功信息和老师的不同，不确定是否成功。
A 专栏上的成功信息很少，我这里的信息非常多，是log设置不同吗？（我用PyCharm4.5）。
B 专栏上的成功信息，有两个链接：
Get https:&#47;&#47;news.sina.com.cn&#47;robots.txt
Get https:&#47;&#47;news.sina.com.cn&#47;china
但我的输出信息中并没有这两个链接，还没有成功吗？
部分信息如下：
2023-05-02 12:56:00 [scrapy.core.engine] INFO: Spider opened</div>2023-05-03</li><br/>
</ul>