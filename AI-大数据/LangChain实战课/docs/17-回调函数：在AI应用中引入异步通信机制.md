你好，我是黄佳，欢迎来到LangChain实战课！

这节课我们一起来学习一下LangChain中的回调函数。

## 回调函数和异步编程

回调函数，你可能并不陌生。它是函数A作为参数传给另一个函数B，然后在函数B内部执行函数A。当函数B完成某些操作后，会调用（即“回调”）函数A。这种编程模式常见于处理异步操作，如事件监听、定时任务或网络请求。

> 在编程中，异步通常是指代码不必等待某个操作完成（如I/O操作、网络请求、数据库查询等）就可以继续执行的能力。异步机制的实现涉及事件循环、任务队列和其他复杂的底层机制。这与同步编程形成对比，在同步编程中，操作必须按照它们出现的顺序完成。

下面是回调函数的一个简单示例。

```plain
def compute(x, y, callback):
    result = x + y
    callback(result)

def print_result(value):
    print(f"The result is: {value}")

def square_result(value):
    print(f"The squared result is: {value**2}")

# 使用print_result作为回调
compute(3, 4, print_result)  # 输出: The result is: 7

# 使用square_result作为回调
compute(3, 4, square_result)  # 输出: The squared result is: 49
```
<div><strong>精选留言（7）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/a0/a4/b060c723.jpg" width="30px"><span>阿斯蒂芬</span> 👍（1） 💬（1）<div>笔记mark：


1.疑似纠错：
第二个代码示例后写道：【compute 函数开始执行。当它遇到 await asyncio.sleep(2) 时，它会暂停】
但是代码中是 await asyncio.sleep(0.5)，休眠时长会影响最终程序的打印输出顺序；
后面花卉部分 【在 MyFlowerShopAsyncHandler 中，我们使用了 await asyncio.sleep(0.3) 】也与代码中的 await asyncio.sleep(0.5) 不一致；
如果属实，还是建议修改下，否则容易造成困惑


2. 自定义回调函数 代码报错问题
一开始直接使用老师的代码，未能获得流式响应的打印，出现报错：
Retrying langchain.chat_models.openai.acompletion_with_retry.&lt;locals&gt;._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI.
但是此时非流式的响应，是能够正常完成的；
折腾许久，定位到是openai代理的问题，但是流式响应是通过SSE协议，此时vpn似乎被绕过了，将vpn代理显示添加到 OPENAI_PROXY 环境变量后解决

3. 最后的 additional_interactions() 示例中，可以将 asynio.gather 的返回结果打印出来，能够看到每个任务使用的token数量，与最终的总数是一致的</div>2023-10-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/a0/a4/b060c723.jpg" width="30px"><span>阿斯蒂芬</span> 👍（1） 💬（1）<div>来交作业了：

思考题1：
在确保效果相同的三轮交互中，使用了其他记忆机制，并记录令牌使用情况（分别测试三次取范围值）
ConversationBufferMemory: 1000 ~ 1500
ConversationBufferWindowMemory(k=2): 1200 ~ 1600
ConversationSummaryMemory: 2000 ~ 2500
ConversationSummaryBufferMemory(max_token_limt=300): 1000~1500

大致看也符合估算示意图第一阶段 0-5 interacitions 的走势，ConversationSummaryMemory 增长较快，其他几个增长速率较为一致


思考题2：
使用LLMChain的 run 方法也可以传递callback

class MyCallBackHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs) -&gt; None:
        print(f&quot;recv token: {token}&quot;)


llm = ChatOpenAI(streaming=True)
prompt = PromptTemplate.from_template(&quot;1 + {number} = &quot;)


chain = LLMChain(llm=llm, prompt=prompt)
chain.run(number=2, callbacks=[MyCallBackHandler()])

其实这个示例就是从老师的延伸阅读中“拿来”的，不知答对没</div>2023-10-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/21/67/fe/5d17661a.jpg" width="30px"><span>悟尘</span> 👍（0） 💬（1）<div>老师，这节课少了输出结果的演示~理解起来有点费劲</div>2023-11-12</li><br/><li><img src="" width="30px"><span>yanyu-xin</span> 👍（1） 💬（0）<div>###3 将课程代码到大模型修订为国产通义千问模型（03_LangChainOpenAICallback.py）
###“用 get_openai_callback 构造令牌计数器”代码。用 ChatOpenAI和千问模型平替 OpenAI
# 原代码3
llm = OpenAI()
# 新代码3
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
        model_name=&quot;qwen-turbo&quot;,
        stream_usage=True,
        api_key= “API_KEY”, #填写你自己的DASHSCOPE_API_KEY
        base_url=&quot;https:&#47;&#47;dashscope.aliyuncs.com&#47;compatible-mode&#47;v1&quot;, 
    )

###4 修改“additional_interactions 异步函数”代码（03_LangChainOpenAICallback.py），将llm.agenerate中的参数用 langchain_core.messages 修订。

# 旧代码4
import asyncio
async def additional_interactions():
    with get_openai_callback() as cb:
        await asyncio.gather(
            *[llm.agenerate([&quot;我姐姐喜欢什么颜色的花？&quot;]) for _ in range(3)]
        )
    print(&quot;\n另外的交互中使用的tokens:&quot;, cb.total_tokens)
asyncio.run(additional_interactions())

# 新代码4
import asyncio
from langchain_core.messages import HumanMessage, SystemMessage
messages = [[
            SystemMessage(content=&quot;你是个花店小助手。&quot;),
            HumanMessage(content=&quot;我姐姐喜欢什么颜色的花？&quot;),
        ]]

#进行更多的异步交互和token计数
async def additional_interactions():
    print(&quot;\n开始进行更多的交互...&quot;)
    with get_openai_callback() as cb:
        await asyncio.gather(

            *[llm.agenerate(messages) for _ in range(3)]
        )
   print(f&quot;交互{i+1}使用的tokens: {cb.total_tokens}&quot;)  
asyncio.run(additional_interactions())</div>2024-08-31</li><br/><li><img src="" width="30px"><span>yanyu-xin</span> 👍（1） 💬（0）<div>将课程代码到大模型修订为国产通义千问模型：
###1 修改“在组件中使用回调处理器”代码，用 ChatOpenAI和千问模型平替 OpenAI

# 原代码1
llm = OpenAI()
# 新代码1
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
        model_name=&quot;qwen-turbo&quot;, #用通义模型
        api_key=“API_KEY”, #填写你自己的DASHSCOPE_API_KEY
        base_url=&quot;https:&#47;&#47;dashscope.aliyuncs.com&#47;compatible-mode&#47;v1&quot;
	）
————

###2 修改“自定义回调函数”代码，将通义模型平替OpenAI
# 原代码2
flower_shop_chat = ChatOpenAI(
        max_tokens=100,
        streaming=True,
        callbacks=[MyFlowerShopSyncHandler(), MyFlowerShopAsyncHandler()],
    )
# 新代码2
    flower_shop_chat = ChatOpenAI(
        model_name=&quot;qwen-turbo&quot;,  #用千问模型
        api_key=“API_KEY”, #填写你自己的DASHSCOPE_API_KEY
        base_url=&quot;https:&#47;&#47;dashscope.aliyuncs.com&#47;compatible-mode&#47;v1&quot;, 
        max_tokens=100,
        streaming=True,
        callbacks=[MyFlowerShopSyncHandler(), MyFlowerShopAsyncHandler()],
    )</div>2024-08-31</li><br/><li><img src="" width="30px"><span>张帅</span> 👍（0） 💬（0）<div>*[llm.agenerate([&quot;我姐姐喜欢什么颜色的花？&quot;]) for _ in range(3)]这一行运行不了了，会抛出错误【ValueError: Got unsupported message type: 我】，改成
*[llm.agenerate([[HumanMessage(&quot;我姐姐喜欢什么颜色的花？&quot;)]]) for _ in range(3)]
可以成功运行</div>2024-12-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/0a/a4/828a431f.jpg" width="30px"><span>张申傲</span> 👍（0） 💬（0）<div>第17讲打卡~
可以通过回调机制，将LLM运行过程中产生的日志异步写入文件或日志服务，后续通过ELK等机制进行日志采集和链路追踪</div>2024-07-18</li><br/>
</ul>