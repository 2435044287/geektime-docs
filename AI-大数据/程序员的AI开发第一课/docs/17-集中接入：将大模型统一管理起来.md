你好，我是郑晔！

前面两讲，我们介绍了两种工程实践，分别是记忆和缓存，它们可以从效果上更好地帮助我们实现一个大模型应用。这一讲，我们再来讨论一种工程实践：模型的集中接入。如果说前面两种工程实践，更多的是代码上的调整，那模型的集中接入更像是一种架构上的调整。

## 为什么要集中接入？

集中接入，就是把大模型的接入统一到一个地方管理起来，下面这张图可以很好地帮我们理解集中接入：

![](https://static001.geekbang.org/resource/image/1d/93/1db8f3fb18d756b2f4e1664e7a2acb93.jpg?wh=3000x1065)

从这个图上，你已经看出来了，所谓的集中接入，其实就是构建了一个代理，我们后面就称它为大模型代理。

到这里，你可能产生这样的疑问：我直接用大模型不好吗？为什么还要在中间加上一层代理呢？

我在前面说过，集中接入是一种架构上的调整，顾名思义，我需要是一个服务，才会有架构调整的说法。如果只是像前面几讲，如果在本地就可以运行起来的一些程序，确实没有必要在中间加入一层。但在真实的项目中，我们往往是要构建一个服务，这时集中接入的价值就体现出来了。

之所以要有一个中间层，最直接的一个问题就是限流问题。大模型服务本身资源消耗很大，提供大模型服务的供应商为了保证尽可能多的用户享受到正常的服务，所以，它对单用户实施了限流。以 OpenAI API 为例，下面就是它的限流标准，其中 RPM 是 requests per minute（每分钟请求数），TPM 是 tokens per minute（每分钟 Token 数）。
<div><strong>精选留言（4）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/13/6a/22/527904b2.jpg" width="30px"><span>hao-kuai</span> 👍（1） 💬（1）<div>看到了架构迭代演进，通俗易懂</div>2024-12-10</li><br/><li><img src="" width="30px"><span>Geek_7cd5a5</span> 👍（0） 💬（1）<div>如何看待最近由docker官方联合ollama，langchain新推出的genai stack这个框架
https:&#47;&#47;github.com&#47;docker&#47;genai-stack</div>2024-12-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/59/26/1015d573.jpg" width="30px"><span>gevin</span> 👍（0） 💬（1）<div>感谢郑老师提供了一个好的 LLM API 代理，我已经有了公司内部使改造和使用的冲动</div>2024-12-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/0a/a4/828a431f.jpg" width="30px"><span>张申傲</span> 👍（2） 💬（0）<div>第17讲打卡~
国内的硅基流动也很好用：https:&#47;&#47;siliconflow.cn&#47;zh-cn&#47;models</div>2025-02-13</li><br/>
</ul>