你好，我是吴磊。

在上一讲，我们重点介绍了机器学习中的特征工程，以及Spark MLlib框架支持的特征处理函数。基于线性回归模型，我们对比了不同特征处理方法下的模型效果。一般来说，线性模型的模型容量比较有限，它仅适合拟合特征向量与预测标的之间存在线性关系的场景。

但在实际应用中，线性关系少之又少，就拿“房价预测”的项目来说，不同的房屋属性与房价之间，显然不是单纯的线性关系。这也是为什么在房价预测的任务上，线性回归模型的预测误差一直高居不下。因此，为了提升房价预测的准确度，我们有必要从模型选型的角度，着手去考虑采用其他类型的模型算法，尤其是非线性模型。

Spark MLlib框架支持种类丰富的模型算法，为了在减轻你学习负担的同时，尽量全面地覆盖其中的内容，我把模型训练分为了上、中、下三讲。今天这一讲，我们专注在决策树系列算法的讲解。

后面两讲我再结合房屋预测和电影推荐场景，带你在实践中掌握Spark MLlib模型算法，从而让你在不同的场景下得心应手地开展模型选型与模型调优。

## 课程安排

因为模型训练的部分内容比较丰富，为了让你有一个清晰的学习计划，咱们还是先来交代一下课程安排。在机器学习领域，如果按照“样本是否存在预测标的（Label）”为标准，机器学习问题可以分为监督学习（Supervised Learning）与非监督学习（Unsupervised Learning）。Spark MLlib同时支持这两大类机器学习算法，如下图所示。
<div><strong>精选留言（3）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/27/bd/95/882bd4e0.jpg" width="30px"><span>Abigail</span> 👍（12） 💬（1）<div>机器学习理论中的集成学习 Ensemble Learning 最主要的学习策略为 Bagging 和 Boosting，都是从一个较弱的Learner 模型出发（例如DecisionTree)逐渐增加 Learner 的数量来的提高整个模型集群的Performance。 
随机森林RF = Bagging(DT), 每棵树可以从数据或特征的一个子集上建模，来增加各个树之间的差异度，减少冗余，建模过程可以并行计算。
梯度提升决策树GBDT =  Boosting(DT), 每课新树的目标就是目前整个树群的残差，属于增量建模过程，无法并行，相比于RF更容易过拟合数据，或者说学习能力更强。
GBDT 比RF 系统的鲁棒性要差，任何一个Learner出现问题都可以导致结果很大的偏差，但是其增量建模方法对于控制模型体积，以及后续的分发，更新和维护非常有帮助，这也是为啥当年XGBoost 可以在Kaggle屠榜，并且在工业界打出一片天地的理由之一。当然XGBoost 内部对并行计算进行了优化，但是其整体还是个GBDT。
RF非常依赖单棵DT树的学习能力，所以相比GBDT一般都会需要比较复杂的树结构或比较复杂的树群，而且后面还要考虑是否对森林进行裁剪来优化集群的体积，这也是它的缺点之一。
总结，模型的比较必须基于客观实际数据和应用场景指标，精度，速度，计算资源一般只能三选二。分享一下我个人这些年来做实际项目时的原则是：Simplicity，Robustness，Fault Tolerance 和 Predictable Timing，这点和做学术研究中各种Fancy的DL神经网络的很不一样，LOL。Amazon awslabs 的 autogluon 工具就是一个非常优秀的单机实验工具来帮你在项目前期快速做模型选择和分析。PS：我可以很“不负责任”的说GBDT类方法可以解决90%～95%以上的表格类数据建模问题，祝大家学习进步！</div>2021-11-14</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q3auHgzwzM4gIlRyVTOlTP8p1ucUN7Ahf2XMAicFpOHfk2UcrxEFm8GKIyCKGxd0PgBU0tMKwfPia8Ulk6rYBHVw/132" width="30px"><span>Geek_d4ccac</span> 👍（3） 💬（1）<div>RF和GBDT都是Ensemble 相比单个Decision Tree更稳定&#47;Robust，模型表现更优。RF并没有在降低残差上做过多的努力但因为每个树都是独立的应该可以更好的并行运算。GBDT的拟合过程在不断的降低残差但会存在过拟合泛化效果差的问题，且因为树与树之间有前后关系可能只能在单个树上并行运算而不能像RF一样同时算森林里的所有树。不知道理解的对不对，请老师指点！</div>2021-11-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/25/42/69/8c56cea0.jpg" width="30px"><span>inkachenko</span> 👍（0） 💬（1）<div>GBDT是先拟合前一棵树的gradient获得树结构，再优化叶子节点的数值吧</div>2022-01-11</li><br/>
</ul>