你好，我是吴磊。

在上一讲，我们通过“包工头与施工工人”的例子，初步认识了Spark进程模型中的Driver和Executors、以及它们之间的交互关系。Driver负责解析用户代码、构建计算流图，然后将计算流图转化为分布式任务，并把任务分发给集群中的Executors交付运行。

不过，你可能会好奇：“对于给定的用户代码和相应的计算流图，Driver是怎么把计算图拆解为分布式任务，又是按照什么规则分发给Executors的呢？还有，Executors具体又是如何执行分布式任务的呢？”

我们之前一再强调，**分布式计算的精髓，在于如何把抽象的计算图，转化为实实在在的分布式计算任务，然后以并行计算的方式交付执行**。深入理解分布式计算，是我们做好大数据开发的关键和前提，它能有效避免我们掉入“单机思维”的陷阱，同时也能为性能导向的开发奠定坚实基础。

而上面的这一系列问题，恰恰是我们吃透分布式计算的关键所在。因此，今天这一讲，我们就顺着这些问题，一起去深入探究Spark调度系统，进而弄清楚分布式计算的来龙去脉。

## 角色划分与斯巴克建筑集团

在上一讲，我们通过“包工头与施工工人”的类比、以及Word Count的示例，其实已经大致厘清了Spark分布式任务调度的核心环节与关键步骤。今天这一讲的核心任务，就是带你去深入其中的每一个环节，做到“既见森林、也见树木”。这里咱们不妨先把这些环节和涉及的组件梳理出来，从而让你在整体上有一个清晰的把握。
<div><strong>精选留言（29）</strong></div><ul>
<li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83epGTSTvn7r4ibk1PuaUrSvvLdviaLcne50jbvvfiaxKkM5SLibeP6jibA2bCCQBqETibvIvcsOhAZlwS8kQ/132" width="30px"><span>Geek_2dfa9a</span> 👍（42） 💬（4）<div>回答这个流程比较长哈，没有点开源源码阅读经验还真不好答哈，我这里都是自己的理解，如有异议欢迎讨论。
以老师的WordCount为例，首先stage分为两种：ResultStage，ShuffleMapStage。
ResultStage是啥呢就是处理Action动作的，一般也就是最后一个Stage（当然一个driver里也可能有多个Action，所以ResultStage也可能有多个，
这里简单点，WordCount例子里就是Stage1）。
ShuffleMapStage是啥呢，就是产生一个shuffle文件的stage，对应WordCount的Stage0。
为啥要分这么两类呢，因为你DAGScheduler就是根据是否shuffle倒推出来的stage嘛。这里多提一句，多个Job会共享Stage，这样就可以避免重复计算提升效率。
再根据老师讲的每个Stage里会根据RDD的partitions创建Task这段结合源码发现Task也有两种：ResultTask和ShuffleMapTask，对应的是Stage的两种类型。

接下来分析下DAGScheduler怎么感知Task的执行状态，DAGScheduler内部有一个线程eventProcessLoop，线程使用了生产者消费者模式，
里面有一个LinkedBlockingDeque队列，生产者TaskSetManager（可以忽略，理解为一个线程即可）发送给DAGScheduler的各种Task的event，
消费者eventProcessLoop把event取出来然后委托给DAGScheduler处理，其中对应Task完成的逻辑在方法handleTaskCompletion(event: 
CompletionEvent)中。既然问的是怎么知道当前Stage已经运行完成，可以运行下一个Stage，那当前Stage肯定不是ResultStage，
因此在handleTaskCompletion找到处理ShuffleMapTask的event的逻辑，具体逻辑为：先找到executor的ID标志execId，
校验execId是否为下发的Executor（以防伪造的event）然后标记ShuffleMapTask的结果可用，然后检查当前ShuffleMapStage是否还有没处理的Task，
如果没有的话说明该Stage完成，最后submitWaitingChildStages提交等待中的后续Stage。

这里使用的eventProcessLoop生产者消费者模式比较巧妙，生产者可能有多个线程（没进一步确认，有可能是1&#47;n个），但是消费者是单线程的，
生产者也不直接修改DAGScheduler内部的成员，只通过丢event给线程安全的LinkedBlockingDeque，这样就保证了没有数据竞争。

最后，老师讲的非常清楚，看了这门课后我买了老师的另一门Spark性能调优，在这里感谢老师。最后提一句，您配图里的字能调大点嘛，字太小了。</div>2021-09-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/65/4c/f7f86496.jpg" width="30px"><span>welldo</span> 👍（5） 💬（2）<div>老师,
根据“当 TaskScheduler 需要调度 Task0 这个任务时，根据 Task0 的 locs 属性，它就知道。。。”这段话，
和你回复unknown同学的“仅仅知道数据在某个机架内（一个机架包含多台机器）”这段话，
结合起来，意思是不是：

task对于它要处理的数据在哪里，
有时候精确知道（在某个进程内或某个节点内）；
有时候模糊知道（在某个机架内）；有时候不知道（any），
并且标注在它的locs属性里，
“精确知道的”task，就只挑选符合要求的offer；
“模糊知道的”task，就挑选大致符合要求的offer；
“不知道的”task，就随便选一个offer。

老师，请问我的理解对吗？









</div>2021-10-27</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/qicejJNHoVjX73w3hkw9RDGwgSLchAU8X6ibbdO8PhBJXGPuB2icbm3f4b74v7DWdTQic4gX6WSQS3M5THMhRoGlzA/132" width="30px"><span>花生耿</span> 👍（4） 💬（3）<div>老师，我是spark零基础。这个例子中的一堆中文名字反而增加了理解的难度，我建议直接把里面的角色直接替换成真实的组件更容易理解，要不然还得一边看中文名字，一边跟那个组件对应。</div>2022-02-28</li><br/><li><img src="https://wx.qlogo.cn/mmopen/vi_32/wgMMrp1hvSB3E30KqZvMsj3KQdAI3T1uQM77LT7hZ65nVSjPGRg3AbUOyiahnssA6AIT5PAkyHFmlTBzUH9gdyQ/132" width="30px"><span>pythonbug</span> 👍（4） 💬（1）<div>老师好，TaskScheduler调度优先级那里是不是可以这样理解：
for 优先级 &lt;- PROCESS_LOCAL to ANY {
  for task &lt;- task0 to taskn{
    if task0.locs == workoffer 分配 else 跳过
  }
}</div>2021-11-13</li><br/><li><img src="" width="30px"><span>bian</span> 👍（4） 💬（1）<div>这章真的是将执行流程讲的棒极了</div>2021-10-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/8b/61/df00ebc5.jpg" width="30px"><span>Jordan·李威</span> 👍（4） 💬（1）<div>总公司和分公司的工作任务和人力资源分配调度的例子太匹配了。</div>2021-09-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/3c/c4/4ee2968a.jpg" width="30px"><span>艾利特-G</span> 👍（3） 💬（1）<div>&gt; 2.SchedulerBackend 通过与 Executors 中的 ExecutorBackend 的交互来实时地获取集群中可用的计算资源，并将这些信息记录到 ExecutorDataMap 数据结构。
这一步中，假如数据是存放在HDFS中，那么SchedulerBackend通过读取HDFS的元数据就知道了某个task所需要的数据分片在哪个DataNode(YARN&#47;Spark worker node)。
这个时候为了知道该DataNode的计算资源，不需要实际地启动一个executor，只需要向YARN NodeManager(或者别的资源调度框架中的worker daemon)询问一下就知道了吧？</div>2022-02-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/d4/a1/8bc8e7e1.jpg" width="30px"><span>赌神很低调</span> 👍（3） 💬（1）<div>老师好，文中说task任务分发通过数据本地性找到合适的executor，想了解executor进程是根据数据所在的节点创建的吗？否则不是很大几率都找不到合适的executor?</div>2022-01-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/04/4f/f5ef8a2a.jpg" width="30px"><span>D.C</span> 👍（3） 💬（1）<div>一个taskSet下的不同task，可以分配到同一个executors么？</div>2021-11-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1a/f4/3d/d4ddbe31.jpg" width="30px"><span>加乘</span> 👍（2） 💬（3）<div>老师好，有个基础的问题，没太明白，就是文中说到“当我们调用 textFile API 从 HDFS 文件系统中读取源文件时，Spark 会根据 HDFS NameNode 当中记录的元数据......”时，数据和Executors应该不在一个物理机器上吧，那么Executors执行的时候，是到数据节点的机器上读取数据再进行处理吗？
后面又提到“数据不动，代码动”，感觉应该是把代码发到数据节点，然后在数据节点上进行计算处理。然后再返回给Executors执行结果，这样理解对吗？</div>2021-09-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg" width="30px"><span>Unknown element</span> 👍（1） 💬（2）<div>机架就是物理上的“铁架”吗 那 rack_local 和 node_local 有什么区别呢</div>2021-09-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/ee/4e/70227e09.jpg" width="30px"><span>虹桥念苏</span> 👍（1） 💬（5）<div>虽说很形象，但有点啰嗦，学习还是喜欢干货满满，可能比较适合完全零基础的同学吧</div>2021-09-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/28/01/9a/d2831441.jpg" width="30px"><span>康</span> 👍（0） 💬（1）<div>老师讲的太好了</div>2022-01-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/81/63/2ceecb43.jpg" width="30px"><span>liugddx</span> 👍（0） 💬（2）<div>我想问下，DAGScheduler划分stage为啥是按照shuffle倒推呢？</div>2021-12-10</li><br/><li><img src="" width="30px"><span>哇塞</span> 👍（0） 💬（1）<div>牛逼 例子很形象啊！！！</div>2021-12-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/cd/04/e27b7803.jpg" width="30px"><span>小新</span> 👍（0） 💬（1）<div>精彩绝伦</div>2021-11-30</li><br/><li><img src="" width="30px"><span>Yadong</span> 👍（0） 💬（1）<div>课程质量高，评论区也是干货！</div>2021-11-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2a/3a/83/74e3fabd.jpg" width="30px"><span>火炎焱燚</span> 👍（0） 💬（1）<div>好文章，对于我这个新手非常合适。</div>2021-10-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg" width="30px"><span>Unknown element</span> 👍（0） 💬（1）<div>老师我想问下 节点 和 机架 的区别是什么？</div>2021-09-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/27/ff/e4/927547a9.jpg" width="30px"><span>无名无姓</span> 👍（0） 💬（1）<div>比较形象</div>2021-09-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/27/f1/e4fc57a3.jpg" width="30px"><span>无隅</span> 👍（2） 💬（0）<div>要是有无比喻的清水版就好了</div>2022-09-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/63/a2/abb7bfe3.jpg" width="30px"><span>星潼</span> 👍（2） 💬（2）<div>感谢老师的精彩讲解，收获满满。但对于SchedulerBackend管理Executors部分有些不理解，想请教一下老师。
凭我对Hadoop YARN的理解(假设spark运行在YARN管理的集群中)，RM是整个集群资源的管理者，spark只是提交给集群的一个JOB，Spark中的Driver应该可以对标到MR中的ApplicationMaster。 
那么这样想的话，在向集群提交spark作业之前，集群中是不存在这个Job的Driver和Executor的。在提交spark作业后，应该由Driver先向RM申请资源，RM向Driver返回分配资源的节点，Driver再与对应节点上的DM通信，DM再启动一个Executor进程。
但目前看文章的描述，觉得Driver和Executor是一直存在于集群中的，Driver是整个集群资源的管理者。我认为Driver只是一个作业的管理者，Driver和Executor是随着作业的调度被创建，作业执行完它们也就消亡了。
所以这里想请教一下老师，当我们提交一个spark作业后，是如何创建对应的Driver和所有的Executor的？创建的流程是什么样的？请您指教，期待回复^_^</div>2022-05-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2d/17/92/0af520ef.jpg" width="30px"><span>十月</span> 👍（1） 💬（0）<div>简直是醍醐罐体！斯巴克公司的例子我愿称之为神比喻</div>2022-03-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/26/eb/24e0ac9c.jpg" width="30px"><span>嬴梦川</span> 👍（0） 💬（0）<div>在执行Stages的过程中，需要知道Stage的依赖是否有被执行，那么Stage的依赖关系怎么确定的呢？是在创建Stage的时候通过RDD的依赖关系来确定的么？</div>2023-08-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/3f/2e/bdeb7a0b.jpg" width="30px"><span>岁月神偷</span> 👍（0） 💬（0）<div>老师您好，课程讲的很生动形象，非常容易理解。

我有一个小问题，可能缺乏深究，问题就是：为什么划分stage需要倒序来进行呢，似乎正序进行也可以完成stage划分的工作。</div>2023-08-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/29/c0/69/12966ad5.jpg" width="30px"><span>一只菜🐶</span> 👍（0） 💬（0）<div>吴老师，我想问下，您讲的这里：DAGScheduler 会根据数据分区的物理地址，来为 Task 设置 locs 属性 以及TaskScheduler调度Task时是根据Task的locs属性处理的，是不是可以理解为 DAGScheduler根据不同分区的数据创建了task任务，TaskScheduler会对Task任务分配不同的计算节点，因为任务有本地倾向性的原则，所以会优先考虑分配的task计算节点与数据是否在同一位置，如果在那么locs属性里面就填NODE_LOCAL，否则就是文中提到那几种模式，当属性设置完了后，SchedulerBackend 发送workoffer被TaskScheduler收到后会依据locs属性里的规则分配对应的资源去计算Task任务？</div>2023-05-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2a/df/24/04ce86da.jpg" width="30px"><span>刘羽儿</span> 👍（0） 💬（1）<div>请问老师，spark底层是怎么判断某个算子是需要进行shuffle的呢？</div>2022-05-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1a/ae/f0/56ddebdd.jpg" width="30px"><span>有点忙</span> 👍（0） 💬（0）<div>我要多看几遍，然后给面试官讲一遍，之前对这块只有大概了解，看完这个，我觉得可以讲十多分钟了，后边应该还有惊喜</div>2022-05-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/05/92/b609f7e3.jpg" width="30px"><span>骨汤鸡蛋面</span> 👍（0） 💬（1）<div>请问下老师，taskBinary 具体是什么形式呢？和haddop 的MapReduce 分发一样嘛？</div>2022-03-17</li><br/>
</ul>