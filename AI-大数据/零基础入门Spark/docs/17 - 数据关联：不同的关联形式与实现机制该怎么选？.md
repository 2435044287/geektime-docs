你好，我是吴磊。

在上一讲，我们学习了Spark SQL支持的诸多算子。其中数据关联（Join）是数据分析场景中最常见、最重要的操作。毫不夸张地说，几乎在所有的数据应用中，你都能看到数据关联的“身影”。因此，今天这一讲，咱们继续详细说一说Spark SQL对于Join的支持。

众所周知，Join的种类非常丰富。如果按照**关联形式（Join Types）**来划分，数据关联分为内关联、外关联、左关联、右关联，等等。对于参与关联计算的两张表，关联形式决定了结果集的数据来源。因此，在开发过程中选择哪种关联形式，是由我们的业务逻辑决定的。

而从**实现机制**的角度，Join又可以分为NLJ（Nested Loop Join）、SMJ（Sort Merge Join）和HJ（Hash Join）。也就是说，同样是内关联，我们既可以采用NLJ来实现，也可以采用SMJ或是HJ来实现。区别在于，在不同的计算场景下，这些不同的实现机制在执行效率上有着天壤之别。因此，了解并熟悉这些机制，对咱们开发者来说至关重要。

今天，我们就分别从这两个角度，来说一说Spark SQL当中数据关联的来龙去脉。

## **数据准备**

为了让你更好地掌握新知识，我会通过一个个例子，为你说明Spark SQL数据关联的具体用法。在去介绍数据关联之前，咱们先把示例中会用到的数据准备好。
<div><strong>精选留言（11）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/28/3f/9d/c59c12ad.jpg" width="30px"><span>实数</span> 👍（18） 💬（1）<div>对于被连接的数据子集较小的情况，Nested嵌套循环连接是个较好的选择

Hash Join散列连接是CBO 做大数据集连接时的常用方式

SortMergeJoin    通常情况下散列连接的效果都比排序合并连接要好，然而如果行源已经被排过序，在执行排序合并连接时不需要再排序了，这时排序合并连接的性能会优于散列连接。可以使用来强制使用排序合并连接</div>2021-12-11</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/bmgpp5wc8GLmOdHNQccSgrunK0VdIicB6rpTHXCTF5xEkm2YvPHOX2DwNt2EqTzJ70JD41h0u5qW4R0yXRY1ZCg/132" width="30px"><span>Eazow</span> 👍（3） 💬（2）<div>老师，请问join是是分布式处理吗？</div>2021-12-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/4c/6d/c20f2d5a.jpg" width="30px"><span>LJK</span> 👍（2） 💬（4）<div>文中的Sort Merge Join的算法是否只适用于join key在基表中不重复的情况？如果join key在基表中可以重复的话应该需要设定好Mark控制基表的游标位置，不然文中的方法貌似会遗漏数据</div>2021-11-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/25/78/a0/7a248ddc.jpg" width="30px"><span>福</span> 👍（1） 💬（1）<div>问下老师，hive里面的join方式，也是老师说的这3种嘛？不考虑 shuffle join，Broadcast join的情况下，，，我的意思是，这3中join方式，是通用的嘛，比如mysql,oracle,hive ，spark 实现join都是这3种方式</div>2021-12-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/27/9a/e1/0867c16b.jpg" width="30px"><span>HHB</span> 👍（1） 💬（1）<div>无缝对接DBA</div>2021-11-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2a/3a/83/74e3fabd.jpg" width="30px"><span>火炎焱燚</span> 👍（1） 💬（1）<div>Python 版代码：

# 在notebook上运行，先构建环境
from pyspark import SparkContext, SparkConf
from pyspark.sql.session import SparkSession
sc = SparkContext()
spark = SparkSession(sc)

seq=[(1, &quot;Mike&quot;, 28, &quot;Male&quot;), (2, &quot;Lily&quot;, 30, &quot;Female&quot;), (3, &quot;Raymond&quot;, 26, &quot;Male&quot;), (5, &quot;Dave&quot;, 36, &quot;Male&quot;)]
employees=spark.createDataFrame(seq,[&#39;id&#39;,&#39;name&#39;,&#39;age&#39;,&#39;gender&#39;])

seq2=[(1, 26000), (2, 30000), (4, 25000), (3, 20000)]
salaries=spark.createDataFrame(seq2,[&#39;id&#39;,&#39;salary&#39;])

# inner join
jointDF=salaries.join(employees,&#39;id&#39;,&#39;inner&#39;)
jointDF.show()
 
# left join
jointDF2=salaries.join(employees,&#39;id&#39;,&#39;left&#39;)
jointDF2.show()

# right join
jointDF3=salaries.join(employees,&#39;id&#39;,&#39;right&#39;)
jointDF3.show()

# outer join
jointDF4=salaries.join(employees,&#39;id&#39;,&#39;outer&#39;)
jointDF4.show()

# leftsemi
jointDF5=salaries.join(employees,&#39;id&#39;,&#39;leftsemi&#39;)
jointDF5.show()

# leftanti
jointDF6=salaries.join(employees,&#39;id&#39;,&#39;leftanti&#39;)
jointDF6.show()</div>2021-10-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/4c/6d/c20f2d5a.jpg" width="30px"><span>LJK</span> 👍（0） 💬（3）<div>老师好，NLJ里左表驱动表应该是体量大的表吗？如果有大小两张表，大表是N条数据，小表是M条数据，不考虑block nested loop join优化的话。复杂度是：大表驱动下：O(M*N) + O(M)，小表驱动下O(M*N) + O(N)，好像应该选择小表驱动？</div>2021-12-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/65/4c/f7f86496.jpg" width="30px"><span>welldo</span> 👍（0） 💬（1）<div>先mark，最近要忙项目，忙完再来追。</div>2021-11-15</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/RA439icTng6iaFUBUbXD7FHsiasroCJflM7nG610e90k2zxWicERXTyQatTl4liakjIpO0nzBpbXSf9VChiaL3n2lYJA/132" width="30px"><span>李刘明</span> 👍（1） 💬（0）<div>SMJ排序不算复杂度？</div>2023-06-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/af/96/4b216878.jpg" width="30px"><span>Daniel</span> 👍（0） 💬（0）<div>请问join的实现方式如何指定呢？</div>2022-09-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1f/05/63/dd59ad18.jpg" width="30px"><span>加油加油</span> 👍（0） 💬（0）<div>HJ 为什么不直接用id值作为key呢？还需要两边同时计算一次哈希值</div>2022-08-25</li><br/>
</ul>