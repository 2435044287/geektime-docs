你好，我是吴磊。

今天是国庆第一天，首先祝你节日快乐。专栏上线以来，有不少同学留言说期待后续内容，所以国庆期间我们仍旧更新正文内容，让我们一起把基础知识模块收个尾。

学习过RDD常用算子之后，回顾这些算子，你会发现它们都是作用（Apply）在RDD之上的。RDD的计算以数据分区为粒度，依照算子的逻辑，Executors以相互独立的方式，完成不同数据分区的计算与转换。

不难发现，对于Executors来说，分区中的数据都是局部数据。换句话说，在同一时刻，隶属于某个Executor的数据分区，对于其他Executors来说是不可见的。

不过，在做应用开发的时候，总会有一些计算逻辑需要访问“全局变量”，比如说全局计数器，而这些全局变量在任意时刻对所有的Executors都是可见的、共享的。那么问题来了，像这样的全局变量，或者说共享变量，Spark又是如何支持的呢？

今天这一讲，我就来和你聊聊Spark共享变量。按照创建与使用方式的不同，Spark提供了两类共享变量，分别是广播变量（Broadcast variables）和累加器（Accumulators）。接下来，我们就正式进入今天的学习，去深入了解这两种共享变量的用法、以及它们各自的适用场景。
<div><strong>精选留言（11）</strong></div><ul>
<li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83epGTSTvn7r4ibk1PuaUrSvvLdviaLcne50jbvvfiaxKkM5SLibeP6jibA2bCCQBqETibvIvcsOhAZlwS8kQ/132" width="30px"><span>Geek_2dfa9a</span> 👍（25） 💬（1）<div>1.使用普通变量是无法得到预期结果的，因为lambda里的必须是final型的变量，那这里我用两种方法做个测试：
首先是原子类，因字数限制，只放关键代码：
    var normal = new AtomicInteger(0)
    val  preRdd = wordRdd.filter(f =&gt;
      if(f.contains(&quot;scala&quot;)) {
        println(normal.incrementAndGet())
        false
      } else {
        true
      }
    )
    println(normal.get())
可以看到Executor运行过程中normal是会正常累加的，但是最后println(normal.get())打印出来是0，这里简单分析下，spark会把闭包序列化后
传递给Executor，然后Executor再把闭包反序列化后作用在RDD上。因此Driver里的normal变量和Executor里的normal变量是多个进程里的多个变量。
然后使用自定义类对象
class IntHolder(var value: Int) {}
测试，报了一个Exception in thread &quot;main&quot; org.apache.spark.SparkException: Task not serializable
从侧面也证明，闭包里的对象是要实现序列化的。变量是多个进程里的多个变量改进下再试，
class IntHolder(var value: Int) extends Serializable {}
发现每个Task都是从0开始计数，更说明每个Task里的对象是Driver丢过来的副本。这里我多想了下Task里的函数是串行执行还是并行执行的，如果我的IntHolder对象
不是线程安全的，那在Task里有无数据竞争？从我的例子看是串行运行的，但是一时找不到看哪些代码，请老师指正。
多讲两句，Spark闭包序列化和反序列化真的是很重要的知识，打开了我的视野，我感觉Spark是Faas一种非常巧妙的场景，函数式编程真的也很符合Spark
把函数作用在RDD上，存算分离的思路，UCB出品真的名不虚传。

2.Spark基本支持任何类型的广播变量，但是不支持RDD类型的广播变量，从代码中可以看到，有一个验证会判断变量是否为RDD类型，如果想要广播RDD类型的话，可以先
collect收集到driver，再作为普通集合广播。
  def broadcast[T: ClassTag](value: T): Broadcast[T] = {
    assertNotStopped()
    require(!classOf[RDD[_]].isAssignableFrom(classTag[T].runtimeClass),
      &quot;Can not directly broadcast RDDs; instead, call collect() and broadcast the result.&quot;)
    val bc = env.broadcastManager.newBroadcast[T](value, isLocal)
    val callSite = getCallSite
    logInfo(&quot;Created broadcast &quot; + bc.id + &quot; from &quot; + callSite.shortForm)
    cleaner.foreach(_.registerBroadcastForCleanup(bc))
    bc
  }</div>2021-10-03</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q3auHgzwzM6iagw7ct4ca3niaSEFNicu2wy2KuCibO6eiaRzoRGJb50WTrbkKQib9mTArnTr8jJUazO9O2ibLZNfjjl35cfCHkBPs7N/132" width="30px"><span>Geek_f39659</span> 👍（12） 💬（1）<div>第一题： driver中声明的非广播变量，如果executor需要用的话，会给每一个分区拷贝一个副本，所以每个分区都会给自己的这份副本加一加一。最后这些副本随着executor 进程的结束就都丢失了。所以driver 中的这个变量仍然还是0.</div>2021-10-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2a/3a/83/74e3fabd.jpg" width="30px"><span>火炎焱燚</span> 👍（3） 💬（1）<div>python 版的代码对累加器这儿有些不一样，貌似没有longAccumulator，double之分，我这儿是这样的：

file=&#39;~~~~&#47;chapter01&#47;wikiOfSpark.txt&#39;
lineRDD=sc.textFile(file)
lineRDD.first() # 会打印出lineRDD的第一行： u&#39;Apache Spark&#39;，如果没有打印出来，则报错
wordRDD=lineRDD.flatMap(lambda line: line.split(&quot; &quot;))

# 定义累加器
ac = sc.accumulator(0) # 这儿的定义方式不一样
# 定义filter算子的判定函数f，注意，f的返回类型必须是Boolean
def f(x):
    if x==&#39;&#39;:
        ac.add(1)
        return False
    else:
        return True
    
# 使用f对RDD进行过滤
cleanWordRDD = wordRDD.filter(f)
kvRDD=cleanWordRDD.map(lambda word:(word,1))
wordCounts=kvRDD.reduceByKey(lambda x,y:x+y)
# 获取计算结果
wordCounts.collect()
# 结果：[(&#39;Spark&#39;, 63), (&#39;Apache&#39;, 34)]
# wordCounts.map(lambda (k,v):(v,k)).sortByKey(False).take(5)

# 作业执行完毕，通过调用value获取累加器结果
ac.value
# 79</div>2021-10-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/65/4c/f7f86496.jpg" width="30px"><span>welldo</span> 👍（2） 💬（1）<div>以前我写累加器，都是手动继承AccumulatorV2，现在找到更简单的方法了😁</div>2021-10-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/23/bb/a1a61f7c.jpg" width="30px"><span>GAC·DU</span> 👍（2） 💬（4）<div>写程序测试了一下老师的第一个思考题，在本机的idea中可以得到正确的结果79，但是在spark- shell或者提交到yarn集群上结果却是0，好纠结，请老师解惑，代码如下。
第二个思考题，我认为广播变量是只读数据，参加计算时不能随着RDD一起变换形态破坏数据的一致性。

import org.apache.spark.sql.SparkSession

object BcAcOpt {

  var n: Long = _ &#47;&#47; 全局变量

  def main(args: Array[String]): Unit = {

    val spark = SparkSession.builder.master(&quot;local[*]&quot;).appName(&quot;board&quot;).getOrCreate()

    val lineRDD = spark.sparkContext.textFile(&quot;wikiOfSpark.txt&quot;)

    println(lineRDD.partitions.length)

    val wordRDD = lineRDD.flatMap(word =&gt; word.split(&quot; &quot;))

    val filterRDD = wordRDD.filter(acf)

    val kvRDD = filterRDD.map(word =&gt; (word, 1))

    val wordCount = kvRDD.reduceByKey((x, y) =&gt; x + y)

    wordCount.take(10)

    println(n)

    spark.stop()

  }

  def acf(word: String): Boolean = {
    if (&quot;&quot;.equals(word)) {
      n += 1
      false
    } else {
      true
    }
  }
}
</div>2021-10-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/65/4c/f7f86496.jpg" width="30px"><span>welldo</span> 👍（1） 💬（1）<div>老师，广播变量有&quot;只读&quot; &#47; &quot;不可变&quot;特性，
但广播引用数据类型时, 广播的实际上是地址值，那么地址值肯定不可变，而地址值指向的内容是可变的;

我今天在idea和shell里做了一个测试，证明了这个说法是对的,
代码如下:
(scala-List是不可变的，所以代码里我用了可变的scala-ListBuffer)

var buffer: ListBuffer[String] = ListBuffer()
var bufferBroad = spark.sparkContext.broadcast(buffer)
val cleanWordRDD2: RDD[String] = wordRDD.filter(word =&gt; {
  if (bc.value.contains(word)) {		&#47;&#47;bc是文章里的广播变量
	bufferBroad.value.append(&quot;test&quot;)	&#47;&#47;关键的一行
	true
  } else {
	false
  }
})

println(cleanWordRDD2.count())&#47;&#47;触发这条dag
println(buffer)				&#47;&#47;97(34+63)个test
println(bufferBroad.value)	&#47;&#47;97(34+63)个test

老师，这个说法和我的证明没有问题吧？
这个算不算“不可变”特性的漏洞 呢？</div>2021-10-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/65/4c/f7f86496.jpg" width="30px"><span>welldo</span> 👍（1） 💬（1）<div>老师，关于第一题，
我的答案：driver和executor是不一样的进程，普通变量会拷贝副本到executor上，
“原本”和“副本”没有任何关系，
spark-shell打印出来的是“原本”，数值是初始化时的值.

我的问题：为何idea能计算正确呢？</div>2021-10-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/f5/4e/c890155f.jpg" width="30px"><span>Sun</span> 👍（1） 💬（1）<div>老师您好。关于第一题的实验，我使用自定义类，如下：
class MyCount implements Serializable{
    int num;
    public MyCount(){
        this.num = 0;
    }
    public void add(){
        this.num++;
    }
}
将它用于代替广播变量，是不行的，因为executor最后销毁了这些对象。
但是我将这个对象置为静态对象，放在Driver中再运行后，这个对象成功得到了最后的结果。
请问老师，为什么设置为静态对象就可以获取结果呢？静态对象在Driver和Executor是怎么一个工作机制？</div>2022-04-20</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/ugib9sF9icd9dhibQoAA025hibbD5zgZTiaddLoeEH457hrkBBhtQK6qknTWt270rHCtBZqeqsbibtHghgjdkPx3DyIw/132" width="30px"><span>唐方刚</span> 👍（0） 💬（0）<div>广播变量是以executor为粒度分发的，那么累加器是怎么分发的？最终的结果又是怎么算出来的？</div>2022-08-12</li><br/><li><img src="" width="30px"><span>杨帅</span> 👍（0） 💬（0）<div>老师，我有一个问题：一个worker上可以有多个executor吗？在读取文件阶段，一个executor（一个进程）里可以有多个task（多个线程）读取不同分区（RDD的分区）的数据吗？那RDD的分区的定义是什么呢，是不同机器上的数据，还是？</div>2022-06-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/e7/8e/318cfde0.jpg" width="30px"><span>Spoon</span> 👍（0） 💬（0）<div>共享变量Java版本
https:&#47;&#47;github.com&#47;Spoon94&#47;spark-practice&#47;blob&#47;master&#47;src&#47;main&#47;java&#47;com&#47;spoon&#47;spark&#47;core&#47;SharedVariableJob.java</div>2022-04-04</li><br/>
</ul>