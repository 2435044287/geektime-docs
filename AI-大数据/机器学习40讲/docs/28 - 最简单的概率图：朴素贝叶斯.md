从今天起，我们将进入概率图模型的模块，以贝叶斯的角度重新审视机器学习。

在机器学习任务中，输入和输出之间并不是简单的一对一的决定关系，两者之间通常存在着一些可见或不可见的中间变量。要计算输出变量的概率分布，就得把这些中间变量纳入到建模的框架之中。要简洁明快地表达多个变量之间的复杂的相关关系，**图模型**无疑是理想的选择。将图模型和概率模型结合起来，就是这个模块的主题——**概率图模型**（probabilistic graphical model）。

在“人工智能基础课”中，我曾用简短的篇幅粗略地介绍过概率图模型的概念和分类。这次我们从实例出发，看一看最简单的概率图模型——**朴素贝叶斯分类器**（naive Bayes classifier），并以它作为从统计机器学习到概率图模型的过渡。

还记得朴素贝叶斯的原理吗？回忆一下，朴素贝叶斯利用后验概率最大化来判定数据所属的类别，其“朴素”之处在于条件独立性的引入。条件独立性假设保证了所有属性相互独立，互不影响，每个属性独立地对分类结果发生作用，这样类条件概率就变成了属性条件概率的乘积。这在概率图中体现为**条件独立关系（conditioanl independence）**：如果将朴素贝叶斯模型画成有向图的话，它就是下图中的贝叶斯网络，**类别信息指向所有的属性，任何两个属性之间都没有箭头的指向**。
<div><strong>精选留言（6）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（7） 💬（1）<div>王老师，我的理解和问题如下。

1. 先验概率即先验分布，是训练数据集上不同类别的数据所占的比例，是朴素贝叶斯因子分解公式中的p(Y)；

2. 似然概率是条件概率的乘积，是朴素贝叶斯因子分解公式中的∏i=1~Np(Xi|Y)；

3. 问题：狄利克雷分布(Dirichlet distribution)利用因子分解后的似然概率对因子分解后的先验概率进行加权。

	3.1 这里∏i=1～Kxi^(αi−1)这一部分中的xi相当于上面朴素贝叶斯因子分解公式中的p(Y)，即类别的先验概率？

	3.2 我的理解狄利克雷分布的计算结果Dir(α)是先验概率(类别分布)的共轭先验，这个结果可以放到朴素贝叶斯因子分解公式中替换掉p(Y)，然后与似然概率	(条件概率的乘积)那一部分相乘，相乘的结果为属性和类别的联合分布。这个理解对吗？

	3.3 狄利克雷分布中因子分解后的似然概率是指公式中的哪一部分？狄利克雷中的α(alpha)和每个类别中不同属性的条件概率有关吗？

	3.4 狄利克雷分布的参数被修正为 Ni+αi，这个参数是在计算狄利克雷分布或伽马函数时作为输入参数代入公式或程序模块用的？

谢谢。</div>2018-08-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（1） 💬（1）<div>树增强朴素贝叶斯不假设各个属性是相互独立的，允许部分属性依赖。它需要计算属性间的互信息值，互信息值越大，就代表2个属性关联性越大。和文中的朴素贝叶斯一样，属性之间的关联性的前提是要在某一分类属性确定下进行计算，不同的类属性值会有不同的属性关联性。

在计算完各个属性对的互信息值之后，进行贝叶斯网络的构建。

1. 根据各个属性对的互信息值降序排序，依次取出其中的节点对，遵循不产生环路的原则，构造最大权重跨度树，直到选择完n-1条边为止；

2. 上述过程构成的是一个无向图，接下来为整个无向图确定边的方向。选择任意一个属性节点作为根节点，由根节点向外的方向为属性节点之间的方向；

3. 为每一个属性节点添加父节点，父节点就是分类属性节点，至此贝叶斯网络结构构造完毕。

我对这个贝叶斯网络的理解是除了上面第2步提到的根结点只有分类属性一个夫节点外，其他的节点都有2个父节点，一个是分类属性，一个是第2步构造的有向图中的父节点。</div>2018-08-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（0） 💬（1）<div>手动计算了下联合概率，发现∀i,(Xi⊥X−i|Y)这个条件要求必须是在Y分类已经确定的情况下，N 个属性变量 Xi才是互相独立，联合分布的概率才等于各自概率的乘积。而∀i,(Xi⊥X−i)是不成立的，即Y分类不确定的情况下，Xi不是互相独立的，所有分类的Xi的概率的乘积并不等于Xi的联合分布的概率。

在文中的例子里x=(1,0,1,1,0)T发生的概率等于英格兰人出现x的概率与苏格兰人出现的x的概率之和，而不等于Xi各个属性在所有样本取值概率的乘积。</div>2018-08-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/0e/b4/6ecadbd3.jpg" width="30px"><span>心流</span> 👍（0） 💬（1）<div>王老师，机器学习的python的文件，如何打开，能给我们讲一下吗？</div>2018-08-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-06-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/0e/b4/6ecadbd3.jpg" width="30px"><span>心流</span> 👍（0） 💬（0）<div>王老师，机器学习的python文件，能讲一下如何打开吗？</div>2018-08-09</li><br/>
</ul>