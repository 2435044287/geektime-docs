截至目前，我所介绍的模型都属于监督学习范畴，它们处理具有标签的输入数据，给出意义明确的输出，回归模型输出的是连续的回归值，分类模型输出的是离散的类别标签，这些模型都属于**预测模型**（predictive model）。

另一类模型则隶属于无监督学习，这类模型学习没有标签的数据，其作用也不是计算类别或回归值，而是要揭示关于数据隐藏结构的一些规律，因此也被称为**描述模型**（descriptive model）。**聚类算法就是最具代表性的描述模型**。

聚类分析（cluster analysis）实际上是一种分组方式，它使每一组中的组内对象的相似度都高于组间对象的相似度，分出来的每个组都是一个簇（cluster）。由于相似度是聚类的依据，作为相似度主要度量方式之一的距离就在聚类中发挥着重要作用。

在“人工智能基础课”中，我曾介绍过四种主要的聚类算法，你可以结合下面的要点图回忆一下。除了以概率分布为基础的分布聚类以外，其他三类聚类算法都涉及对距离的使用，而其中最典型的就是$k$均值所代表的原型聚类算法。

![](https://static001.geekbang.org/resource/image/be/6f/be9208083ca3c520e1c530efd3b4dd6f.jpg?wh=1110%2A1122)

[《机器学习 | 物以类聚，人以群分：聚类分析》](https://time.geekbang.org/column/article/2196)

理解$k$均值算法的基础是理解它对距离的使用方式。前面介绍的$k$近邻算法其实也用到了距离，近邻的选择就是以距离为依据的。但近邻点是以内收的形式影响未知的数据，所有近邻点按照一定的规则共同决定处于中心的未知数据的类别。如果将这种影响的方式调转方向，让处于中心的样本作为原型（prototype），像一个小太阳一样用万有引力牵引着周围的其他样本，那么其他样本就会像卫星一样被吸附在原型周围，共同构成一个星系，也就是簇。
<div><strong>精选留言（2）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/10/3c/1f/3948a3c6.jpg" width="30px"><span>paradox</span> 👍（1） 💬（1）<div>老师，您好
我有两个关于马氏距离的问题：
1、Gxi 的维度会比 xi 的原始维度有所降低，故可以用作降维，这里不理解G的含义以及为什么会使维度有所降低
2、马氏距离的好处在于引入了可调节的参数，从而使距离可以通过对数据的学习来加以改善，是不是因为中间的协方差矩阵起了权重的作用，也就是后面所说的G起了权重作用，因此可以用作降维？
谢谢!</div>2018-08-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-06-05</li><br/>
</ul>