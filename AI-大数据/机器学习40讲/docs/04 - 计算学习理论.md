无论是频率学派的方法还是贝叶斯学派的方法，解决的都是怎么学的问题。但对一个给定的问题到底能够学到什么程度，还需要专门的**计算学习理论**（computational learning theory）来解释。与机器学习中的各类具体算法相比，这部分内容会略显抽象。

学习的目的不是验证已知，而是探索未知，人类和机器都是如此。**对于机器学习来说，如果不能通过算法获得存在于训练集之外的信息，学习任务在这样的问题上就是不可行的**。

下图就是来自于加州理工大学教授亚瑟·阿布-穆斯塔法（Yaser S. Abu-Mostafa）的课程Learning from Data中的一个例子：假设输入$\\mathbf{x}$是个包含三个特征的三维向量，输出$y$则是二元的分类结果，训练集中包含着五个训练数据，学习的任务是预测剩下的三个测试数据对应的分类结果。

![](https://static001.geekbang.org/resource/image/e6/f8/e67715f6dd3b9d6cb1f88a92aa363bf8.png?wh=707%2A309) 学习任务示意图（图片来自Yaser S. Abu-Mostafa, et. al., Learning from Data）

横线上方为训练数据，下方为待估计的分类结果，$f\_1$~$f\_8$代表所有可能的映射关系。

预测三个二分类的输出，总共就有$2 ^ 3 = 8$种可能的结果，如上图所示。可在这穷举出来的8个结果里，到底哪个是符合真实情况的呢？遗憾的是，单单根据这5个输入数据其实是没有办法确定最适合的输出结果的。输出结果为黑点可能对应所有只有1个特征为1的输入数据（此时三个测试数据的结果应该全是白点）；也可能对应所有奇偶校验和为奇数的输入数据（此时三个测试数据的结果应该是两白一黑）；或者还有其他的潜在规律。关于这个问题唯一确定的结果就是不确定性：不管生成机制到底如何，训练数据都没有给出足以决定最优假设的信息。

既然找不到对测试数据具有更好分类结果的假设，那机器学习还学个什么劲呢？别忘了，我们还有概率这个工具，可以对不同假设做出定量描述。**虽然不能对每个特定问题给出最优解，但概率理论可以用来指导通用学习问题的求解，从而给出一些基本原则**。
<div><strong>精选留言（19）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/11/6e/c6/4a7b2517.jpg" width="30px"><span>Will王志翔(大象)</span> 👍（68） 💬（2）<div>以问答的方式，做了文章的笔记。

❶ 给出问题，一问是否可解，二是如何解？频率学派和贝叶斯学派都在讲如何解，即回答问题二。那是否可解，但往往不是非此即彼，更多的问法，是在投入计算资源之前，先评估一下机器学习能够学到说明什么程度？(就如软件工程中可行性分析)
	
答：所以，有了“计算学习理论”来回答这个问题。
	
❷ 那时什么是“计算学习理论”呢？从一个例子入手，5个三维(0&#47;1)输出，预测3个数据，用机器学习训练，发现，“不管生成机制到底如何，训练数据都没有给出足以决定最优假设的信息。” 那机器学习在学习什么呢？
	
答：“概率”,不能给出最优解，可以给出近似解啊！
	
❸ 第二个例子：“红球与白球”，既然用“概率”工具，那么用 ν 来近似 μ 有多高的精确度呢？有没有理论支撑？
	
答：可以用“Hoeffding不等式”回答，“Hoeffding不等式”描述了训练误差和泛化误差之间的近似关系。
训练误差是模型在训练集上的误差，泛化误差是用来衡量模型的泛化性。
结论是总会存在一个足够大的样本容量 N 使两者近似相等，这时就可以根据模型的训练误差来推导其泛化误差，从而获得关于真实情况的一些信息。
	
❹ 现在有了“概率”工具，说明问题的可学习性，那需要多少训练数据才能达到给定的准确度参数和置信参数呢？
	
答：我们可以用“样本复杂度”（sample complexity）来表示。所有假设空间有限的问题都是“概率近似正确”PAC 可学习的，其样本复杂度有固定的下界，输出假设的泛化误差会随着样本数目的增加以一定速度收敛到0。
	
❺ 有限空间有了，那如何判断具有无限假设空间的问题是否是 PAC 可学习的呢？
	
答：用“VC维”：对无限假设空间复杂度的一种度量方式。任何 VC 维有限的假设空间都是 PAC 可学习的。由于 VC 维并不依赖于数据分布的先验信息，因此它得到的结果是个松散的误差界（error bound），这个误差界适用于任意分布的数据。
	
❻ “松散的误差界”？是不是可用性较差？不能加入数据分布的先验信息呢？
	
答：所以有了“Rademacher 复杂度”。在已知的数据分布下，Rademacher 复杂度既可以表示函数空间的复杂度，也可以用来计算泛化误差界
	
❼ 学习理论的研究对解决实际问题到底具有什么样的指导意义呢？
	
答：举一个例子：神经网络的VC维相对较高，因而它的表达能力非常强，可以用来处理任何复杂的分类问题。要充分训练该神经网络，所需样本量为10倍的VC维。如此大的训练数据量，是不可能达到的。所以在20世纪，复杂神经网络模型在out of sample的表现不是很好，容易overfit。但现在为什么深度学习的表现越来越好？
其中一条是：通过修改神经网络模型的结构，以及提出新的regularization方法，使得神经网络模型的VC维相对减小了。例如卷积神经网络，通过修改模型结构(局部感受野和权值共享)，减少了参数个数，降低了VC维。2012年的AlexNet，8层网络，参数个数只有60M；而2014年的GoogLeNet，22层网络，参数个数只有7M。再例如dropout，drop connect，denosing等regularization方法的提出，也一定程度上增加了神经网络的泛化能力。 —— 例子参考《VC维的来龙去脉》
	</div>2018-06-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/6e/1c/5a375b44.jpg" width="30px"><span>Spencer</span> 👍（10） 💬（1）<div>可以增加一些参考论文吗？</div>2018-06-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/01/25/240e01b4.jpg" width="30px"><span>安邦</span> 👍（7） 💬（1）<div>讲得非常好，但是模仿机器学习基石中的内容，会不会不太好</div>2018-06-14</li><br/><li><img src="" width="30px"><span>Geek_4ca45d</span> 👍（4） 💬（1）<div>老师，请问学这类课题是不是，主要掌握概率学就基本可以了？还有机器学习是不是人工智能？</div>2018-06-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/87/cc/8b94c0d3.jpg" width="30px"><span>Geek_e2d0qt</span> 👍（3） 💬（1）<div>周志华在书上说，计算学习理论是机器学习的理论基础，可以根据它来分析学习任务并指导算法设计。但是想请问老师，在具体问题上，是怎么使用它的呢?或者它还有其他什么作用吗?</div>2018-06-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/41/8d/f14a278d.jpg" width="30px"><span>风的轨迹</span> 👍（3） 💬（1）<div>王老师，我可以这么理解吗？
整篇文章都是围绕着“如何刻画训练误差和泛化误差之间的精度”来进行阐述的
1. 如果在某种条件（样本容量足够大的情况下）下，精度足够小，那么通过计算的方法来学到最优假设是可行的。
2. 引入两个维度来描述样本的复杂度，使我们了解到样本的复杂度其实是会影响到训练误差和泛化误差的精度的。
由此我们可以在设计机器学习任务的时候能够考虑到更多的因素从而使各方面达到一个比较好的均衡。</div>2018-06-12</li><br/><li><img src="https://wx.qlogo.cn/mmopen/vi_32/ajNVdqHZLLCcMYXXdD9iap4JzKJia4XmeeCnGqgpQOT6EVxKCZgDWFe31FHpicQ4dibQJibp1jCPR0Btlgr5RmnMS4g/132" width="30px"><span>Addison</span> 👍（2） 💬（1）<div>您好，我想问下：“在打散的基础上可以进一步定义 VC 维。假设空间的 VC 维是能被这个假设空间打散的最大集合的大小，它表示的是完全正确分类的最大能力。上面的例子告诉我们，对于具有两个自由度的线性模型来说”这句话的，具有两个自由度的线形模型中的两个自由度是不是理解为：y=kx+b中，k和b是两个可以自由定义的变量？</div>2018-06-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/6f/c7/7cf09346.jpg" width="30px"><span>知足</span> 👍（0） 💬（1）<div>太书面化了，就没有一种比较生动形象的描述么？</div>2018-06-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（0） 💬（1）<div>有了这些理论基础，明白哪些方式是降低训练误差，哪些方式是减少训练误差和泛化误差的差距的，结合模型的表现判断和选择合适的优化方向或方式，这是我的理解。</div>2018-06-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/89/f3/20c6e5e7.jpg" width="30px"><span>刘艾伦04166666</span> 👍（3） 💬（0）<div>一个具有无穷 VC 维的假设空间是 y=sin(kx)，原因: 傅里叶变换可以拟合任意函数 </div>2021-01-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-05-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/20/02/6f/7e125dd5.jpg" width="30px"><span>元气🍣 🇨🇳</span> 👍（0） 💬（0）<div>和哲学讨论的，世界可认知和不可认知一个道理</div>2021-12-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/27/1f/2d/10fd95d1.jpg" width="30px"><span>一</span> 👍（0） 💬（0）<div>一个疑问一直困扰我，混沌的模型可否用机器学习求解</div>2021-06-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/4a/64/454b9fae.jpg" width="30px"><span>tanC</span> 👍（0） 💬（0）<div>请教老师，&quot;模型复杂度和泛化性能之间的折中关系&quot;这部分的讲解，是否就是过拟合的原理？</div>2021-03-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/11/fa/e0dcc1bf.jpg" width="30px"><span>榕</span> 👍（0） 💬（0）<div>老师，是不是可以这样理解：一个VC维无限的假设空间是必然过拟合的，也就不符合PAC可学习的定义？</div>2021-01-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/89/f3/20c6e5e7.jpg" width="30px"><span>刘艾伦04166666</span> 👍（0） 💬（0）<div>王教授去讲脱口秀得了, 搞啥机器学习</div>2021-01-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg" width="30px"><span>建强</span> 👍（0） 💬（0）<div>个人理解：计算学习理论主要用于评估机器学习模型的性能优劣，分析模型的训练误差和泛化误差之间的关系，对于模型的选择和评估有着理论指导意义</div>2020-10-25</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（0） 💬（0）<div>极客时间
21天打卡行动 42&#47;21
&lt;&lt;机器学习40讲&#47;04&gt;&gt;学什么与怎么学
今日所学:
1,对于机器学习来说，如果不能通过算法获得存在于训练集之外的信息，学习任务在这样的问题上就是不可行的;
2,在概率论中，有界的独立随机变量的求和结果与求和数学期望的偏离程度存在一个固定的上界，这一关系可以用 Hoeffding 不等式（Hoeffding&#39;s Inequality）来表示;
3,器学习利用训练集来选择出的模型很可能（对应名称中的“概率”）具有较低的泛化误差（对应名称中的“近似正确”）;
4,PAC 可学习性;
5,样本复杂度（sample complexity）是保证一个概率近似正确解所需要的样本数量。
6,。VC 维（Vapnik-Chervonenkis dimension）的名称来源于统计学习理论的两位先驱名字的首字母，它是对无限假设空间复杂度的一种度量方式，也可以用于给出模型泛化误差在概率意义上的上界。
7,线性模型可以对任何 3 个不共线的点进行划分，也就是将这个数据集打散。
8,在维度有限的前提下，VC 维的大小也会影响模型的特性。较小的 VC 维虽然能够让训练误差和泛化误差更加接近，但这样的假设空间不具备较强的表达能力（想想上面线性模型的例子），训练误差本身难以降低。反过来，VC 维更大的假设空间表达能力更强，得到的训练误差也会更小，但训练误差下降所付出的代价是训练误差和泛化误差之间更可能出现较大的差异，训练集上较小的误差不能推广到未知数据上。这其实也体现了模型复杂度和泛化性能之间的折中关系。
9,要是将数据的分布特性纳入可学习性的框架，复杂性的指标就变成了 Rademacher 复杂度（Rademacher complexity）。
10,“没有经验的”Rademacher 复杂度，它表示了函数空间在给定的数据分布上拟合噪声的性能。
重点:
1,Hoeffding 不等式描述了训练误差和泛化误差之间的近似关系；
2,PAC 学习理论的核心在于学习出来的模型会以较大概率接近于最优模型；
3,假设空间的 VC 维是对无限假设空间复杂度的度量，体现了复杂性和性能的折中；
4,Rademacher 复杂度是结合了先验信息的对函数空间复杂度的度量。</div>2020-01-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/07/3f/53ae01f6.jpg" width="30px"><span>Kevin.zhang🌏</span> 👍（0） 💬（0）<div>作业：
关于学习理论的研究和解决实际的问题的意义：
个人意见：是指明一个方向，并在这个方向上不断精进和优化！</div>2018-12-20</li><br/>
</ul>