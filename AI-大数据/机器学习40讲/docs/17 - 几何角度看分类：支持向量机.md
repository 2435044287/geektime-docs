前文中介绍过的逻辑回归是基于似然度的分类方法，通过对数据概率建模来得到软输出。而在另一类基于判别式的硬输出分类方法中，代表性较强的就得数今天要介绍的支持向量机了。

支持向量机并不关心数据的概率，而是要基于判别式找到最优的超平面作为二分类问题的决策边界。其发明者弗拉基米尔·瓦普尼克（Vladimir Vapnik）用一句名言清晰地解释了他的思想：能走直线就别兜圈子。

当然啦，这是“信达雅”的译法，老瓦的原话是“不要引入更加复杂的问题作为解决当前问题的中间步骤（When trying to solve some problem, one should not solve a more difficult problem as an intermediate step. ）”。

在他看来，估算数据的概率分布就是那个作为中间步骤的复杂问题。这就像当一个人学习英语时，他只要直接报个班或者自己看书就行了，而不需要先学习诘屈聱牙的拉丁语作为基础。既然解决分类问题只需要一个简单的判别式，那就没有必要费尽心机地去计算似然概率或是后验概率。正是这化繁为简的原则给支持向量机带来了超乎寻常的优良效果。

一提到支持向量机，大部分人的第一反应都是核技巧。可核技巧诞生于1995年，而支持向量机早在30年前就已经面世。支持向量机（support vector machine）是基于几何意义的非概率线性二分类器，所谓的核技巧（kernel trick）只是支持向量机的一个拓展，通过维度的升高将决策边界从线性推广为非线性。所以对于支持向量机的基本原则的理解与核技巧无关，而是关乎**决策边界的生成方式**。
<div><strong>精选留言（6）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/11/86/a5/6ddc0156.jpg" width="30px"><span>鱼大</span> 👍（3） 💬（1）<div>多分类问题是不是要拆解成多个二分类问题，再综合？</div>2018-07-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（0） 💬（2）<div>“让这两条平行线以它们各自经过的异类点为不动点进行旋转，同时保证平行关系和分类特性不变。在旋转的过程中，两个不动点之间的欧式距离是不变的，但两条线的斜率一直在改变，因此它们之间的距离也会不断变化。当其中一条直线经过第二个数据点时，两条直线之间的距离就会达到最大值。“这里面的旋转方向有2种，只有其中一种会令直线之间的距离达到最大值，有什么形象且可推理的方式能判定哪种方向会令距离达到最大值吗？</div>2018-07-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（0） 💬（1）<div>之前看论文把支持向量机应用于多分类问题是一层层做二分类。</div>2018-07-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-06-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg" width="30px"><span>建强</span> 👍（0） 💬（0）<div>查了一下相关资料，用SVM解决多分类问题主要有以下4种方法：

1. 一对多:
训练时依次把某个类别的样本归为一类,其他剩余的样本归为另一类，这样k个类别的样本就构造出了k个SVM。
分类时将未知样本分类为具有最大分类函数值的那类。
优点：处理优化问题规模较小，实现简单，分类效率高。
缺点：可能会出现无法分类的现象，即一个未知样本无法预测其归属哪一类。

2.一对一：
在任意两个样本之间设计一个SVM，有K个类别的样本，就需要设计K(K-1)&#47;2个分类器，对一个未知样本进行分类时，取得票最多的类别作为其类别。
优点：实现简单，不会出现无法分类的现象。
缺点：当类别数增加时，分类器数量会成倍增加，增加了计算的复杂度，降低了分类的效率。

3.有向无环图：
这种方法类似于一对一，但类别数增加时，但分类器数量不会成倍增加，也不会出现无法分类现象。其基本思想是，先是用第1个类别和其他类别构造分类器进行预测，如果预测结果是其他类别；
则用第2类别和其他类别分类器进行预测，如果预测还是其他，则依次类推，继续用第3类别，第4类别和其他类别分类器进行预测。
优点：分类效率高，没有分类重叠和不可分类现象。
缺点：如果在某个节点上发生分类错误，则会把分类错误延续到该节点的后续结点上。

4.层次支持向量机：
采用决策树的基本思想，从根节点开始，采用某种方法将该结点所包含的类别划分为两个子类，然后再对两个子类进一步划分，如此循环，直到子类中只包含一个类别为止。</div>2021-04-11</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（0） 💬（0）<div>极客时间
21天打卡行动 55&#47;21
&lt;&lt;机器学习40讲&#47;17&gt;&gt;  几何角度看分类：支持向量机
今日所学
1,机器学习的算法关注的不仅是训练误差，更是泛化误差。
2,正中间的超平面实际上就是几何意义上最优的决策边界;
3,落在两条平行线上的几个异类点就是支持向量（support vector）。
4,间隔（margin）是支持向量机的核心概念之一，它是对支持向量到分离超平面的距离度量，可以进一步表示分类的正确性和可信程度;
5,间隔还可以分成几何间隔（geometric margin）和函数间隔（functional margin），
6,。支持向量机的基本思想就是找出能够正确划分数据集并且具有最大几何间隔的分离超平面（maximum-margin hyperplane）。
7,最终的决策边界仅与少数的支持向量有关，并不会受到大量普通数据的影响。
重点:
支持向量机是基于线性判别式几何意义的分类算法；
支持向量机通过间隔最大化来定义最优的决策边界；
支持向量机通过对偶问题来求解最优的决策边界；
支持向量机的目标是让结构风险最小化</div>2020-02-11</li><br/>
</ul>