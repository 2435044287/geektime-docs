今天的内容是线性回归的正则化扩展。正则化称得上是机器学习里的刮骨疗毒，刮的是过拟合（overfitting）这个任何机器学习方法都无法摆脱的附骨之疽。

本质上讲，**过拟合就是模型过于复杂，复杂到削弱了它的泛化性能**。由于训练数据的数目是有限的，因此我们总是可以通过增加参数的数量来提升模型的复杂度，进而降低训练误差。可人尽皆知的是，学习的本领越专精，应用的口径就越狭窄，过于复杂的模型就像那个御膳房里专门切黄瓜丝的御厨，让他改切萝卜就下不去刀了。

正则化（regularization）是用于抑制过拟合的方法的统称，它**通过动态调整估计参数的取值来降低模型的复杂度，以偏差的增加为代价来换取方差的下降**。这是因为当一些参数足够小时，它们对应的属性对输出结果的贡献就会微乎其微，这在实质上去除了非相关属性的影响。

在线性回归里，最常见的正则化方式就是在损失函数（loss function）中添加**正则化项**（regularizer），而添加的正则化项$R(\\lambda)$往往是待估计参数的$p$-范数。将均方误差和参数的范数之和作为一个整体来进行约束优化，相当于额外添加了一重关于参数的限制条件，避免大量参数同时出现较大的取值。由于正则化的作用通常是让参数估计值的幅度下降，因此在统计学中它也被称为**系数收缩方法**（shrinkage method）。
<div><strong>精选留言（8）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（27） 💬（1）<div>当参数的数目远远大于样本的数目的高维统计问题，并且参数的选择比较简单粗暴，其中有不少参数存在相关性时，比较建议用LASSO回归来降低参数数目。这样处理后才能做矩阵求逆运算。

LASSO回归会让很多参数的系数变成零，只保留一部分参数，一般是保留系数最大的，系数小的因子很可能是噪音。参数取值的幅度有可能不一样，比如有的参数是-1到1，有的是-10到10，那么系数也会受影响。因此，在使用LASSO之前，需要对参数的取值幅度进行调整，这样计算出来的系数才具有可比性。

当样本数远大于参数的数目时，岭回归计算更快。如果参数数量少而精，数值都调整好，偏度、峰度、正态化、去极值等等，而且普遍适用多种场景，参数可解释，这时比较适合用岭回归。

岭回归不会删除参数，会对参数的取值幅度进行压缩。特征值小的特征向量会被压缩得最厉害，因此，它也要求参数取值幅度最好差不多，这样系数差不多，压缩起来才更有意义。</div>2018-07-02</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/qsmAdOC3R3twep9xwiboiaNGlZ9dtY5NQZibVTKSpkwd6l63kicv3v5vSW3oO0erfxACL679azGTwEBkxfKNxs0VkQ/132" width="30px"><span>土土</span> 👍（3） 💬（1）<div>到章就晕头转向了，不知道问题出现在哪里，老师能列一下前置知识吗</div>2019-01-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/88/ec/1460179b.jpg" width="30px"><span>我心飞扬</span> 👍（1） 💬（1）<div>请问老师如果想做贝叶斯的这种优化方法，py里面或者matlab里面有对应的包吗？</div>2018-07-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/7e/8c/f029535a.jpg" width="30px"><span>hallo128</span> 👍（0） 💬（1）<div>贝叶斯统计老师有没有什么推荐书籍或课程，感觉贝叶斯视角这块完全没有入门。一直接触的都是频率学派的内容。</div>2019-03-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/d2/94/8bd217f1.jpg" width="30px"><span>Kudo</span> 👍（2） 💬（1）<div>LASSO和Ridge的图象说明太直观！！

不过关于LASSO还有一个小疑问，按照图示说的系数约束方形和等误差圆的切点应该只有一个点。推广到三维的情况，应该是系数结束立方体与等误差球的切点，似乎也只是一个顶点。如果是这样的话，是不是说LASSO只会过滤掉一个属性？

或者是我哪里理解的不对，请老师点解！</div>2018-12-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（1） 💬（0）<div>学习打卡</div>2023-05-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/29/ca/58/8b4723dc.jpg" width="30px"><span>奔跑的火龙果</span> 👍（0） 💬（0）<div>老师，请问这门课有配套的代码吗？
</div>2022-08-26</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（0） 💬（0）<div>极客时间
21天打卡行动 50&#47;21
&lt;&lt;机器学习40讲&#47;12&gt;&gt;正则化处理：收缩方法与边际化
今日所学:
1,过拟合就是模型过于复杂，复杂到削弱了它的泛化性能,
2,正则化（regularization）是用于抑制过拟合的方法的统称,通过动态调整估计参数的取值来降低模型的复杂度，以偏差的增加为代价来换取方差的下降;
3,贝叶斯主义对正则化的理解：正则化就是引入关于参数的先验信息。
4,利用贝叶斯定理可以得出，最可能的超参数取值应该让下面的后验概率最大化;
5,贝叶斯边际化:价值就在于计算出的结果就是最优的结果。
6, Python 库都可以直接实现不同的正则化处理。在 Scikit-learn 库中，线性模型模块 linear_model 中的 Lasso 类和 Ridge 类就可以实现 l_1 正则化和 l_2 正则化。
重点:
 正则化的作用是抑制过拟合，通过增加偏差来降低方差，提升模型的泛化性能；
正则化项的作用是对解空间添加约束，在约束范围内寻找产生最小误差的系数；
频率视角下的正则化与贝叶斯视角下的边际化作用相同；
边际化对未知的参数和超参数进行积分以消除它们的影响，天然具有模型选择的功能</div>2020-02-06</li><br/>
</ul>