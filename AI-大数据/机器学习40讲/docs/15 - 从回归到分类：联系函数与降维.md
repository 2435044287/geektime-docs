线性模型最初被用来解决回归问题（regression），可在实际应用中，更加普遍的是分类问题（classification）。要用线性模型解决分类问题的话，就需要将线性模型原始的连续输出转换成不同的类别。

在分类问题中，一种特殊的情况是类别非黑即白，只有两种，这样的问题就是二分类问题，它可以看成是多分类问题的一个特例，也是今天讨论的对象。

将回归结果转化为分类结果，其实就是将属性的线性组合转化成分类的标准，具体的操作方式有两种：一种是**直接用阈值区分回归结果**，根据回归值与阈值的关系直接输出样本类别的标签；另一种是**用似然度区分回归结果**，根据回归值和似然性的关系输出样本属于某个类别的概率。

这两类输出可以分别被视为**硬输出**和**软输出**，它们代表了解决分类问题不同的思路。

**硬输出是对数据的分类边界进行建模**。实现硬输出的函数，也就是将输入数据映射为输出类别的函数叫作**判别函数**（discriminant）。判别函数可以将数据空间划分成若干个决策区域，每个区域对应一个输出的类别。不同判别区域之间的分界叫作**决策边界**（decision boundary），对应着判别函数取得某个常数时所对应的图形。用线性模型解决分类问题，就意味着得到的决策边界具有线性的形状。
<div><strong>精选留言（8）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（8） 💬（1）<div>当不同分类样本的协方差矩阵相同时，使用线性判别分析；当不同分类样本的协方差矩阵不同时，则应该使用二次判别分析（Quadratic Discriminant Analysis）。LDA适合均值不同，方差相同的高斯分布，其决策边界是一个平面。QDA适合均值不同，方差也不同的高斯分布。在协方差矩阵相同时，LDA和QDA没有分类结果差异。在不同的协方差矩阵下，LDA和QDA的决策边界存在明显差异。</div>2018-07-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/79/9a/4f907ad6.jpg" width="30px"><span>Python</span> 👍（1） 💬（1）<div>老师，逻辑回归只适用于带标签的数据的分类任务吗</div>2019-01-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/79/9a/4f907ad6.jpg" width="30px"><span>Python</span> 👍（0） 💬（1）<div>x_min,x_max = shots.min() - 0.2,shots.max() +0.2
y_min, y_max = tackles.min() - 0.2, tackles.max() + 0.2

老师为什么要用最小值减去0.2，和最大值加0.2</div>2019-01-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/2a/dd/aee4d7de.jpg" width="30px"><span>夏震华(围巾)</span> 👍（0） 💬（1）<div>LDA、QDA ：http:&#47;&#47;www.mamicode.com&#47;info-detail-1819236.html这个比较直观，容易理解</div>2018-10-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/3c/1f/3948a3c6.jpg" width="30px"><span>paradox</span> 👍（0） 💬（1）<div>老师，您好
文中
说LR与LDA是以每个输出类别为单位，将每个类别的数据看作不同的整体，并寻找它们之间的分野。
如何理解呢？</div>2018-08-11</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（1） 💬（0）<div>极客时间
21天打卡行动 53&#47;21
&lt;&lt;机器学习40讲&#47;15&gt;&gt; 从回归到分类：联系函数与降维
今日所学
1,将回归结果转化为分类结果:一种是直接用阈值区分回归结果,另一种是用似然度区分回归结果;
2,硬输出是对数据的分类边界进行建模。实现硬输出的函数，也就是将输入数据映射为输出类别的函数叫作判别函数（discriminant）,
3,输出利用的是似然度，需要建立关于数据的概率密度的模型，常见的具体做法是对线性回归的结果施加某种变换,
4,好的分类算法既要让相同类别的数据足够接近，又要让不同类别的数据足够远离,
5,线性判别分析需要较强的假设来支持。
重点
在解决分类问题时，线性模型的回归值可以通过联系函数转化为分类结果；
线性判别分析假定数据来自均值不同但方差相同的正态分布，通过最大化类间方差与类内方差的比值计算线性边界；
逻辑回归计算的是不同类别的概率决策边界，输出的是给定数据属于不同类别的后验概率；
基于线性模型的分类方法计算出的决策边界是输入属性的线性函数。</div>2020-02-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-06-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/86/a5/6ddc0156.jpg" width="30px"><span>鱼大</span> 👍（0） 💬（0）<div>干货</div>2018-07-10</li><br/>
</ul>