你好，我是王天一。

计算机处理器速度的不断提升、存储容量的不断增加和网络带宽的不断扩展使得海量数据、尤其是非结构化数据的储存与处理成为可能，这不仅催生了“大数据”这一新生学科，也成为了机器学习和深度学习爆炸式发展的推动力。

在机器学习领域有这样一句箴言：“**数据和特征决定了机器学习性能的上限，而模型和算法只是不断逼近这个上限**。”数据的不足会导致维度灾难的问题：简而言之就是太少的数据不足以揭示其背后的规律，从而导致过拟合的发生，这显然不是理想的结果。反过来，数据数量越大，数据中蕴藏的模式与规律就更容易被捕捉，从而为算法的优化奠定良好的基础。

但现实与理想往往存在差距。在现实生活中，某些学习任务可能只有少量数据或少量标注数据可供使用，数据本身可能由于各种各样的原因无法获取，无标签数据的标注则会因消耗大量的时间和人力而难以实现。

从另外一个角度看，人工智能要模拟的是人类的智能，可人类的学习绝不需要大量同质数据的堆砌：卷积神经网络需要大量的训练图像来习得分辨猫狗的能力，可人类幼儿只需要在电视上或者生活中看见猫狗的形象，当他再看到相似的动物时，就会马上分清什么是猫什么是狗。

受到人类学习过程的启发，研究者们逐渐开始反其道而行之，力图突破数据数量对学习性能的限制，小样本学习（Few-shot Learning）的概念应运而生。**小样本学习希望解决传统机器学习方法在样本数据不充分时性能严重下降的问题，旨在利用较少的有监督样本数据去构建具有良好性能的机器学习模型。**