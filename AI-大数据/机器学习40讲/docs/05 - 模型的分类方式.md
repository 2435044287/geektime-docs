机器学习学的是输入和输出之间的映射关系，学到的映射会以模型的形式出现。从今天开始，我将和你聊聊关于模型的一些主题。

大多数情况下，机器学习的任务是求解输入输出单独或者共同符合的概率分布，或者拟合输入输出之间的数量关系。**从数据的角度看，如果待求解的概率分布或者数量关系可以用一组有限且固定数目的参数完全刻画，求出的模型就是参数模型（parametric model）；反过来，不满足这个条件的模型就是非参数模型（non-parametric model）**。

**参数模型的优点在于只用少量参数就完整地描述出数据的概率特性，参数集中的每个参数都具有明确的统计意义**。你可以回忆一下常用的典型概率分布，离散变量的二项分布$B(n, p)$只包含两个参数，分别代表独立重复试验的次数和每次试验中事件发生的概率；连续变量的正态分布$N(\\mu, \\sigma)$也是只包含两个参数，分别代表着随机变量的均值和方差。所以在参数模型的学习中，算法的任务就是求出这些决定概率特性的参数，只要参数确定了，数据的统计分布也就确定了，即使未知的数据无穷无尽，我们也可以通过几个简单的参数来确定它们的性质。

为什么在参数模型中，有限的参数就能够描述无限的数据呢？想必你已经发现，这样的便捷来自于超强的先验假设：所有数据符合特定类型的概率分布。在实际的学习任务中，我们并非对问题一无所知，通常会具有一定的先验知识。**先验知识并不源于对数据的观察，而是先于数据存在，参数模型恰恰就是先验知识的体现与应用**。
<div><strong>精选留言（15）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/11/6e/c6/4a7b2517.jpg" width="30px"><span>Will王志翔(大象)</span> 👍（31） 💬（1）<div># 从学习方法角度进行划分
	
## 参数模型 vs 非参数模型：全局普适性 vs 局部适用性
	
❶ 参数模型
	
“新闻总是抄袭历史，模式在深处等待发掘。“
	
当我们对问题有认知，有了「定性」的判断，然后我们便可以用「定量」的方式将它们刻画出来。所谓“参数模型”。
	
优点：简单，只需付出较小的计算代价就可以从假设空间中习得一个较好的模型
	
缺点：其可用性却严重依赖于先验知识的可信度，但若先验分布错误，则无法学出好的结果。
	
❷ 非参数模型
	
“知之为知之，不知为不知，是知也。”
	
当我们对问题知之甚少，最好的办法反而是避免对潜在模型做出过多的假设，所谓“非参数模型。”
	
优点：当训练数据趋于无穷多时，非参数模型可以逼近任意复杂的真实模型。
	
缺点：和参数相比，非参数模型的时空复杂度都会比参数模型大得多。
	
误区：“非参数模型”不是“无参数模型”，恰恰相反，非参数模型意味着模型参数的数目是不固定的，并且极有可能是无穷大，这决定了非参数模型不可能像参数模型那样用固定且有限数目的参数来完全刻画。
	
❸ 参数模型 vs 非参数模型
	
例子： 假定一个训练集中有 99 个数据，其均值为 100，方差为 1。那么对于第 100 个数据来说，它会以 99% 的概率小于哪一个数值呢？
	
核心区别：数据分布特征的整体性与局部性。
	
参数模型具有全局的特性，所有数据都满足统一的全局分布，如履至尊而制六合得到的扁平化结构。
	
非参数模型是种局部模型，每个局部都有支配特性的参数，如战国时代每个诸侯国都有自己的国君一样。
		
## 数据模型 vs 算法模型：可解释性 vs 精确性
	
❹ 数据模型 
	
代表：线性回归
	
优点：可解释性强
	
缺点：简单模型有时不能充分体现出复杂作用机制
	
❺ 算法模型
	
代表：随机森林
	
优点：可描绘复杂的数据，精确度高
	
缺点：可解释性弱
	
# 从学习对象角度进行划分
	
❻ 生成模型 vs 判别模型：联合分布 vs 条件分布
	
生成模型（generative model）学习的对象是输入 x 和输出 y 的联合分布 p(x,y)
	
判别模型学习的则是已知输入 x 的条件下，输出 y 的条件分布 p(y|x)
	
区分的例子：以判断某种语言是什么？前者输出学完所有语言。后者是学会不同语言的区别。
		
# 参数模型是主流，大数据出现后，非参数模型的应用前景如何？
	
有没有参数，并不是参数模型和非参数模型的区别。其区别主要在于总体的分布形式是否已知。而为何强调“参数”与“非参数”，主要原因在于参数模型的分布可以有参数直接确定。
	
参数模型：线形回归
	
非参模型：决策树 -&gt; 随机森林；核SVM；
	
半参数模型：神经网络(层数和神经元 → 参数模型) &#47;(深度学习中dropout → 非参数)
	
非参数应用挺广泛，如Kaggle火热模型，XGBOOST，效果就十分好。
	</div>2018-07-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（6） 💬（2）<div>GBDT，随机森林和SVM都是非参数模型？虽然可解释性不强，但在特征维度多，数据量够多，有标注的条件下，貌似读近10年的医疗类文献时用这几种机器学习方法声称预测准确度提高的例子还挺多的。感觉非参数就是用个黑盒子来猜数据规律的。</div>2018-06-15</li><br/><li><img src="" width="30px"><span>never_giveup</span> 👍（6） 💬（1）<div>看的有点吃力，王老师能举一些参数模型和非参数模型的例子吗？比如说逻辑思蒂回归，线性回归，决策树，随机森林，朴素贝叶斯，神经网络分别属于哪一类？判别模型和生成模型学习的分别是条件分布和联合分布，怎么理解？能以具体的模型举个例子么？</div>2018-06-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/ae/94/6932783f.jpg" width="30px"><span>Geek_405126</span> 👍（2） 💬（1）<div>请问老师：在用随机森林算法前，需要对数据先进行处理吗？比如，missing 值，或者特殊值。还有如果数据有categorical 的值，需要先进行处理吗？谢谢！</div>2018-06-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/ae/11/f04cc393.jpg" width="30px"><span>杨森</span> 👍（2） 💬（1）<div>有些疑惑，支持向量机是非参模型还是参数模型？ 网上有博客说是非参模型，对于线性svm，我理解他跟线性回归只是优化目标不一样。有些想归入参数模型，不知怎么看待</div>2018-06-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/92/6b/8e090306.jpg" width="30px"><span>Geek_360c81</span> 👍（1） 💬（1）<div>老师，神经网络是高度自由的非参模型吗</div>2018-06-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/cd/2b/5691bda8.jpg" width="30px"><span>刘明</span> 👍（0） 💬（1）<div>请问老师非参数模型的局部性如何理解？</div>2019-10-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/d8/4c/5bf9e5b5.jpg" width="30px"><span>z</span> 👍（0） 💬（1）<div>假设空间是什么?是所有的模型(映射)叫假设空间,或者说所有的参数组合</div>2018-12-14</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTI1EgOAyyS2vJicx5uIpNbWsEElUtNuD1WDBHuJ0NNx6k6ZDzEia7ibrI8L9679vFXzBMm7Q2zx6cjGg/132" width="30px"><span>韶华</span> 👍（0） 💬（1）<div>参数模型与非参数模型，生成模型与非生成模型，这两对模型之间有可比性吗，比较困惑</div>2018-06-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（1） 💬（0）<div>学习打卡</div>2023-05-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/33/a2/6c0ffc15.jpg" width="30px"><span>皮皮侠</span> 👍（1） 💬（0）<div>老师，如果数据集里有些数据是错的，因为日常生活里，出于某些随机或者其他原因，采集的数据有时并不能真实表达产生此数据主体的意图。这些数据掺合在里面，会对我们的模型有怎样的影响？以及如何处理这样的情况呢？</div>2020-03-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/21/d9/84/f1b10393.jpg" width="30px"><span>进击的矮子</span> 👍（0） 💬（0）<div>看了几节，感觉不同一般教材的是，说的很全面也很深入，少了算法的具体内容，多了高位的视角。</div>2021-09-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg" width="30px"><span>建强</span> 👍（0） 💬（0）<div>个人理解：
   非参模型并不依赖于有限且固定的参数来刻画数据模型，数据的所有统计特性全部源于数据本身，也即依据数据本身的特征来刻画出数据的总体特征，因此，非参模型真正发挥作用需要依赖大量的数据，当前，随着大数据概念的兴起，非参模型可以依托大量的数据来挖掘出大数据中潜在的特性，从而可以
更精确地刻画出真正的模型。</div>2020-11-01</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（0） 💬（0）<div>极客时间
21天打卡行动 43&#47;21
&lt;&lt;机器学习40讲&#47;05&gt;&gt;模型的分类方式
今日所学:
1,从数据的角度看，如果待求解的概率分布或者数量关系可以用一组有限且固定数目的参数完全刻画，求出的模型就是参数模型（parametric model）；反过来，不满足这个条件的模型就是非参数模型（non-parametric model）。
2,参数模型的优点在于只用少量参数就完整地描述出数据的概率特性，参数集中的每个参数都具有明确的统计意义;
3,先验知识并不源于对数据的观察，而是先于数据存在，参数模型恰恰就是先验知识的体现与应用。
4,非参数模型意味着模型参数的数目是不固定的，并且极有可能是无穷大，这决定了非参数模型不可能像参数模型那样用固定且有限数目的参数来完全刻画;
5,非参数模型其实可以理解为一种局部模型;
6,数据分布模型构造上数据模型（data model）和算法模型（algorithm model）;
7,参数模型与非参数模型的核心区别在于数据分布特征的整体性与局部性,数据模型和算法模型之间的矛盾就是模型的可解释性与精确性的矛盾;
8,生成模型（generative model）学习的对象是输入 \mathbf{x} 和输出 y 的联合分布 p(\mathbf{x}, y)，判别模型学习的则是已知输入 \mathbf{x} 的条件下，输出 y 的条件分布 p(y | \mathbf{x})。
重点:
1,不同的学习思路对应假设空间中不同的建模方式与学习方法；
2,参数模型和非参数模型的区别体现的是全局普适性和局部适用性的区别；
3,数据模型和算法模型的区别体现的是可解释性和精确性的区别；
4,生成模型和判别模型的区别体现的是联合分布和条件分布的区别。
</div>2020-01-30</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqWJ2sP28ZpDl3o7AwQkFFlMmvcWFHD3WK6XYunAN0gxZdVicibrcRdmc7xiczwYeFZychl4Lc4hGgeg/132" width="30px"><span>刘华原</span> 👍（0） 💬（0）<div>写的很清晰，很棒</div>2019-04-03</li><br/>
</ul>