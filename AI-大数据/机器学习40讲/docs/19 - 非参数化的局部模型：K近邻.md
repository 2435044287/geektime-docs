到目前为止，专栏中介绍的机器学习模型都属于参数模型，它们利用训练数据求解出关于问题的一般性知识，再将这些知识通过全局性模型的结构和参数加以外化。

一旦模型的结构和参数被确定，它们就不再依赖训练数据，可以直接用于未知数据的预测。而径向基核的出现一定程度上打破了这种规律，它将普适的全局特性打散成若干局部特性的组合，每个局部特性只能在它所覆盖的近邻区域内得以保持，由此产生的非结构化模型会具有更加灵活的表示能力。

在我看来，**局部化的核心作用是模型复杂度和拟合精确性的折中**。如果将整个输入空间看作一个大的整体区间，对它进行全局式的建模，那么单个模型就足以描述输入输出之间的规律，但这不可避免地会对表达能力造成较大的限制。

一个极端的情形是让所有输入的输出都等于同一个常数，这样的模型显然毫无信息量可言。可是在另一个极端，如果将局部特性继续加以细化，细化到让每个数据点都定义出不同局部特性的子区间，其结果就是基于实例的学习。

**基于实例的学习**（instance-based learning）也叫**基于记忆的学习**（memory-based learning），**它学习的不是明确的泛化模型，而是样本之间的关系**。

当新的样本到来时，这种学习方式不会用拟合好的算式去计算输出结果或是输出结果的概率，而是根据这个新样本和训练样本之间的关系来确定它的输出。在本地化的语境里，这就叫“近朱者赤，近墨者黑”。
<div><strong>精选留言（6）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/10/43/82/923f8ae8.jpg" width="30px"><span>Eric</span> 👍（2） 💬（1）<div>K取基数也只能保证二分类问题不会出现平票呀  其实好的排序选最前面的那个分类就行吧 即使出现平票 还是选择平票中距离小的类</div>2019-05-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/28/1e/76e19bd3.jpg" width="30px"><span>张立春</span> 👍（1） 💬（1）<div>训练集的样本库是不是也需要随着新数据的增加而不断扩大，否则怎么与时俱进呢？</div>2018-09-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/4d/eb/7a6d58b3.jpg" width="30px"><span>TomZ,张锐</span> 👍（0） 💬（1）<div>替老师回答一下，k尽量取奇数，避免出现平票的问题</div>2018-07-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/16/65/c22b4415.jpg" width="30px"><span>风华神使</span> 👍（0） 💬（1）<div>knn如何处理平票？</div>2018-07-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg" width="30px"><span>建强</span> 👍（1） 💬（0）<div>积极方法：根据训练样本直接训练模型，新样本出现时，直接根据训练模型就可以得到结果，
优点：预测新样本简单、高效；
缺点：模型训练的时间复杂度受制于训练样本的数据质量和数据规模以及某些超参数的配置，会产生过拟合或欠拟合现象，影响新样本的预测精度

消极方法：不预先训练模型，只存贮训练样本，等到新样本出现时，根据优化的算法对新样本进行预测
优点：不需要预先训练模型，没有模型训练的开销，新样本根据现有样本进行预测，由于样本的不断累积，会使预测精度逐步改善和提升。
缺点：当样本累积到一定程度，新样本的预测时间复杂度会增加，同样，受超参数的影响也会有过拟合或欠拟合现象。</div>2021-05-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-06-04</li><br/>
</ul>