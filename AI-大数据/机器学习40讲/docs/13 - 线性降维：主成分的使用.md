在前一篇文章中，我以岭回归和LASSO为例介绍了线性回归的正则化处理。这两种方法都属于**收缩方法**（shrinkage method），它们能够使线性回归的系数连续变化。但和岭回归不同的是，LASSO可以将一部分属性的系数收缩为0，事实上起到了筛选属性的作用。

和LASSO这种间接去除属性的收缩方法相对应的是**维度规约**。维度规约这个听起来个高大上的名称是数据挖掘中常用的术语，它有一个更接地气的同义词，就是**降维**（dimensionality reduction），也就是直接降低输入属性的数目来削减数据的维度。

对数据维度的探讨来源于“**维数灾难**”（curse of dimensionality），这个概念是数学家理查德·贝尔曼（Richard Bellman）在动态优化问题的研究中提出的。

发表于《IEEE模式分析与机器智能汇刊》（IEEE Transactions on Pattern Analysis and Machine Intelligence）第1卷第3期的论文《维数问题：一个简单实例（A Problem of Dimensionality: A Simple Example）》在数学上证明了当所有参数都已知时，属性维数的增加可以让分类问题的错误率渐进为0；可当未知的参数只能根据数量有限的样本来估计时，属性维数的增加会使错误率先降低再升高，最终收敛到0.5。
<div><strong>精选留言（9）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（5） 💬（2）<div>PCA和ICA都是把原始特征线性组合转换成新的不相关的特征，PCA里转换后的特征是正交的。网上搜索到的ICA会在数据白化预处理(data whitening)用到PCA，我的理解ICA转换产生的特征也是正交的。

PCA和LDA都是以观测数据点呈高斯分布为假设前提，而ICA假设观测信号是非高斯分布的信号源的线性组合，信号源分量都不是高斯分布或者最多只有一个是高斯分布。

ICA生成的新特征分量不仅是不相关的，而且要求是统计独立的。我的理解是这个要求比PCA的不相关或正交要求更高，包含更多信息。PCA的数据有可能不是由一些互相统计独立的特征分量生成的。如何区别ICA中的统计独立和PCA中的不相关&#47;正交我不懂。有文章提到在原始随机信号x是高斯随机向量，其PCA变换得到的y也是高斯随机向量，y的各个分量不仅是线性无关的，它们还是独立的。这段描述令我对线性无关和独立的区别更加困惑。

PCA选择新特征时用方差作为衡量标准，ICA根据网上部分描述会用到“非高斯性度量“来作为衡量标准。这里是不是衡量标准一定会有区别我不确定。</div>2018-07-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/fd/e3/f40eaddd.jpg" width="30px"><span>兆熊</span> 👍（1） 💬（2）<div>和第一季相比，第二季每篇文章的篇幅长了很多。建议老师将长文章一分为二，将每篇文章的语音控制在十分钟左右，以达到更好的学习效果。</div>2018-07-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/c4/eb/0cd6d6ff.jpg" width="30px"><span>zhoujie</span> 👍（0） 💬（1）<div>收缩方法可以使系数连续变化，这里“连续变化”怎么理解，收缩方法可以使系数缩小或者带来稀疏可以理解</div>2018-09-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/3c/1f/3948a3c6.jpg" width="30px"><span>paradox</span> 👍（0） 💬（3）<div>老师，您好
对于用SVD解释PCA
是不是
行数表示特征数，列数表示数据样本的个数，这样SVD后，就是U矩阵用作降维了。
如果是行数表示数据样本的个数，列数表示特征数，SVD后，就是V矩阵用作降维了。</div>2018-08-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/57/6e/f7fa7278.jpg" width="30px"><span>Howard.Wundt</span> 👍（1） 💬（0）<div>老师的文章排版非常优美，值得学习。
目前极客时间导出到印象笔记时，版面会发生变化，公式与文字之间错位严重，各位同学有何好办法处理之？</div>2018-10-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-06-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-06-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2b/3c/06/c0327d9c.jpg" width="30px"><span>哈皮先生</span> 👍（0） 💬（0）<div>老师，这一节能推荐一些相关的文献或书籍吗？</div>2022-09-07</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（0） 💬（0）<div>极客时间
21天打卡行动 51&#47;21
&lt;&lt;机器学习40讲&#47;13&gt;&gt; 线性降维：主成分的使用
回答老师问题
在机器学习中，还有一种和主成分分析名字相似的方法，叫作独立成分分析（independent component analysis）。那么这两者之间到底有什么区别和联系呢？
1. 主成分分析假设源信号间彼此非相关，独立成分分析假设源信号间彼此独立。
2. 主成分分析认为主元之间彼此正交，样本呈高斯分布；独立成分分析则不要求样本呈高斯分布。
来源:[https:&#47;&#47;blog.csdn.net&#47;shenziheng1&#47;article&#47;details&#47;53547401]
今日所学:
1,维数灾难深层次的原因在于数据样本的有限。
2,特征选择（feature selection）;
3,岭回归收缩系数的对象并非每个单独的属性，而是由属性的线性组合计算出来的互不相关的主成分，主成分上数据的方差越小，其系数收缩地就越明显。
4,主成分回归;
5,主成分分析是典型的特征提取方法，它和收缩方法的本质区别在于将原始的共线性特征转化为人为生成的正交特征，从而带来了数据维度的约简和数据压缩的可能性;
6,主成分分析可以看成对高斯隐变量的概率描述,
7,隐变量（latent variable）是不能直接观测但可以间接推断的变量;
8,概率主成分分析（probabilistic principal component analysis）体现的就是高斯型观测结果和高斯隐变量之间线性的相关关系，它是因子分析（factor analysis）的一个特例;
重点:
降维方法和特征提取技术要点:
 在有限的数据集下，数据维度过高会导致维数灾难；
降维的方法包括特征选择和特征提取；
主成分分析将原始的共线性特征转化为新的正交特征，从而实现特征提取；
概率主成分分析是因子分析的一种，是数据的生成模型。</div>2020-02-07</li><br/>
</ul>