俗话说得好：“支持向量机有三宝，间隔对偶核技巧”。在上一篇文章中我和你分享了间隔这个核心概念，今天就来看看对偶和核技巧的使用。对偶性主要应用在最优决策边界的求解中，其逻辑比较简单。

但在介绍核技巧时，会先后涉及核函数、核方法、核技巧这些近似的概念。虽然从名字上看，它们都是“核”字辈的兄弟，但是在含义和用途上却不能一概而论，因此有必要对它们做一些系统的梳理。

当支持向量机用于线性可分的数据时，不同类别的支持向量到最优决策边界的距离之和为$2 / || {\\bf w} ||$，其中的${\\bf w}$是超平面的线性系数，也就是法向量。不难看出，让间隔$|| {\\bf w} || ^ {-1}$最大化就是让$|| {\\bf w} || ^ 2$最小化，所以线性可分的支持向量机对应的最优化问题就是

$$ \\mathop {\\min }\\limits\_{{\\bf w}, b} \\dfrac{1}{2} || {\\bf w} || ^ 2$$

$$ {\\rm s.t.} y\_i ({\\bf w} ...
<div><strong>精选留言（10）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/16/a8/87/7b0a4036.jpg" width="30px"><span>婉儿飞飞</span> 👍（5） 💬（1）<div>“支持向量机对原问题和对偶问题之间等价关系的利用就是它的对偶性（duality）。”
这句话似乎也有点问题。首先，对偶性是拉格朗日函数的性质，任何带约束的非线性规划问题都可以写出对偶函数。
其次，主问题和对偶问题等价，不是利用了对偶性，而是因为满足KKT条件时，强对偶成立成立，也就是主偶问题的解相等。
最后，从拉格朗日函数里并不能看出最优解只和支持向量相关，而是由于KKT条件里的“对偶互补性条件”可推出。也就是，第j个输入向量的拉格朗日因子a_j大于0小于惩罚因子C时，向量j落在wx+b=1的边界上，从这里才能看出来只和支持向量相关。</div>2019-07-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（2） 💬（0）<div>支持向量机的推导公式我理解起来有些费劲。

(1) 第一组公式里s.t. yi(w⋅xi+b)≤1 是不是 s.t. yi(w⋅xi+b)≥1？这样在已知拉格朗日函数中alpha_i ≥ 0，后面的陈述“由于alpha_i 和 1−yi(w⋅xi+b)的符号相反，因此两者之积必然是小于0的“成立时需满足yi(w⋅xi+b)≥1。

(2) 我的理解：求解原问题（primal problem）相当于求解改写成所谓的广义拉格朗日函数（对偶函数？）的最大值（对于alpha）。因此“对于不是支持向量的数据点来说，等式右侧第二项中的1−yi(w⋅xi+b)是大于0的...”这句是不是可以写作“对于不是支持向量的数据点来说，等式右侧第二项中的1−yi(w⋅xi+b)是小于0的，因此在让L(w,b,α)最大化时，必须把这些点的贡献去除，去除的方式就是让系数alpha_i = 0”？

(3) “当参数w和b不满足原问题的约束时，总会找到能让目标函数取值为正无穷的alpha，这意味着最大值其实就是不存在。”里的参数w和b不满足原问题的约束等同于yi(w⋅xi+b)&lt;1和1−yi(w⋅xi+b)&gt;0？

(4) “原始的最小化问题就被等效为min&lt;w,b&gt;θp(w,b)⁡，也就是广义拉格朗日函数的极小极大问题”这里为什么等效于对于w和b求广义拉格朗日函数L(w,b,α)的最小值我不是太理解。除了我们需要寻找一个由w和b定义的超平面，它们到分类点的最近距离等同于||w||值最小（满足原问题的s.t.条件时）感觉有些联系外。就不太理解一个原来的受不等式约束的最小值问题是如何变成一个受等式约束（除了拉格朗日乘子的非负约束外）的既求最大值也求最小值的问题的。

(5) 上一季中的课程还未全部阅读。请问老师提到的鞍点（saddle point）是在哪一部分出现的？现在没什么印象了。谢谢。</div>2018-07-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（1） 💬（1）<div>王老师，请问径向基函数中的gamma参数变大，即高斯核的宽度变窄适用于什么场景？

调用scikit-learn的包时，一些默认参数无法分类的数据当把gamma值变大后，数据出现了较好分类的边界。什么情况下应该调整gamma参数？往什么方向调由什么因素决定？

谢谢。</div>2018-08-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/cd/5c/e09eac13.jpg" width="30px"><span>刘強</span> 👍（1） 💬（2）<div>老师，现在人工智能依赖信息的表示方式吗？比如现在计算机处理的信息都是二进制表示的，如果换一种表示方式，人工只能还灵不灵？如果依赖的话，现在所遇到的各种难题会不会是二进制的局限性导致的？</div>2018-07-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/41/8d/f14a278d.jpg" width="30px"><span>风的轨迹</span> 👍（0） 💬（1）<div>王老师这篇文章真好，我终于把与“核”相关的三个概念（核函数，核方法，核技巧）搞明白了。之前在别的教程里看到有&quot;先引入相似度函数，在相似度函数的基础上再引入核函数的&quot;这样的讲解方法。当时我就在想难道核方法和相似度上有某种联系？看了老师这篇文章才恍然大悟，原来核函数确实有局部化的特点。同时径向函数又把径向神经网络联系起来，透过知识点的相互联系我有感觉到这些相互联系的知识点背后可能存在一个更高层的思想把他们统一起来的感觉，我想我已经摸到了一些门路了吧</div>2018-12-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/35/4c/bba73ec1.jpg" width="30px"><span>terency李</span> 👍（0） 💬（1）<div>请问下老师，您文章里有句话是这么说的:
对于不是支持向量的数据点来说，等式右侧第二项中的 1−yi(w⋅xi+b)是大于 0 的
        不是支持向量的数据点指的是误分类点吗？如果不是，那按照原始优化问题说的yi(w⋅xi+b)≧1的，这就和你前面说的矛盾了</div>2018-11-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/c4/eb/0cd6d6ff.jpg" width="30px"><span>zhoujie</span> 👍（0） 💬（1）<div>线性可分的支持向量机是一个标准的凸二次规划问题，求解起来轻松加随意，既然如此，那么对于线性可分的问题，为何要通过拉格朗日乘子引入它的对偶问题？</div>2018-09-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（1） 💬（0）<div>学习打卡</div>2023-06-03</li><br/><li><img src="" width="30px"><span>炁氣气気</span> 👍（0） 💬（0）<div>各向同性不应该是isotropy吗？</div>2022-06-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（0） 💬（0）<div>https:&#47;&#47;blog.csdn.net&#47;deepinC&#47;article&#47;details&#47;79341632和https:&#47;&#47;www.cnblogs.com&#47;90zeng&#47;p&#47;Lagrange_duality.html对于理解拉格朗日函数会有一些帮助。</div>2018-07-15</li><br/>
</ul>