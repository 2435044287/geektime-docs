华盛顿大学教授、《终极算法》（The Master Algorithm）的作者佩德罗·多明戈斯曾在Communications of The ACM第55卷第10期上发表了一篇名为《机器学习你不得不知的那些事》（A Few Useful Things to Know about Machine Learning）的小文，介绍了12条机器学习中的“金科玉律”，其中的7/8两条说的就是对数据的作用的认识。

**多明戈斯的观点是：数据量比算法更重要**。即使算法本身并没有什么精巧的设计，但使用大量数据进行训练也能起到填鸭的效果，获得比用少量数据训练出来的聪明算法更好的性能。这也应了那句老话：**数据决定了机器学习的上限，而算法只是尽可能逼近这个上限**。

但多明戈斯嘴里的数据可不是硬件采集或者软件抓取的原始数据，而是经过特征工程处理之后的精修数据，**在他看来，特征工程（feature engineering）才是机器学习的关键**。通常来说，原始数据并不直接适用于学习，而是特征筛选、构造和生成的基础。一个好的预测模型与高效的特征提取和明确的特征表示息息相关，如果通过特征工程得到很多独立的且与所属类别相关的特征，那学习过程就变成小菜一碟。

**特征的本质是用于预测分类结果的信息，特征工程实际上就是对这些信息的编码。**机器学习中的很多具体算法都可以归纳到特征工程的范畴之中，比如使用$L\_1$正则化项的**LASSO回归**，就是通过将某些特征的权重系数缩小到0来实现特征的过滤；再比如**主成分分析**，将具有相关性的一组特征变换为另一组线性无关的特征。这些方法本质上完成的都是特征工程的任务。
<div><strong>精选留言（16）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/12/79/df/bae913b7.jpg" width="30px"><span>曾珍</span> 👍（5） 💬（2）<div>空值我是用独热编码的方式，好想处理结果比线回归填充好一点</div>2018-09-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/08/eb/594e9e6c.jpg" width="30px"><span>五岳寻仙</span> 👍（3） 💬（2）<div>老师好！在删除不具备区分度的特征时，老师讲到：

什么样的特征不具备区分度呢？这里有两个经验性的标准：一是特征取值的总数与样本数目的比例在 10% 以下，这样的特征在 100 个样本里的取值数目不超过 10 个；二是出现频率最高的特征取值的出现频率应该在出现频率次高的特征取值频率的 20 倍以上，如果有 90 个样本的特征取值为 1，4 个样本的特征取值为 2，其余取值的样本数目都在 4 个以下，这样的特征就可以被删除了。

我不太理解，意思是如果一个特征(类别变量)的取值太少(小于样本数的10%)就该被删掉吗？可是我们平时遇到很多情况，类别变量取值都是有限的几个(比如性别：男，女)。</div>2018-09-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/88/ec/1460179b.jpg" width="30px"><span>我心飞扬</span> 👍（3） 💬（1）<div>请问空间标识和log的方法是要一起用吗？还是说。有负数就不能用log，这时候怎么办？如果统一把他加成正数，这样合理吗？会不会对分析产生一些误导呢。</div>2018-06-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（2） 💬（1）<div>特征尺度不一致还是挺常见的。用的是文中提到的标准化方法。缺失值的K近邻和插值方法以前实践中只知道信号处理里有插值的函数，其他领域还没用过。</div>2018-06-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/59/9e/20ba145c.jpg" width="30px"><span>rkq@geekbang</span> 👍（1） 💬（1）<div>关于特征缩放我有一个问题：如果我的模型是普通的线性回归，需要对特征做缩放处理吗？我的理解是不需要，因为最终学得的参数就会体现出特征的缩放。不知道对不对？</div>2018-06-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/33/f8/8c2bae4b.jpg" width="30px"><span>暴走的carry</span> 👍（0） 💬（1）<div>对于处理缺失值，以前我只知道用平均值或众数来代替，现在学会了，还能内嵌一个机器学习算法来处理缺失值，突然高端了好多</div>2019-01-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/59/fd/128cc75b.jpg" width="30px"><span>Daryl</span> 👍（0） 💬（1）<div>有个入门的问题，麻烦帮我解答下。
1:对训练集标准化&#47;归一化&#47;pca后，是否也要对测试集执行同样操作？
2:如果同样的操作，是直接对测试集transferm()，还是fit_transferm()？
3:标准化&#47;归一化&#47;pca 怎么针对数据集选择用哪种方式？</div>2019-01-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/8d/54/5a286327.jpg" width="30px"><span>黄海娜</span> 👍（0） 💬（1）<div>老师，空值怎么用独热编码的方式呀？</div>2018-11-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/88/c8/6af6d27e.jpg" width="30px"><span>Y024</span> 👍（0） 💬（1）<div>https:&#47;&#47;homes.cs.washington.edu&#47;~pedrod&#47;papers&#47;cacm12.pdf
文中所提小文链接</div>2018-10-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/ae/94/6932783f.jpg" width="30px"><span>Geek_405126</span> 👍（0） 💬（1）<div>在用随机森林模型的时候，我们能知道每棵树在不同layer的具体特征变量名字吗？</div>2018-06-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/88/ec/1460179b.jpg" width="30px"><span>我心飞扬</span> 👍（0） 💬（1）<div>做标准化之后有负数不能log了 是不是先log</div>2018-06-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/06/1f/9537d8cb.jpg" width="30px"><span>打不过就跑</span> 👍（0） 💬（1）<div>求平方根和求倒数 具体是什么方法，查都查不到</div>2024-04-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/eb/d7/90391376.jpg" width="30px"><span>ifelse</span> 👍（0） 💬（0）<div>学习打卡</div>2023-05-30</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/g1icQRbcv1QvJ5U8Cqk0ZqMH5PcMTXcZ8TpS5utE4SUzHcnJA3FYGelHykpzTfDh55ehE8JO9Zg9VGSJW7Wxibxw/132" width="30px"><span>杨家荣</span> 👍（0） 💬（0）<div>极客时间
21天打卡行动 48&#47;21
&lt;&lt;机器学习40讲&#47;10&gt;&gt;实验设计
今日所学:
0,多明戈斯的观点是：数据量比算法更重要
1,数据决定了机器学习的上限，而算法只是尽可能逼近这个上限
2,特征工程（feature engineering）才是机器学习的关键;
3,特征的本质是用于预测分类结果的信息，特征工程实际上就是对这些信息的编码;
4,特征缩放的作用就是消除特征的不同尺度所造成的偏差;
5,标准化的方法用原始数据减去均值再除以标准差，不管原始特征的取值范围有多大，得到的每组新数据都是均值为 0，方差为 1;
6,面对偏度较大的数据，第一反应就应该是检查是否有异常点存在;
7,空间标识算法将所有的数据点都映射到高维空间的球面上，这个映射和标准化或者归一化的不同之处在于它处理的对象并不是所有样本的同一个特征，而是同一个样本的所有特征，让所有样本呈现一致的尺度。
8,对数据进行去偏度处理的常用方法就是取对数变换（log transformation）;
9,在模型训练之前移除一些特征有助于增强模型的可解释性，也可以降低计算中的开销。
重点:
特征缩放可以让不同特征的取值具有相同的尺度，方法包括标准化和归一化；
异常点会导致数据的有偏分布，对数变换和空间标识都可以去除数据的偏度；
 k 近邻方法和线性回归可以用来对特征的缺失值进行人为赋值；
删除不具备区分度的特征能够降低计算开销，增强可解释性。</div>2020-02-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/53/94/0ae2a315.jpg" width="30px"><span>Right as rain</span> 👍（0） 💬（1）<div>老师，如何判断数据正负样本不平衡，1000正100负就算，还是1000正10负，有没有一些数据样本不平衡的评价标准呢？</div>2019-12-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/07/3f/53ae01f6.jpg" width="30px"><span>Kevin.zhang🌏</span> 👍（0） 💬（0）<div>作业：
       前段时间在通过爬虫程序获取了原始数据，在数据清洗的阶段，发现有很多的缺失数据，还有重复数据，重复数据之前没有使用pandas，就直接用的SQL筛选，对于缺失数据，我采用的笨办法，就是直接观察是哪个特征缺失，然后进行最原始的人工赋值替换操作，说实话，工作量大还不靠谱！边做心里还边打鼓！我不知道如何采用线性回归和K近邻算法操作！</div>2018-12-26</li><br/>
</ul>