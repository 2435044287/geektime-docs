我们上一节讲了朴素贝叶斯的工作原理，今天我们来讲下这些原理是如何指导实际业务的。

朴素贝叶斯分类最适合的场景就是文本分类、情感分析和垃圾邮件识别。其中情感分析和垃圾邮件识别都是通过文本来进行判断。从这里你能看出来，这三个场景本质上都是文本分类，这也是朴素贝叶斯最擅长的地方。所以朴素贝叶斯也常用于自然语言处理NLP的工具。

今天我带你一起使用朴素贝叶斯做下文档分类的项目，最重要的工具就是sklearn这个机器学习神器。

## sklearn机器学习包

sklearn的全称叫Scikit-learn，它给我们提供了3个朴素贝叶斯分类算法，分别是高斯朴素贝叶斯（GaussianNB）、多项式朴素贝叶斯（MultinomialNB）和伯努利朴素贝叶斯（BernoulliNB）。

这三种算法适合应用在不同的场景下，我们应该根据特征变量的不同选择不同的算法：

**高斯朴素贝叶斯**：特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。

**多项式朴素贝叶斯**：特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的TF-IDF值等。

**伯努利朴素贝叶斯**：特征变量是布尔变量，符合0/1分布，在文档分类中特征是单词是否出现。
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/11/02/82/abed70a0.jpg" width="30px"><span>北方</span> 👍（35） 💬（2）<div>#!&#47;usr&#47;bin&#47;env python
# -*- coding:utf8 -*-
# __author__ = &#39;北方姆Q&#39;
# __datetime__ = 2019&#47;2&#47;14 14:04

import os
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

LABEL_MAP = {&#39;体育&#39;: 0, &#39;女性&#39;: 1, &#39;文学&#39;: 2, &#39;校园&#39;: 3}
# 加载停用词
with open(&#39;.&#47;text classification&#47;stop&#47;stopword.txt&#39;, &#39;rb&#39;) as f:
    STOP_WORDS = [line.strip() for line in f.readlines()]


def load_data(base_path):
    &quot;&quot;&quot;
    :param base_path: 基础路径
    :return: 分词列表，标签列表
    &quot;&quot;&quot;
    documents = []
    labels = []

    for root, dirs, files in os.walk(base_path):    # 循环所有文件并进行分词打标
        for file in files:
            label = root.split(&#39;\\&#39;)[-1]        # 因为windows上路径符号自动转成\了，所以要转义下
            labels.append(label)
            filename = os.path.join(root, file)
            with open(filename, &#39;rb&#39;) as f:     # 因为字符集问题因此直接用二进制方式读取
                content = f.read()
                word_list = list(jieba.cut(content))
                words = [wl for wl in word_list]
                documents.append(&#39; &#39;.join(words))
    return documents, labels


def train_fun(td, tl, testd, testl):
    &quot;&quot;&quot;
    构造模型并计算测试集准确率，字数限制变量名简写
    :param td: 训练集数据
    :param tl: 训练集标签
    :param testd: 测试集数据
    :param testl: 测试集标签
    :return: 测试集准确率
    &quot;&quot;&quot;
    # 计算矩阵
    tt = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5)
    tf = tt.fit_transform(td)
    # 训练模型
    clf = MultinomialNB(alpha=0.001).fit(tf, tl)
    # 模型预测
    test_tf = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5, vocabulary=tt.vocabulary_)
    test_features = test_tf.fit_transform(testd)
    predicted_labels = clf.predict(test_features)
    # 获取结果
    x = metrics.accuracy_score(testl, predicted_labels)
    return x


# text classification与代码同目录下
train_documents, train_labels = load_data(&#39;.&#47;text classification&#47;train&#39;)
test_documents, test_labels = load_data(&#39;.&#47;text classification&#47;test&#39;)
x = train_fun(train_documents, train_labels, test_documents, test_labels)
print(x)</div>2019-02-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/a3/f9/9180d6d1.jpg" width="30px"><span>szm</span> 👍（37） 💬（3）<div>需要完整代码，不然看不明白！</div>2019-01-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/79/9a/4f907ad6.jpg" width="30px"><span>Python</span> 👍（18） 💬（1）<div>老师，能不能在答疑的时候给这道题的完整代码看看</div>2019-01-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/28/9c/73e76b19.jpg" width="30px"><span>姜戈</span> 👍（16） 💬（1）<div>看过很多朴素贝叶斯原理和分类的讲解文章，很少能像前辈这样既有理论，又有实战的讲解，让大家既了解了理论知识，又有相应实际的操作经验可学，真的好棒，这个专栏，必须多多点赞，为老师加油！！！</div>2019-01-30</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIlShPdVFbIaUu0wtcuSrlkG9r5zBedPuuN4Pyichof0QnMvr4J0G4kykyA4cgrlYibZ6wZ6NJNevFQ/132" width="30px"><span>Geek_z0wqck</span> 👍（5） 💬（1）<div>https:&#47;&#47;github.com&#47;yourSprite&#47;AnalysisExcercise&#47;tree&#47;master&#47;%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB</div>2019-02-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/ee/b8/da245945.jpg" width="30px"><span>几何</span> 👍（4） 💬（2）<div>老师，弱弱的说一句，代码感觉能看明白，但是不明白的是模型是如何使用的， 比如上一节和本节，都是只知道了准确率，但是对于有新的要处理的数据，如何做，怎么做好总是感觉差一点点东西。</div>2019-09-08</li><br/><li><img src="" width="30px"><span>Jack</span> 👍（3） 💬（1）<div>#!&#47;usr&#47;bin&#47;env python
# coding: utf-8

import os
import jieba
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
# 1. 加载数据
# 加载停用词表
l_stopWords = set()
with open(&#39;.&#47;text_classification&#47;text_classification&#47;stop&#47;stopword.txt&#39;, &#39;r&#39;) as l_f:
    for l_line in l_f:
        l_stopWords.add(l_line.strip())

l_labelMap = {&#39;体育&#39;: 0, &#39;女性&#39;: 1, &#39;文学&#39;: 2, &#39;校园&#39;: 3}
# 加载训练数据和测试数据
def LoadData(filepath):
    l_documents = []
    l_labels = []
    for root, dirs, files in os.walk(filepath):
        for l_file in files:
            l_label = root.split(&#39;&#47;&#39;)[-1]
            l_filename = os.path.join(root, l_file)
            
            with open(l_filename, &#39;r&#39;) as l_f:
                l_content = l_f.read()
                l_wordlist = list(jieba.cut(l_content))
                l_words = [item for item in l_wordlist if item not in l_stopWords]
                l_documents.append(&#39; &#39;.join(l_words))
                l_labels.append(l_labelMap[l_label])
                
    return l_documents, l_labels

l_trainDocuments, l_trainLabels = LoadData(&#39;.&#47;text_classification&#47;text_classification&#47;train&#39;)
l_testDocuments, l_testLabels = LoadData(&#39;.&#47;text_classification&#47;text_classification&#47;test&#39;)

# # 2. 计算权重矩阵
l_tfidfVec = TfidfVectorizer(max_df=0.5)
l_tfidfMatrix = l_tfidfVec.fit_transform(l_trainDocuments)

# for item in l_tfidfVec.get_feature_names():
#     print item
# print l_tfidfVec.get_feature_names()
# print l_tfidfVec.vocabulary_
print l_tfidfMatrix.toarray().shape

# # 3. 朴素贝叶斯模型
# ## 3.1 模型训练
l_clf = MultinomialNB(alpha=0.001)
l_clf.fit(l_tfidfMatrix, l_trainLabels)

# ## 3.2 模型预测
l_testTfidf = TfidfVectorizer(max_df=0.5, vocabulary=l_tfidfVec.vocabulary_)
l_testFeature = l_testTfidf.fit_transform(l_testDocuments)
l_hats = l_clf.predict(l_testFeature)

# ## 3.3 模型评估
from sklearn.metrics import accuracy_score
print accuracy_score(l_hats, l_testLabels)</div>2019-02-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/21/b4/76/edb4988e.jpg" width="30px"><span>Jasmine</span> 👍（2） 💬（2）<div>老师，我想请教一下，计算单词权重时，为什么train_features用的fit_transform方法，而test_feature用的是transform</div>2020-11-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/a3/87/c415e370.jpg" width="30px"><span>滢</span> 👍（2） 💬（2）<div>最后面的代码太乱，很多都不知道从哪里来的，无法顺着看下去~~~</div>2019-04-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg" width="30px"><span>Geek_hve78z</span> 👍（2） 💬（1）<div># -*- coding:utf8 -*-
# 系统：mac 

# 1. 加载数据
# 加载停用词表

l_stopWords = [line.strip() for line in open(&#39;.&#47;text_classification-master&#47;text classification&#47;stop&#47;stopword.txt&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;).readlines()]  
   
l_labelMap = {&#39;体育&#39;: 0, &#39;女性&#39;: 1, &#39;文学&#39;: 2, &#39;校园&#39;: 3}
# 加载训练数据和测试数据
def LoadData(filepath):
    l_documents = []
    l_labels = []
    
    for root, dirs, files in os.walk(filepath):
        for l_file in files:
            if l_file==&#39;.DS_Store&#39;:
                continue
            l_label = root.split(&#39;&#47;&#39;)[-1]
            l_filename = os.path.join(root, l_file)
            
            with open(l_filename, &#39;r&#39;,encoding=&#39;gbk&#39;) as l_f:
                try:
                    l_content = l_f.read()
                except Exception as err:
                    print(err)
                    print(l_filename)
                    continue
                generator = jieba.cut(l_content)
                words = &#39; &#39;.join(generator)
                l_wordlist=words.split(&#39; &#39;)
                l_words = [item for item in l_wordlist if item not in l_stopWords]
                l_documents.append(&#39; &#39;.join(l_words))
                l_labels.append(l_labelMap[l_label])
                
    return l_documents, l_labels

l_trainDocuments, l_trainLabels = LoadData(&#39;.&#47;text_classification-master&#47;text classification&#47;train&#39;)
l_testDocuments, l_testLabels = LoadData(&#39;.&#47;text_classification-master&#47;text classification&#47;test&#39;)

# # 2. 计算权重矩阵
l_tfidfVec = TfidfVectorizer(max_df=0.5)
l_tfidfMatrix = l_tfidfVec.fit_transform(l_trainDocuments)

print (l_tfidfMatrix.toarray().shape)

# # 3. 朴素贝叶斯模型
# ## 3.1 模型训练
l_clf = MultinomialNB(alpha=0.001)
l_clf.fit(l_tfidfMatrix, l_trainLabels)

# ## 3.2 模型预测
l_testTfidf = TfidfVectorizer(max_df=0.5, vocabulary=l_tfidfVec.vocabulary_)
l_testFeature = l_testTfidf.fit_transform(l_testDocuments)
l_hats = l_clf.predict(l_testFeature)

# ## 3.3 模型评估
from sklearn.metrics import accuracy_score
print (accuracy_score(l_hats, l_testLabels))</div>2019-04-05</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqMiaIuXLFmXvVlnP9Do2icudO3JV6l5ueicWYYFhZb2ftT9XSKHFHJWa33XLnUKlCSs0JhvI7omF8Mg/132" width="30px"><span>wzhan366</span> 👍（2） 💬（1）<div>建议 大家先做英文版本，因为中文的unicode encode和decode不是很好弄，不利于中间步骤的可视化。如果对代码有疑惑，可以试试这个pipeline， sklearn 的。 不过，这个没有用NTLK。</div>2019-02-06</li><br/><li><img src="" width="30px"><span>lemonlxn</span> 👍（1） 💬（1）<div>此处没有对 sklearn 的tfidf进行讲解，为完善结果，总结如下：

传统 TF_IDF = TF * IDF

	TF  = 该文档某单词出现次数 &#47; 该文档的总单词数

	IDF = loge(文档总数 &#47; 该单词出现的文档数 + 1)


sklearn TF_IDF = TF * IDF

	TF  = 该文档某单词出现次数
        IDF = loge((1 + n) &#47; (1 + dn)) + 1
       
        其中 n  为训练集文档数
               dn 为测试集出现该单词的文档数
        
        如果norm=None 则结果为 
          X.toarray()
        
        如果 norm=&#39;l2&#39; 则结果为：
          X.toarray() &#47; np.sum(X.toarray() ** 2) ** 0.5
        
        如果 norm=&#39;l1&#39;则结果为
          X.toarray() &#47; np.sum(X.toarray())</div>2020-09-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/56/64/7d80093c.jpg" width="30px"><span>黄争辉</span> 👍（1） 💬（1）<div>这里百思不得解，
训练集中，（4个文件的词混在一起）每个词的 TF-IDF值和分类都混在一起了，是如何实现分类器的呢？
</div>2020-09-10</li><br/><li><img src="" width="30px"><span>Viola</span> 👍（1） 💬（1）<div>有大量代码实现的 ，能否将github地址贴一下</div>2019-01-31</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/SAicxHNJGBiavTgLlYXydetlV4S1Lr1icEbVVCY7LCFK0WVnP8udTWCwkibevnclMWOnfREugguzLM11aBunicIicyCg/132" width="30px"><span>许愿字节上岸冲冲冲</span> 👍（0） 💬（1）<div>老师，是不是可以理解为fit函数计算了idf的值和特征单词，transform函数计算了tf的值并与idf相乘算出了真正的tf-idf。所以train_features调用了fit_transform计算出了idf，test_features调用了transform则通过test_features计算出的TF值乘以了之前通过train_features算出的idf值得出来最终的TF-IDF矩阵？</div>2021-04-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/06/4d/430faf63.jpg" width="30px"><span>.</span> 👍（0） 💬（2）<div>最后只是得出来了 分类的准确率？那么 再有一批新的文档数据 怎么把分类算法应用上去把新的文档数据进行分类开呢？</div>2021-01-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/86/79/066a062a.jpg" width="30px"><span>非同凡想</span> 👍（0） 💬（1）<div>交作业：
import jieba
from sklearn.naive_bayes import MultinomialNB
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score

BASE_DIR=&#39;&#47;home&#47;zjtprince&#47;Documents&#47;text_classification&#47;text classification&#47;&#39;

def cut_text(filepath):
    text = open(filepath,&#39;r&#39;,encoding=&#39;gb18030&#39;).read()
    words = jieba.cut(text)
    return &#39; &#39;.join(words) ;

def load_features_and_labels(dir , label):
    features = []
    labels = []
    files = os.listdir(dir)
    for file in files:
        features.append(cut_text(dir + os.sep + file))
        labels.append(label)
    return features , labels

def build_word_list_and_label_list(type_name):
    train_features1, labels1 = load_features_and_labels(BASE_DIR+type_name+&#39;&#47;女性&#39;, &#39;女性&#39;)
    train_features2, labels2 = load_features_and_labels(BASE_DIR+type_name+&#39;&#47;文学&#39;, &#39;文学&#39;)
    train_features3, labels3 = load_features_and_labels(BASE_DIR+type_name+&#39;&#47;校园&#39;, &#39;校园&#39;)
    train_features4, labels4 = load_features_and_labels(BASE_DIR+type_name+&#39;&#47;体育&#39;, &#39;体育&#39;)
    train_list = train_features1 + train_features2 + train_features3 + train_features4
    label_list = labels1 + labels2 + labels3 + labels4
    return train_list, label_list

def load_stop_words():
    stop_words = open(BASE_DIR+&quot;stop&#47;stopword.txt&quot;, &#39;r&#39;,encoding=&#39;utf-8&#39;).read()
    stop_words = stop_words.encode(&#39;utf-8&#39;).decode(&#39;utf-8-sig&#39;)
    return stop_words.split(&#39;\n&#39;)

if __name__ == &#39;__main__&#39;:
    stop_words = load_stop_words()
    train_list, label_list = build_word_list_and_label_list(&#39;train&#39;)
    test_list, test_labels = build_word_list_and_label_list(&#39;test&#39;)

    vec = TfidfVectorizer(stop_words=stop_words)
    vec.fit(train_list)
    train_data = vec.transform(train_list)
    test_data = vec.transform(test_list)

    bayes = MultinomialNB(alpha=0.001)
    ctf = bayes.fit(train_data, label_list)

    predict = ctf.predict(test_data)
accur = accuracy_score(predict,test_labels)
print(&quot;准确率为：%f&quot; , accur)</div>2020-11-21</li><br/><li><img src="" width="30px"><span>与君共勉</span> 👍（0） 💬（1）<div>那个矩阵每个元素代表什么？每个单词的TF-IDF值吗？怎么感觉对不上。不同的行的又代表什么呢？详细解释一下这个矩阵</div>2020-10-29</li><br/><li><img src="" width="30px"><span>lemonlxn</span> 👍（0） 💬（1）<div>IDF = log2(total_document &#47; (appear_document + 1 )) 

由于 total_document 是固定的，如果希望 IDF 越大，则 appear_document 越小越好。

appear_document 越小，那么IDF则会越大，该词区分度也会越大，该文档可以通过这个词，更容易区分其他文档</div>2020-09-24</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJAl7V1ibk8gX62W5I4SER2zbQAj3gy5icJlavGhnAmxENCia7QFm8lE3YBc5HOHvlyNVFz7rQKFQ7dA/132" width="30px"><span>timeng27</span> 👍（0） 💬（1）<div>我准备先大致过一遍，里面的练习还是不错的。
不过这样听着音频学习，还是不习惯。

三个朴素贝叶斯:
1,高斯——数据连续变化；
2,多项——常用文本分类，TF-IDF
3,伯努利——0&#47;1</div>2020-04-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/b5/98/ffaf2aca.jpg" width="30px"><span>Ronnyz</span> 👍（0） 💬（3）<div>练习题：
acc_score :  0.755 比老师的代码运行的结果差了许多，不知道是什么原因呢
思考题：
身高、体重 ：这是连续型变量，适合高斯贝叶斯
鞋码、外貌 ：这个一般用离散取值，适合多项式贝叶斯
</div>2019-11-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/f4/52/10c4d863.jpg" width="30px"><span>FeiFei</span> 👍（0） 💬（2）<div>我有个疑问，贝叶斯是基于训练数据来进行分类的，训练数据已经有标签了。
但在没有分类好的数据前，怎么去把一些数据打上这个标签？
感觉我陷入了一个死循环。
</div>2019-07-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/50/91/0dd2b8ce.jpg" width="30px"><span>听妈妈的话</span> 👍（0） 💬（1）<div>我的代码位于：https:&#47;&#47;pastebin.com&#47;kqjXgy0c ，最终结果0.925
注意: 中文分词，TfidfVectorizer增加一个参数：tokenizer=jieba.cut,（来自github jieba issue: https:&#47;&#47;github.com&#47;fxsjy&#47;jieba&#47;issues&#47;332）


train_contents=[]
train_labels=[]
test_contents=[]
test_labels=[]
#  导入文件
import os
import io
start=os.listdir(r&#39;text classification&#47;train&#39;)
for item in start:
    test_path=&#39;text classification&#47;test&#47;&#39;+item+&#39;&#47;&#39;
    train_path=&#39;text classification&#47;train&#47;&#39;+item+&#39;&#47;&#39;
    for file in os.listdir(test_path):
        with open(test_path+file,encoding=&quot;GBK&quot;) as f:
            test_contents.append(f.readline())
            #print(test_contents)
            test_labels.append(item)
    for file in os.listdir(train_path):
        with open(train_path+file,encoding=&#39;gb18030&#39;, errors=&#39;ignore&#39;) as f:
            train_contents.append(f.readline())
            train_labels.append(item)
print(len(train_contents),len(test_contents))
 
# 导入stop word
import jieba
from sklearn import metrics
from sklearn.naive_bayes import MultinomialNB  
stop_words = [line.strip() for line in io.open(&#39;text classification&#47;stop&#47;stopword.txt&#39;).readlines()]
 
# 分词方式使用jieba,计算单词的权重
tf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words, max_df=0.5)
train_features = tf.fit_transform(train_contents)
print(train_features.shape)
 
模块 4：生成朴素贝叶斯分类器
# 多项式贝叶斯分类器
clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)
 
模块 5：使用生成的分类器做预测
test_tf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words, max_df=0.5, vocabulary=tf.vocabulary_)
test_features=test_tf.fit_transform(test_contents)
 
print(test_features.shape)
predicted_labels=clf.predict(test_features)
print(metrics.accuracy_score(test_labels, predicted_labels))
 
# 最终结果0.925

</div>2019-03-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/de/17/a3b8f785.jpg" width="30px"><span>小莫</span> 👍（0） 💬（1）<div>老师，完整代码能贴出来吗？</div>2019-03-10</li><br/><li><img src="" width="30px"><span>三硝基甲苯</span> 👍（0） 💬（1）<div>import jieba
import glob
import io
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

classification = [&quot;campus&quot;, &quot;female&quot;, &quot;sports&quot;, &quot;literature&quot;]
train_files_list = []
test_files_list = []
trainpathprefix = &quot;.&#47;text_classification&#47;train&#47;&quot;
testpathprefix = &quot;.&#47;text_classification&#47;test&#47;&quot;
pathsuffix = &quot;&#47;*.txt&quot;
train_label = []
test_label = []
train_docments = []
test_docments = []
stopword_path = &#39;.&#47;text_classification&#47;stop&#47;stopword.txt&#39;

for i in classification:
    trainpathstr = trainpathprefix + i + pathsuffix
    testpathstr = testpathprefix + i + pathsuffix
    trainpathlist = glob.glob(trainpathstr)
    lentrainlist = len(trainpathlist)
    train_label += [i for j in range(lentrainlist)]
    testpathlist = glob.glob(testpathstr)
    lentestlist = len(testpathlist)
    test_label += [i for j in range(lentestlist)]
    train_files_list += trainpathlist
    test_files_list += testpathlist

for i in train_files_list:
    f = open(i, &#39;r&#39;)
    content = f.readlines()[0]
    contentlist = list(jieba.cut(content))
    contentwithspace = &quot; &quot;.join(contentlist)
    train_docments.append(contentwithspace)

for i in test_files_list:
    f = open(i, &#39;r&#39;)
    content = f.readlines()[0]
    contentlist = list(jieba.cut(content))
    contentwithspace = &#39; &#39;.join(contentlist)
    test_docments.append(contentwithspace)

stopwords = [l.strip(&#39;\n&#39;) for l in io.open(stopword_path, encoding=&#39;utf-8&#39;).readlines()]
train_tf = TfidfVectorizer(stop_words=stopwords, max_df=0.5)
train_features = train_tf.fit_transform(train_docments)
clf = MultinomialNB(alpha=0.001).fit(train_features, train_label)
test_tf = TfidfVectorizer(stop_words=stopwords, max_df=0.5, vocabulary=train_tf.vocabulary_)
test_features = test_tf.fit_transform(test_docments)
predicted_labels = clf.predict(test_features)
print(metrics.accuracy_score(test_label, predicted_labels))
运动的300.txt文件因为字符问题手动修改了一下。</div>2019-03-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/54/74/2959ff0b.jpg" width="30px"><span>飞Lisa</span> 👍（0） 💬（1）<div>我觉得老师你讲的，太好了！以前看机器学习总是看不进去，看了你的讲解真的让我提起了兴趣，一下子看了十几篇。然后请问一下老师，要继续锻炼机器学习的实战能力的话你推荐什么书或者课程或者练习项目？</div>2019-03-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/24/b0/a6e0b03a.jpg" width="30px"><span>一语中的</span> 👍（0） 💬（1）<div>前后看了3遍，我终于理解了，也可以自己敲出结果了！</div>2019-02-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/8d/7e/85a3ff2c.jpg" width="30px"><span>乔巴</span> 👍（0） 💬（1）<div>#code:utf-8
import pandas as pd
import numpy as np
import io
import os
import jieba

def preprocess(path_name):
    text_with_spaces=&quot;&quot;
    textfile=open(path_name,&quot;r&quot;).read() 
    textcut=jieba.cut(textfile)
    for word in textcut:
        text_with_spaces+=word+&quot; &quot;
    return text_with_spaces

def loadtrainset(path,classtag):
    allfiles=os.listdir(path)
    processed_textset=[]
    allclasstags=[]
    for thisfile in allfiles:
        path_name=path+&quot;&#47;&quot;+thisfile
        processed_textset.append(preprocess(path_name))
        allclasstags.append(classtag)
    return processed_textset,allclasstags

stop_words = open(&#39;D:&#47;stop&#47;stopword.txt&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;).read()
stop_words = stop_words.encode(&#39;utf-8&#39;).decode(&#39;utf-8-sig&#39;) 
stop_words = stop_words.split(&#39;\n&#39;) 

processed_textdata1,class1=loadtrainset(&quot;D:&#47;train&#47;女性&quot;, &quot;女性&quot;)
processed_textdata2,class2=loadtrainset(&quot;D:&#47;train&#47;体育&quot;, &quot;体育&quot;)
processed_textdata3,class3=loadtrainset(&quot;D:&#47;train&#47;文学&quot;, &quot;文学&quot;)
processed_textdata4,class4=loadtrainset(&quot;D:&#47;train&#47;校园&quot;, &quot;校园&quot;)
integrated_train_data=processed_textdata1+processed_textdata2+processed_textdata3+processed_textdata4
classtags_list=class1+class2+class3+class4
print(integrated_train_data[0])
tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)
train_features = tf.fit_transform(integrated_train_data)

train_labels=[0]

clf = MultinomialNB(alpha=0.01).fit(train_features, classtags_list)

test_textdata1,testClass1=loadtrainset(&quot;D:&#47;test&#47;女性&quot;, &quot;女性&quot;)
test_textdata2,testClass2=loadtrainset(&quot;D:&#47;test&#47;体育&quot;, &quot;体育&quot;)
test_textdata3,testClass3=loadtrainset(&quot;D:&#47;test&#47;文学&quot;, &quot;文学&quot;)
test_textdata4,testClass4=loadtrainset(&quot;D:&#47;test&#47;校园&quot;, &quot;校园&quot;)
integrated_test_data=test_textdata1+test_textdata2+test_textdata3+test_textdata4
classtags_list=testClass1+testClass2+testClass3+testClass4

test_tf = TfidfVectorizer( max_df=0.5)
test_features=tf.transform(integrated_test_data)
predicted_labels=clf.predict(test_features)

print(metrics.accuracy_score(classtags_list, predicted_labels))
</div>2019-02-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/03/3f/09308258.jpg" width="30px"><span>雨先生的晴天</span> 👍（0） 💬（1）<div>还是希望老师可以在github分享一下代码，练习题还是没有办法解</div>2019-02-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/50/91/0dd2b8ce.jpg" width="30px"><span>听妈妈的话</span> 👍（5） 💬（0）<div># 由于评论不支持markdown，代码放在https:&#47;&#47;pastebin.com&#47;kqjXgy0c

train_contents=[]
train_labels=[]
test_contents=[]
test_labels=[]
#  导入文件
import os
import io
start=os.listdir(r&#39;text classification&#47;train&#39;)
for item in start:
    test_path=&#39;text classification&#47;test&#47;&#39;+item+&#39;&#47;&#39;
    train_path=&#39;text classification&#47;train&#47;&#39;+item+&#39;&#47;&#39;
    for file in os.listdir(test_path):
        with open(test_path+file,encoding=&quot;GBK&quot;) as f:
            test_contents.append(f.readline())
            #print(test_contents)
            test_labels.append(item)
    for file in os.listdir(train_path):
        with open(train_path+file,encoding=&#39;gb18030&#39;, errors=&#39;ignore&#39;) as f:
            train_contents.append(f.readline())
            train_labels.append(item)
print(len(train_contents),len(test_contents))

# 导入stop word
import jieba
from sklearn import metrics
from sklearn.naive_bayes import MultinomialNB  
stop_words = [line.strip() for line in io.open(&#39;text classification&#47;stop&#47;stopword.txt&#39;).readlines()]

# 分词方式使用jieba,计算单词的权重
tf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words, max_df=0.5)
train_features = tf.fit_transform(train_contents)
print(train_features.shape)

模块 4：生成朴素贝叶斯分类器
# 多项式贝叶斯分类器
clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)

模块 5：使用生成的分类器做预测
test_tf = TfidfVectorizer(tokenizer=jieba.cut,stop_words=stop_words, max_df=0.5, vocabulary=tf.vocabulary_)
test_features=test_tf.fit_transform(test_contents)

print(test_features.shape)
predicted_labels=clf.predict(test_features)
print(metrics.accuracy_score(test_labels, predicted_labels))

# 最终结果0.925</div>2019-03-21</li><br/>
</ul>