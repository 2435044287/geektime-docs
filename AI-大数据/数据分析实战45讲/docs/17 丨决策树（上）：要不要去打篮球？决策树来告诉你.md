想象一下一个女孩的妈妈给她介绍男朋友的场景：

女儿：长的帅不帅？

妈妈：挺帅的。

女儿：有没有房子？

妈妈：在老家有一个。

女儿：收入高不高？

妈妈：还不错，年薪百万。

女儿：做什么工作的？

妈妈：IT男，互联网公司做数据挖掘的。

女儿：好，那我见见。

在现实生活中，我们会遇到各种选择，不论是选择男女朋友，还是挑选水果，都是基于以往的经验来做判断。如果把判断背后的逻辑整理成一个结构图，你会发现它实际上是一个树状图，这就是我们今天要讲的**决策树**。

## 决策树的工作原理

决策树基本上就是把我们以前的经验总结出来。我给你准备了一个打篮球的训练集。如果我们要出门打篮球，一般会根据“天气”、“温度”、“湿度”、“刮风”这几个条件来判断，最后得到结果：去打篮球？还是不去？

![](https://static001.geekbang.org/resource/image/dc/90/dca4224b342894f12f54a9cb41d8cd90.jpg?wh=1792%2A852)  
上面这个图就是一棵典型的决策树。我们在做决策树的时候，会经历两个阶段：**构造和剪枝**。

**构造**

什么是构造呢？构造就是生成一棵完整的决策树。简单来说，**构造的过程就是选择什么属性作为节点的过程**，那么在构造过程中，会存在三种节点：

1. 根节点：就是树的最顶端，最开始的那个节点。在上图中，“天气”就是一个根节点；
2. 内部节点：就是树中间的那些节点，比如说“温度”、“湿度”、“刮风”；
3. 叶节点：就是树最底部的节点，也就是决策结果。
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/13/2f/e2/3640e491.jpg" width="30px"><span>小熊猫</span> 👍（80） 💬（5）<div>决策树学习通常包括三个步骤：
1. 特征选择。选取最优特征来划分特征空间，用信息增益或者信息增益比来选择
2. 决策树的生成。ID3、C4.5、CART
3. 剪枝

总结优缺点：
ID3:
优点：算法简单，通俗易懂
缺陷：1. 无法处理缺失值
          2. 只能处理离散值，无法处理连续值
          3. 用信息增益作为划分规则，存在偏向于选择取值较多的特征。因为特征取值越多，说明划分的 
              越细，不确定性越低，信息增益则越高
          4. 容易出现过拟合

C4.5:
优点：1. 能够处理缺省值
          2. 能对连续值做离散处理
          3. 使用信息增益比，能够避免偏向于选择取值较多的特征。因为信息增益比=信息增益&#47;属性 
              熵，属性熵是根据属性的取值来计算的，一相除就会抵消掉
          4. 在构造树的过程中，会剪枝，减少过拟合
缺点：构造决策树，需要对数据进行多次扫描和排序，效率低

学习的时候发现了这两点错误：
1. Gain(D , 天气)=0 ---&gt; 1
Gain(D , 湿度)=0 ----&gt; 1
Gain(D , 刮风)=0.0615

2. 针对将属性选择为温度的信息增益率为：
Gain(D′, 温度)=Ent(D′)-0.792=1.0-0.792=-0.208
这里算出来的还是信息增益，不是信息增益率，没有除以属性熵
属性熵=-3&#47;6log3&#47;6 - 1&#47;6log1&#47;6 - 2&#47;6log2&#47;6

作业：
经验熵 H(D) = -1&#47;2log1&#47;2 - 1&#47;2log1&#47;2 = 1
属性 红的信息增益：
g(D, A1) = H(D) - H(D|A1)
             = 1 - 1&#47;2*0 - 1&#47;2 * 0
             = 1
属性 大的信息增益：
g(D,A2) = 1 - 1&#47;2*(-1&#47;2log1&#47;2-1&#47;2log1&#47;2)*2
             = 0
属性熵都是1，所以信息增益比跟信息增益一样
特征选择 红作为最优特征，红的就是好苹果，不红的就是坏苹果

</div>2019-02-14</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/wJphZ3HcvhjVUyTWCIsCugzfQY5NAy6VJ0XoPLibDlcHWMswFmFe678zd0lUjFETia80NQhyQcVnGDlKgKPcRGyw/132" width="30px"><span>JingZ</span> 👍（20） 💬（3）<div>今天去面试一个金融分析师职位

问：算法知道吗？
我答：还在学习中，但我会python 爬虫，Numpy&#47;Pandas~还有标准化(心想为嘛早上不认真看看今天的课程，起码说的出来C4.5是啥)😂😂

以后要好好做作业~及时看课程</div>2019-01-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/a3/87/c415e370.jpg" width="30px"><span>滢</span> 👍（16） 💬（1）<div>根集合D = {2个好苹果，2个不是号苹果}，求得Ent(D) =  1。

按红作为属性划分可得$D_1、D_2$两个子集

$D_1$ (红 = 是) = {2个好苹果}

$D_2$ (红= 否) = {2个不是好苹果}

可得Ent( $D_1$ ) = 0 、Ent( $D_2$ ) = 0

可得归一化信息熵为$\dfrac{1}{2} \times 0 + \dfrac{1}{2} \times 0 $ = 0

则G(D,红) = 1-0 = 1

按大作为属性，同样可得 $D_1、D_2$两个子集

$D_1$ (大 = 是) = {1个好苹果，1个不是好苹果}

$D_2$ (大 = 否) = {1个好苹果，1个不是好苹果}

可得Ent( $D_1$ ) = 1 、Ent( $D_2$ ) = 1

归一化信息熵 = $\dfrac{2}{4}\times1 + \dfrac{2}{4}\times1$ = 0.5

则G（D，大）= 1- 0.5 = 0.5

由此可得按红作为属性的信息增益大于按大作为属性的信息增益，所以选择红作为根节点。

接着在红为是的基础上，分析按大作为属性的信息增益。在红为是的集合里共有两个苹果集合D = {2个好苹果} Ent(D) = 0

$D_1$ (大 = 否) = {1个好苹果}

$D_2$ (大 = 是) = {1个好苹果} 

Ent( $D_1$ ) = 0 、Ent( $D_2$ ) = 0

G(D，大) = 0

因为大是与否在红决定的前提下对好苹果的决定没有影响，所以剪去该分支。</div>2019-04-17</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q3auHgzwzM7bXw7fCU4jTfg6QFRm1TbvENbOiccym74sPxPMSVFxSFYAdJiafonrgP1ck35RXkicgjOQ6DbicNQibcw/132" width="30px"><span>李龍</span> 👍（15） 💬（2）<div>算法一点听不懂咋整</div>2019-01-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/7f/ab/a694f8ae.jpg" width="30px"><span>aDongaDong</span> 👍（7） 💬（2）<div>脑瓜疼</div>2019-04-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/93/a7/c53882d3.jpg" width="30px"><span>黄加生</span> 👍（4） 💬（1）<div>老师你好，根据信息增益公式的构造，不应该是信息增益越小，纯度越高么？还有假设按照编号进行划分，那么他的信息增益算出来应该是最小的才对，因为子节点的归一化信息熵之和是1！您看可以解释一下不？</div>2020-12-14</li><br/><li><img src="" width="30px"><span>Geek_4b34a9</span> 👍（2） 💬（2）<div>一个小建议：对于这种信息密度极度不对等的培训材料，区别于小说类科普类的材料，我认为以同样的语速从头到尾把内容念出来，没有太大的意义，增加语音反而是起到反效果。如果该停顿的地方没有细致的超出文本材料的讲解，真的没必要加这个语音，会让人把精力分摊到非重点内容上。</div>2020-05-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/60/de/5c67895a.jpg" width="30px"><span>周飞</span> 👍（2） 💬（1）<div>1.根结点的信息熵是 -（1&#47;2*log（1&#47;2）+1&#47;2*log（1&#47;2） = 1
2.假如以红来作为根结点，那么有两个叶子 红和不红，
红的信息熵是 -（1*log（1））= 0
不红的信息熵是 -(1*log(1)) = 0
所以 以红作为根结点的信息增益是 1-0 = 1
3.假如以大来作为根结点，那么有两个叶子节点： 大和不大
大的信息熵是 -(1&#47;2*log(1&#47;2)+1&#47;2*log(1&#47;2)) = 1
不大的信息熵是 -(1&#47;2*log(1&#47;2)+1&#47;2*log(1&#47;2)) = 1
以大作为根结点的信息增益是 1- (1&#47;2*1 +1&#47;2*1) = 0;
因为 以红作为根结点的信息增益大于以 大来作为根结点的信息增益，所以选择红来作为根结点。
4.第一个叶子节点 的节点是大  ，第二个叶子节点的节点也是大。</div>2019-03-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/a6/7c/fc571405.jpg" width="30px"><span>ken</span> 👍（2） 💬（1）<div>苹果-大的信息熵：5&#47;3
苹果-红的信息熵：1
归一化的信息熵：1
苹果-大信息增益更大，作为根节点，红作为子节点。

决策树：
大（是）-红(是）-好苹果（是）
大（是）-红(否）-好苹果（否）
大（否）-红(是）-好苹果（是）
大（否）-红（否）-好苹果（否）
</div>2019-01-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/e8/ba/6d318c08.jpg" width="30px"><span>GS</span> 👍（1） 💬（1）<div>看了四五遍看不懂，拿笔和计算器算了一波，才懂了一点点。</div>2019-11-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/b5/98/ffaf2aca.jpg" width="30px"><span>Ronnyz</span> 👍（1） 💬（1）<div>① 根结点的信息熵
Ent(D)=1
② 以红属性划分
D1={1+,2+}, D2={3-,4-}
Ent(D1)=0, Ent(D2)=0
③ 信息增益
Gain(D,红) = 1
同理方法计算以大划分
Gain(D,大) = 0
因此选择红作为最优特征</div>2019-11-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg" width="30px"><span>Geek_hve78z</span> 👍（1） 💬（1）<div>1、计算案例中的下一个节点的信息增益有疑惑：
算得Gain(D , 天气)=1，与文章提到的Gain(D , 天气)=0矛盾，不知道哪里理解有误。
2、作业
苹果训练集中有4条数据，2个是好苹果，2个不是好苹果。
根结点的信息熵是：1
1）将“是为红”作为属性进行划分，有2个叶子结点D1和D2，分别对应‘是’红苹果，‘不是’红苹果。
用+代表‘是’好苹果，-代表‘不是’好苹果。
D1（红苹果-&gt;是）=【1+，2+】
D2（红苹果-&gt;否）=【3-，4-】
2个叶子节点的信息熵：
Ent（D1）=0
Ent（D2）=0
子节点归一化信息熵=0
所以，‘是红苹果’作为属性节点的信息增益为，Gain（D，是红苹果）=1
2）将‘是大苹果’作为属性进行划分，2个叶子节点D1，D2对应‘是大苹果’，‘不是大苹果’。
D1（大苹果-&gt;是）=【1+，3-】
D2（大苹果-&gt;否）=【2+，4-】
2个叶子节点的信息熵：
Ent（D1）=1
Ent（D2）=1
子节点归一化信息熵=1
所以，‘是大苹果’作为属性节点的信息增益为，Gain（D，是大苹果）=0.
综上，‘是红苹果’作为属性的信息增益最大，所以将‘是红苹果’作为根节点。
3）将D1（红苹果-&gt;是）=【1+，2+】进一步分裂，往下划分。
得到Gain（D，大苹果）=0
所以无需进一步划分。
最终决策树。
红苹果（是）-》好苹果（是）
红苹果（否）-〉好苹果（否）</div>2019-02-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/6d/88/d6e6ddcf.jpg" width="30px"><span>开心</span> 👍（1） 💬（1）<div>这节课对数学知识有点涉及，要听明白了需要自己亲自算一下，验证结果，才有真正的理解，我一早先烧脑10分钟听完，慢慢消化。</div>2019-01-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/23/9d/ea/c827690e.jpg" width="30px"><span>陈文宇abaaba</span> 👍（0） 💬（1）<div>作业

根节点的属性熵ENT(D) = -1&#47;2 * log(1&#47;2) - 1&#47;2 * log(1&#47;2) = 1

1. 按“红”属性划分：
R1 = Yes {1+, 2+}  ;  R2 = No {3-, 4-}
ENT(R1）= -(1 * log 1) = 0
ENT(R2) = -(1 * log1) = 0
子节点 ENT(D|红) = 1&#47;2 * 0 + 1&#47;2 * 0 =0
Gain(D| 红）= ENT(D) - ENT(D|红） = 1-0=1

2. 按“大小&quot;划分
S1 = 大, {1+, 3-}, S2 = 小 {2+， 4-}
ENT(S1) = -(1&#47;2* log(1&#47;2) + 1&#47;2* log(1&#47;2)) = 1
ENT(S2) = -(1&#47;2* log(1&#47;2) + 1&#47;2* log(1&#47;2)) = 1
子节点ENT(D|大小） = 1&#47;2 * 1 + 1&#47;2 * 1 = 1
Gain(D|大小）= ENT(D) - ENT(D|大小） = 1-1 =0

选取Gain值最大的属性”红“，作为根节点。”红“的为好苹果，”不红“的为坏苹果。
</div>2020-11-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/50/99/44378317.jpg" width="30px"><span>李皮皮皮皮皮</span> 👍（0） 💬（1）<div>D1(天气 = 晴天)={1-,2-,6+}这里没看懂，1-，2-和6+是嘛意思</div>2020-11-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/ad/59/8dac6577.jpg" width="30px"><span>Haoge</span> 👍（0） 💬（1）<div>老师好，有个问题，打篮球的决策树，为什么是第二层以后就到叶子节点呢？比如说温度 -- 天气，为什么就没有下一层？是因为样本数据在这两层筛选之后已经非常明确的原因？假设样本数据是7万条，而且比如“温度中+天气晴”的数据里有打篮球和不打篮球，那么是否还有往下计算的过程？公式能明白，但怎么确定已经到叶子节点，看文章还没看明白。有请老师解惑，谢谢</div>2020-10-13</li><br/><li><img src="" width="30px"><span>lemonlxn</span> 👍（0） 💬（1）<div>ID3,选择 信息增益变化最大的维度进行划分，那 C4.5选择的是 信息增益率变化最小的维度进行划分吗？</div>2020-09-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/b5/0d/df1f17b5.jpg" width="30px"><span>哎哟哟</span> 👍（0） 💬（1）<div>知道算法难，没想到这么难，照着老师的案例画一遍，照着同学的答案画一遍，还是晕。</div>2020-04-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/7a/ac/26e6b505.jpg" width="30px"><span>杨翔珲</span> 👍（0） 💬（1）<div>好精彩的决策树分析</div>2020-03-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1a/ab/57/a57266b0.jpg" width="30px"><span>Handsome</span> 👍（0） 💬（1）<div>C4.5算法的缺陷：（来自百度百科）
在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。
此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。</div>2019-12-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/18/86/3599130b.jpg" width="30px"><span>dany</span> 👍（0） 💬（2）<div>D1(天气 = 晴天)={1-,2-,6+}
D2(天气 = 阴天)={3+,7-}
D3(天气 = 小雨)={4+,5-}
这段没有明白是什么意思？ </div>2019-09-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/dc/68/006ba72c.jpg" width="30px"><span>Untitled</span> 👍（0） 💬（1）<div>作业
D={1+,2+,3−,4−}
Ent(D) = -\frac{2}{4}log_2{\frac{2}{4}} - \frac{2}{4}log_2{\frac{2}{4}} = 1
D_1{红,是}={1+,2+},Ent(D_1) = 0
D_2{红,否}={3-,4-},Ent(D_2) = 0
G(红) = 1 - 0 = 1
D_1{大,是} = {1+,3-},Ent(D_1) = 1&#47;2
D_2{大,否} = {2+,4-},Ent(D_2) = 1&#47;2
G(大) = 1 - (1&#47;2+1&#47;2) = 0
属性熵(红)={1+,2+,3−,4−}=1
属性商(大)={1+,2-,3+,4-} = 1
信息增益和信息增益率一样
G(红)比G(大)大，根节点选红属性。</div>2019-09-25</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoT9nVNcyrunC5RjsOZwLObffWPgKnsCVcjctqFPNSK6j1XHNibDPQpBVmO6jyIemnepILyTIJ7SQw/132" width="30px"><span>canownu</span> 👍（0） 💬（1）<div>老师讲的很细 我没有一点算法的基础 还是听不懂 得多学几遍</div>2019-06-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/c3/81/557f5a13.jpg" width="30px"><span>feiw</span> 👍（0） 💬（1）<div>D′的样本个数为 6，而 D 的样本个数为 7，所以所占权重...

这个不是7&#47;6吗？</div>2019-05-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/78/4e/46047725.jpg" width="30px"><span>月亮上的熊猫_lv</span> 👍（0） 💬（1）<div>Ent(D) = 1

颜色作为划分属性：
D1 (颜色=红)={1+，2+}
D2(颜色=不红)={3-，4-}
Ent(D1) = 0
Ent(D2) = 0

Gain（D，颜色）= 1 - 0 = 1


大小作为划分属性：
D1(个头=大)={1+，3-}
D2(个头=小)={2+，4-}
Ent(D1) = 1
Ent(D2) = 1

Gain（D，大小）= 1 - 1 = 0

由增益值可得，应选颜色作为根节点，且颜色已经足够完成划分，无需添加更多属性。</div>2019-02-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/cb/07/e34220d6.jpg" width="30px"><span>李沛欣</span> 👍（0） 💬（1）<div>今天的看完了。

决策树构造分为根节点，内部节点和叶节点。

对于冗余分枝需要进行剪枝处理，不然可能造成过拟合。分为预剪枝和后剪枝。

决策树和概率论有很大关系，特定情况下，某事件发生频率越高，信息熵越小，信息纯度大，反之信息熵越大，纯度越小。

常见不纯度算法有ID3，C4.5，Cart。

计算机的世界里有很多是非题，现实世界里有很多送命题。




</div>2019-01-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/3a/c8/b85eeb42.jpg" width="30px"><span>夏落若</span> 👍（0） 💬（1）<div>老师信息熵的数学公式就不知道怎么计算，数学该怎么补？</div>2019-01-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/fe/16/aac4d7f2.jpg" width="30px"><span>Geek_zclap3</span> 👍（0） 💬（1）<div>慢慢开始看不懂了！</div>2019-01-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/a7/f0/e3212f18.jpg" width="30px"><span>胡</span> 👍（0） 💬（1）<div>每次学到数学只能仰望</div>2019-01-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/f5/12/a5383fff.jpg" width="30px"><span>志</span> 👍（50） 💬（5）<div>「红」的信息增益为：1
「大」的信息增益为：0
因此选择「红」的作为根节点，「大」作为子节点。接着再通过计算得出「大」作为子节点效果更差，故进行剪枝。因此最终的完整决策树就只有「红」一个节点：
红（是）---好苹果（是）
红（否）---好苹果（否）

通过使用sklearn来验证一下：
from sklearn import tree
import sys
import os  
import graphviz
import numpy as np
os.environ[&quot;PATH&quot;] += os.pathsep + &#39;D:&#47;Program Files&#47;Anaconda3&#47;Library&#47;bin&#47;graphviz&#39;

#创建数据[红，大]，1==是，0==否
data = np.array([[1,1],[1,0],[0,1],[0,0]])
#数据标注为，1==好苹果，0==坏苹果
target = np.array([1,1,0,0])

clf = tree.DecisionTreeClassifier()  #创建决策树分类器模型
clf = clf.fit(data, target)        #拟合数据

#最后利用graphviz库打印出决策树图
dot_data = tree.export_graphviz(clf,out_file=None) 
graph = graphviz.Source(dot_data) 
graph</div>2019-01-22</li><br/>
</ul>