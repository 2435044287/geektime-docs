今天我来带你学习EM聚类。EM的英文是Expectation Maximization，所以EM算法也叫最大期望算法。

我们先看一个简单的场景：假设你炒了一份菜，想要把它平均分到两个碟子里，该怎么分？

很少有人用称对菜进行称重，再计算一半的分量进行平分。大部分人的方法是先分一部分到碟子A中，然后再把剩余的分到碟子B中，再来观察碟子A和B里的菜是否一样多，哪个多就匀一些到少的那个碟子里，然后再观察碟子A和B里的是否一样多……整个过程一直重复下去，直到份量不发生变化为止。

你能从这个例子中看到三个主要的步骤：初始化参数、观察预期、重新估计。首先是先给每个碟子初始化一些菜量，然后再观察预期，这两个步骤实际上就是期望步骤（Expectation）。如果结果存在偏差就需要重新估计参数，这个就是最大化步骤（Maximization）。这两个步骤加起来也就是EM算法的过程。

![](https://static001.geekbang.org/resource/image/91/3c/91f617ac484a7de011108ae99bd8cb3c.jpg?wh=1842%2A711)

## EM算法的工作原理

说到EM算法，我们先来看一个概念“最大似然”，英文是Maximum Likelihood，Likelihood代表可能性，所以最大似然也就是最大可能性的意思。

什么是最大似然呢？举个例子，有一男一女两个同学，现在要对他俩进行身高的比较，谁会更高呢？根据我们的经验，相同年龄下男性的平均身高比女性的高一些，所以男同学高的可能性会很大。这里运用的就是最大似然的概念。
<div><strong>精选留言（28）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/a4/5a/e708e423.jpg" width="30px"><span>third</span> 👍（52） 💬（1）<div>想起了一个故事，摘叶子
要找到最大的叶子
1.先心里大概有一个叶子大小的概念（初始化模型）
2.在三分之一的的路程上，观察叶子大小，并修改对大小的评估（观察预期，并修改参数）
3.在三分之二的路程上，验证自己对叶子大小模型的的评估（重复1,2过程）
4.在最后的路程上，选择最大的叶子（重复1.2，直到参数不再改变）

相同点
1.EM，KMEANS，都是随机生成预期值，然后经过反复调整，获得最佳结果
2.聚类个数清晰

不同点
1.EM是计算概率，KMeans是计算距离。
计算概率，概率只要不为0，都有可能即样本是每一个类别都有可能
计算距离，只有近的的票高，才有可能，即样本只能属于一个类别</div>2019-02-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/79/9a/4f907ad6.jpg" width="30px"><span>Python</span> 👍（17） 💬（1）<div>em聚类和K均值的区别就是一个软一个硬，软的输出概率，硬的要给出答案。我理解的em聚类的过程是一个翻来覆去决策的过程，这种聚类方式是先确定一个初始化的参数，再反过来推算结果，看和自己期望的差距，又在翻回去调整。好就好在，你想要一个什么样的结果他都能慢慢给你调整出来</div>2019-02-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/77/be/1f2409e8.jpg" width="30px"><span>梁林松</span> 👍（14） 💬（1）<div>EM 就好像炒菜，做汤，盐多了放水，味淡了再放盐，直到合适为止。然后，就能得出放盐和水的比例（参数）</div>2019-02-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg" width="30px"><span>mickey</span> 👍（11） 💬（2）<div>文中抛硬币的例子，应该还要说明“5组实验，每组实验投掷10次，每组中只能抛同一枚硬币”。</div>2019-02-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/42/76/256bbd43.jpg" width="30px"><span>松花皮蛋me</span> 👍（11） 💬（1）<div>有同学说:核心是初始参数啊。如果一开始就错那就完了。这完全是错的，只不过增加了更新次数而已。</div>2019-02-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/aa/d1/076482f3.jpg" width="30px"><span>白夜</span> 👍（7） 💬（1）<div>EM，聚类的个数是已知的，首先，预设初始化的参数，然后获得对应的结果，再通过结果计算参数，不断循环以上两步，直到收敛。属于软分类，每个样本有一定概率和一个聚类相关。
K-Means，聚类的个数也是已知的，首先选定一个中心点，然后计算距离，获得新的中心点，重复，直到结果收敛。属于硬分类，每个样本都只有一个分类。</div>2019-02-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg" width="30px"><span>JustDoDT</span> 👍（3） 💬（1）<div>EM算法法第一次接触，要多看两遍。</div>2020-04-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1f/17/01/1c5309a3.jpg" width="30px"><span>McKee Chen</span> 👍（0） 💬（1）<div>EM聚类：
对参数进行初始化，以此估计隐含变量，然后再反推初始参数，如果参数有变化，就不断重复上述过程，知道参数不变为止，此时也能得到隐含变量，即样本的聚类情况。

EM聚类和K-Means聚类的区别：
K-Means聚类是预先给定中心点个数，然后计算样本与中心点之间的距离来进行分类，通过不断迭代优化中心点，直至中心点不再发生变换，最后确定聚类情况。这种过程也称之为硬聚类算法。

有一个疑问，对于掷硬币的五次实验，每次实验都是只选择A或B其中一个么？

</div>2021-01-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/10/57/1adfd4f7.jpg" width="30px"><span>追梦</span> 👍（0） 💬（1）<div>想起了一个故事，摘叶子
要找到最大的叶子
1.先心里大概有一个叶子大小的概念（初始化模型）
2.在三分之一的的路程上，观察叶子大小，并修改对大小的评估（观察预期，并修改参数）
3.在三分之二的路程上，验证自己对叶子大小模型的的评估（重复1,2过程）
4.在最后的路程上，选择最大的叶子（重复1.2，直到参数不再改变）

相同点
1.EM，KMEANS，都是随机生成预期值，然后经过反复调整，获得最佳结果
2.聚类个数清晰

不同点
1.EM是计算概率，KMeans是计算距离。
计算概率，概率只要不为0，都有可能即样本是每一个类别都有可能
计算距离，只有近的的票高，才有可能，即样本只能属于一个类别


“”通过猜测的结果{A, A, B, B, A}来完善初始化的θA 和θB“” 这个步骤是怎样的？

A 5
A 7
B 8
B 9
A 4
θA=(5+7+4)&#47;(10+10+10)
θB=(8+9)&#47;(10+10)

以留言方式暂时记录一下</div>2019-10-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/f4/52/10c4d863.jpg" width="30px"><span>FeiFei</span> 👍（0） 💬（1）<div>EM聚类算法，通过假定参数值，来推断未知隐含变量。再不断重复这个过程，至到隐含变量恒定不变时，得出假定参数的值。也就是实际的聚类分类的结果。
K-Means：非黑即白
EM：黑白通吃</div>2019-08-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/d3/fd/41eb3ecc.jpg" width="30px"><span>奔跑的徐胖子</span> 👍（0） 💬（1）<div>原理的话就拿老师的这个抛掷硬币的例子来看：
1、初始的时候，我们并不知道1~5次试验抛掷的分别是A硬币还是B硬币，我们就先假设一下A、B正面向上的概率。
2、通过我们假设的概率，我们根据1~5次实验中每次正面向上的频率，使用我们1中假设的A、B正面的概率来分别计算期望值。两个期望值比较哪个大，我们就觉得这次试验抛掷的是哪个硬币。
3、我们通过2，就第一次将本来没有分类的试验（该次实验抛掷的是哪一个硬币）给分类了，但是这个结果是我们初始化一个随机的正面向上的概率来算出来的，不准确。
4、我们把1、2、3的出来的初始的分类结果当做已知，通过全体数据来算一下此时A、B正面向上的概率（全体数据的频率），这样，我们就得到了类似2步骤中的正面向上的概率，这里就优化了A、B这面向上的概率（完善参数）。
5、就这样一直重复2、3的过程，直到稳定为止</div>2019-04-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/d3/fd/41eb3ecc.jpg" width="30px"><span>奔跑的徐胖子</span> 👍（0） 💬（1）<div>EM的原理，其实就拿这个老师给的硬币的例子来看。初始的时候，我们只有一堆数据，并不知道试验1~5分别抛掷的是哪一个硬币。这样，我们先随机一下A、B两枚硬币的正面出现的概率。</div>2019-04-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/cb/07/e34220d6.jpg" width="30px"><span>李沛欣</span> 👍（0） 💬（1）<div>今天的看完了。我理解的EM算法，是先估计一个大概率的可能参数，然后再根据数据不断进行调整，直到找到最终的确认参数。

它主要有高斯模型和隐马尔科夫模型，前者在自然语言处理领域有很多应用。

它和K-means都属于聚类算法，但是，EM属于软聚类，同一样本可能属于多个类别；而后者则属于硬聚类，一个样本只能属于一个类别。所以前者能够发现一些隐藏的数据。</div>2019-02-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/f9/81/54b1a5a8.jpg" width="30px"><span>littlePerfect</span> 👍（0） 💬（1）<div>陈老师什么时候会更新面试的内容？</div>2019-02-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/ab/5d/430ed3b6.jpg" width="30px"><span>从未在此</span> 👍（0） 💬（2）<div>核心是初始参数啊。如果一开始就错那就完了</div>2019-02-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/ae/ef/cbb8d881.jpg" width="30px"><span>黄智荣</span> 👍（21） 💬（0）<div>“”通过猜测的结果{A, A, B, B, A}来完善初始化的θA 和θB“” 这个步骤是怎样的？

A  5
A  7
B  8
B  9
A  4
θA=(5+7+4)&#47;(10+10+10)
θB=(8+9)&#47;(10+10)</div>2019-02-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg" width="30px"><span>mickey</span> 👍（8） 💬（0）<div>to third：

吴军老师说过，这种找最大叶子的问题，最优解最大概率会在37%的时候，而不是最后。</div>2019-02-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/b8/21/c03839f1.jpg" width="30px"><span>FORWARD―MOUNT</span> 👍（7） 💬（3）<div>请问：

通过猜测的结果{A, A, B, B, A}来完善初始化的参数θA 和θB。
然后一直重复第二步和第三步，直到参数不再发生变化。


怎么完善初始化参数？，急需解答。</div>2019-03-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/5d/27/74e152d3.jpg" width="30px"><span>滨滨</span> 👍（5） 💬（0）<div>em算法是假定一个样本分布概率，然后根据最大似然估计进行聚类，然后根据聚类结果修正参数，直到结果不在变化，而kmeans算法则是根据随机确定初始点，根据欧式距离等算法来计算和初始点的距离，完成初始聚类，然后迭代直到聚类结果不发生变化。kmeans是计算硬聚类，em是软聚类。</div>2019-04-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/5d/27/74e152d3.jpg" width="30px"><span>滨滨</span> 👍（4） 💬（1）<div>说的通俗一点啊，最大似然估计，就是利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。
例如：一个麻袋里有白球与黑球，但是我不知道它们之间的比例，那我就有放回的抽取10次，结果我发现我抽到了8次黑球2次白球，我要求最有可能的黑白球之间的比例时，就采取最大似然估计法： 我假设我抽到黑球的概率为p,那得出8次黑球2次白球这个结果的概率为：
P(黑=8)=p^8*（1-p）^2,现在我想要得出p是多少啊，很简单，使得P(黑=8)最大的p就是我要求的结果，接下来求导的的过程就是求极值的过程啦。
可能你会有疑问，为什么要ln一下呢，这是因为ln把乘法变成加法了，且不会改变极值的位置（单调性保持一致嘛）这样求导会方便很多~</div>2019-04-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1f/d9/2c/296b02b0.jpg" width="30px"><span>烟雨平生</span> 👍（1） 💬（0）<div>老师您好，请问有什么书对于数学原理讲的比较透彻的，个人人为机器学习算法就难在数学，像西瓜书数学推导少，为什么用这样的统计学方法也没有解释，求推荐，可以英文</div>2021-12-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/e4/f5/e722d833.jpg" width="30px"><span>对三要不起</span> 👍（1） 💬（0）<div>TO FORWARD―MOUNT
【通过猜测的结果{A, A, B, B, A}来完善初始化的参数θA 和θB。
然后一直重复第二步和第三步，直到参数不再发生变化。】

这个步骤就是通过第一次随机，我们一直知道了顺序了可能是{A A B B A}，然后就可以算出A和B投正面的概率，再通过算出来的这个新概率（之前是随即指定的），再去模拟一遍五组硬币，可能这次模拟出来的就不是{A A B B A}了，重复这个步骤直到模拟出来的五枚硬币不再改变。此时的概率就是A和B 投正面的概率。
</div>2019-05-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/6a/78/7ab6f411.jpg" width="30px"><span>老师 冯</span> 👍（1） 💬（0）<div>“”通过猜测的结果{A, A, B, B, A}来完善初始化的θA 和θB“”  这个步骤是怎样的？跪求解答



</div>2019-02-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/59/37/8028950c.jpg" width="30px"><span>明亮</span> 👍（0） 💬（0）<div>不讲清楚具体实验规则，要读者猜，真是服了</div>2022-12-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/21/d9/84/f1b10393.jpg" width="30px"><span>进击的矮子</span> 👍（0） 💬（0）<div>给2个K-means失效的例子。
第一个：交错半圆
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.cluster import KMeans
from sklearn import datasets

x,y=datasets.make_moons(n_samples=500, shuffle=True, noise=None, random_state=None)
km=KMeans(n_clusters=2,
    init=&#39;k-means++&#39;,
    n_init=10,
    max_iter=300,
    tol=0.0001,
    precompute_distances=&#39;deprecated&#39;,
    verbose=0,
    random_state=None,
    copy_x=True,
    n_jobs=&#39;deprecated&#39;,
    algorithm=&#39;auto&#39;,)

label=km.fit_predict(x)

fig=plt.figure()
axes = fig.add_subplot(121)
for idx,lab in enumerate(y):
    if lab==0:
        axes.scatter(x[idx][0],x[idx][1],color=&#39;r&#39;)
    else:
        axes.scatter(x[idx][0],x[idx][1],color=&#39;g&#39;)
ax2=fig.add_subplot(122)
for idx,lab in enumerate(label):
    if lab==0:
        ax2.scatter(x[idx][0],x[idx][1],color=&#39;r&#39;)
    else:
        ax2.scatter(x[idx][0],x[idx][1],color=&#39;g&#39;)
plt.show()
第二个：大小圆
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.cluster import KMeans
from sklearn import datasets

x,y=datasets.make_circles(n_samples=200, shuffle=True, noise=None, random_state=None,
                 factor=.8)
km=KMeans(n_clusters=2,
    init=&#39;k-means++&#39;,
    n_init=10,
    max_iter=300,
    tol=0.0001,
    precompute_distances=&#39;deprecated&#39;,
    verbose=0,
    random_state=None,
    copy_x=True,
    n_jobs=&#39;deprecated&#39;,
    algorithm=&#39;auto&#39;,)

label=km.fit_predict(x)

fig=plt.figure()
axes = fig.add_subplot(121)
for idx,lab in enumerate(y):
    if lab==0:
        axes.scatter(x[idx][0],x[idx][1],color=&#39;r&#39;)
    else:
        axes.scatter(x[idx][0],x[idx][1],color=&#39;g&#39;)
ax2=fig.add_subplot(122)
for idx,lab in enumerate(label):
    if lab==0:
        ax2.scatter(x[idx][0],x[idx][1],color=&#39;r&#39;)
    else:
        ax2.scatter(x[idx][0],x[idx][1],color=&#39;g&#39;)
plt.show()</div>2021-06-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/4a/35/66caeed9.jpg" width="30px"><span>完美坚持</span> 👍（0） 💬（0）<div>EM聚类，首先给每个样本点人为设定一个类别，设定好之后就可以计算每个类别的样本点分布的特征参数（E-step）。根据得到的特诊参数决定的分布，可以将样本点重新归类（相当于重新设定参数，M-step），至此又可以根据新的类别划分重复上述步骤，直到每个样本点的类别划分不再有大的变化</div>2021-04-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/7d/05/4bad0c7c.jpg" width="30px"><span>Geek_hve78z</span> 👍（0） 💬（0）<div>1、 EM 算法的原理？
当我们需要从样本观察数据中，找出样本的模型参数。 但是问题含有未观察到的隐含数据，这时采用EM算法。
在EM算法的Expectation步，先猜想隐含数据，接着基于观察数据和猜测的隐含数据一起来极大化对数似然，求解我们的模型参数。（EM算法的Maximization步)。
我们基于当前得到的模型参数，继续猜测隐含数据（EM算法的E步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。
2、EM 聚类和 K-Means 聚类的相同和不同之处又有哪些？
k-means 计算过程：
1）随机选择k个类簇的中心
2）计算每一个样本点到所有类簇中心的距离，选择最小距离作为该样本的类簇
3）重新计算所有类簇的中心坐标，直到达到某种停止条件（迭代次数&#47;簇中心收敛&#47;最小平方误差）</div>2019-02-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/26/53/60fe31fb.jpg" width="30px"><span>深白浅黑</span> 👍（0） 💬（0）<div>原理哪里都有，还是需要结合实战！
个人觉得，如果从数学定义角度出发，会更容易对算法原理进行理解。
EM算法是求解隐含参数的算法，依据算法推导过程，可以视为求局部最优解的方法，可以归属为求解凸函数的问题。
https:&#47;&#47;www.cnblogs.com&#47;bigmoyan&#47;p&#47;4550375.html
</div>2019-02-19</li><br/>
</ul>