在前面的两篇文章中，我给你讲了决策树算法。决策树算法是经常使用的数据挖掘算法，这是因为决策树就像一个人脑中的决策模型一样，呈现出来非常直观。基于决策树还诞生了很多数据挖掘算法，比如随机森林（Random forest）。

今天我来带你用决策树进行项目的实战。

决策树分类的应用场景非常广泛，在各行各业都有应用，比如在金融行业可以用决策树做贷款风险评估，医疗行业可以用决策树生成辅助诊断，电商行业可以用决策树对销售额进行预测等。

在了解决策树的原理后，今天我们用sklearn工具解决一个实际的问题：泰坦尼克号乘客的生存预测。

## sklearn中的决策树模型

首先，我们需要掌握sklearn中自带的决策树分类器DecisionTreeClassifier，方法如下：

```
clf = DecisionTreeClassifier(criterion='entropy')
```

到目前为止，sklearn中只实现了ID3与CART决策树，所以我们暂时只能使用这两种决策树，在构造DecisionTreeClassifier类时，其中有一个参数是criterion，意为标准。它决定了构造的分类树是采用ID3分类树，还是CART分类树，对应的取值分别是entropy或者gini：

- entropy: 基于信息熵，也就是ID3算法，实际结果与C4.5相差不大；
- gini：默认参数，基于基尼系数。CART算法是基于基尼系数做属性划分的，所以criterion=gini时，实际上执行的是CART算法。
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/18/43/dc/4f70c936.jpg" width="30px"><span>一只眼看世界</span> 👍（50） 💬（1）<div>还有个问题 决策树怎么读？  就是每个方框内的数据都代表什么意思呢？</div>2019-07-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/2f/e2/3640e491.jpg" width="30px"><span>小熊猫</span> 👍（29） 💬（2）<div>fit 从一个训练集中学习模型参数，其中就包括了归一化时用到的均值，标准偏差等，可以理解为一个训练过程。
transform: 在fit的基础上，对数据进行标准化，降维，归一化等数据转换操作
fit_transform: 将模型训练和转化合并到一起，训练样本先做fit，得到mean，standard deviation，然后将这些参数用于transform（归一化训练数据），使得到的训练数据是归一化的，而测试数据只需要在原先fit得到的mean，std上来做归一化就行了，所以用transform就行了。</div>2019-02-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/54/9a/76c0af70.jpg" width="30px"><span>每天晒白牙</span> 👍（26） 💬（4）<div># 依赖包从 cmd中 pip install即可
import pandas as pd
import numpy as np
from sklearn.feature_extraction import DictVectorizer
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn import tree
import graphviz
# 数据加载
train_data = pd.read_csv(&#39;D:&#47;workspace&#47;study&#47;python&#47;Titanic_Data&#47;train.csv&#39;)
test_data = pd.read_csv(&#39;D:&#47;workspace&#47;study&#47;python&#47;Titanic_Data&#47;test.csv&#39;)
# 数据探索
print(train_data.info())
print(&#39;-&#39;*30)
print(train_data.describe())
print(&#39;-&#39;*30)
print(train_data.describe(include=[&#39;O&#39;]))
print(&#39;-&#39;*30)
print(train_data.head())
print(&#39;-&#39;*30)
print(train_data.tail())
# 数据清洗
# 使用平均年龄来填充年龄中的 nan 值
train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(), inplace=True)
test_data[&#39;Age&#39;].fillna(test_data[&#39;Age&#39;].mean(), inplace=True)
# 使用票价的均值填充票价中的 nan 值
train_data[&#39;Fare&#39;].fillna(train_data[&#39;Fare&#39;].mean(), inplace=True)
test_data[&#39;Fare&#39;].fillna(test_data[&#39;Fare&#39;].mean(), inplace=True)
# 使用登录最多的港口来填充登录港口的 nan 值
train_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)
test_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)
# 特征选择
features = [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]
train_features = train_data[features]
train_labels = train_data[&#39;Survived&#39;]
test_features = test_data[features]
dvec = DictVectorizer(sparse=False)
train_features = dvec.fit_transform(train_features.to_dict(orient=&#39;record&#39;))
print(dvec.feature_names_)
# 决策树模型
# 构造 ID3 决策树
clf = DecisionTreeClassifier(criterion=&#39;entropy&#39;)
# 决策树训练
clf.fit(train_features, train_labels)
# 模型预测 &amp; 评估
test_features=dvec.transform(test_features.to_dict(orient=&#39;record&#39;))
# 决策树预测
pred_labels = clf.predict(test_features)
# 决策树准确率
acc_decision_tree = round(clf.score(train_features, train_labels), 6)
print(u&#39;score 准确率为 %.4lf&#39; % acc_decision_tree)
#  K 折交叉验证统计决策树准确率
print(u&#39;cross_val_score 准确率为 %.4lf&#39; % np.mean(cross_val_score(clf, train_features, train_labels, cv=10)))
# 决策树可视化
dot_data = tree.export_graphviz(clf, out_file=None)
graph = graphviz.Source(dot_data)
graph.view()</div>2019-01-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/9d/93/945393c1.jpg" width="30px"><span>不做键盘侠</span> 👍（17） 💬（4）<div>Fare似乎没有缺失值？</div>2019-02-05</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJnryDfaYkQh7mTgVIQqk4vdeuY2oa82w0yxOVNfsz4qfSMqEqPFE9gKFvlhS53xY6YOrib86Z6vPA/132" width="30px"><span>Lambert</span> 👍（9） 💬（2）<div># 决策树可视化
from sklearn import tree
import graphviz
dot_data = tree.export_graphviz(clf, out_file=None)
graph = graphviz.Source(dot_data)
graph.render(&quot;tree&quot;)
graph.view(&#39;graph&#39;)</div>2019-02-27</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTI45zO9GOMquk9JymTibN9sC25Sy4WtsDGRQzIRVIoIzPnaJGKmGe3jXqxP0zKZyTYazrXHBGYjBzw/132" width="30px"><span>柚子</span> 👍（8） 💬（2）<div>关于graphviz：我用的是anaconda，通过在anaconda prompt界面输入 conda install python-graphviz 可以直接安装graphviz</div>2019-02-19</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJAeZ2VCia2y3bW9N7EMfgBqX8WClXUydwaXDPcK7Bm3XaMnMKx7q5ffA0UuTeJmEusxtQAibf8djCA/132" width="30px"><span>上官</span> 👍（8） 💬（1）<div>Carbin缺失率分别为 77% 和 78%， Age\Fare有缺失值，这都是在哪儿判断出来的？
</div>2019-01-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/50/91/0dd2b8ce.jpg" width="30px"><span>听妈妈的话</span> 👍（7） 💬（3）<div>https:&#47;&#47;github.com&#47;apachecn&#47;kaggle&#47;tree&#47;master&#47;competitions&#47;getting-started&#47;titanic
我个人认为这里的预测方案写的更加详细一点，大家可以参考一下</div>2019-03-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/e8/d7/9209f511.jpg" width="30px"><span>hh</span> 👍（7） 💬（1）<div>老师的课太值了，请问老师还有其他课吗，真是干货满满</div>2019-02-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/51/86/b5fd8dd8.jpg" width="30px"><span>建强</span> 👍（5） 💬（1）<div>      transform()和fit_transform()二者的功能都是对数据进行某种统一处理（比如标准化~N(0,1)，将数据缩放(映射)到某个固定区间，归一化，正则化等）。
      fit_transform(trainData)对部分训练数据先拟合fit，找到部分训练数据的整体指标，如均值、方差、最大值最小值等等（根据具体转换的目的），然后对训练数据进行转换transform，从而实现数据的标准化、归一化等等。
       根据对之前部分训练数据进行fit的整体指标，对测试数据集使用同样的均值、方差、最大、最小值等指标进行转换transform(testData)，从而保证train、test处理方式相同。
</div>2019-09-10</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqyhBcYwcW0049zCvImcuiaKoKovSZUpoAbzCjiaEBc2vuuUWtuBDjt63sg37mSMNUVVndpGKBCcLpQ/132" width="30px"><span>笔落惊风雨</span> 👍（5） 💬（1）<div>我表示真的没看明白 来回来看5遍了</div>2019-02-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/0c/0f/93d1c8eb.jpg" width="30px"><span>mickey</span> 👍（3） 💬（1）<div># encoding=utf-8
import pandas as pd
from sklearn.feature_extraction import DictVectorizer
from sklearn.tree import DecisionTreeClassifier
import numpy as np
from sklearn.model_selection import cross_val_score

# 数据加载
train_data = pd.read_csv(&#39;.&#47;Titanic_Data&#47;train.csv&#39;)
test_data = pd.read_csv(&#39;.&#47;Titanic_Data&#47;test.csv&#39;)

# 数据探索
print(train_data.info())
print(train_data.describe())
print(train_data.describe(include=[&#39;O&#39;]))
print(train_data.head())
print(train_data.tail())

# 数据清洗
# 使用平均年龄来填充年龄中的Nan值
train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(), inplace=True)
test_data[&#39;Age&#39;].fillna(test_data[&#39;Age&#39;].mean(),inplace=True)
# 使用票价的均值填充票价中的Nan值
train_data[&#39;Fare&#39;].fillna(train_data[&#39;Fare&#39;].mean(), inplace=True)
test_data[&#39;Fare&#39;].fillna(test_data[&#39;Fare&#39;].mean(),inplace=True)

# 使用登录最多的港口来填充登录港口的nan值
# print(train_data[&#39;Embarked&#39;].value_counts())
train_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)
test_data[&#39;Embarked&#39;].fillna(&#39;S&#39;,inplace=True)

# 特征选择
features = [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]
train_features = train_data[features]
train_labels = train_data[&#39;Survived&#39;]
test_features = test_data[features]

dvec = DictVectorizer(sparse=False)
train_features = dvec.fit_transform(train_features.to_dict(orient=&#39;record&#39;))

# 构造ID3决策树
clf = DecisionTreeClassifier(criterion=&#39;entropy&#39;)

# 决策树训练
clf.fit(train_features, train_labels)

# 得到决策树准确率
acc_decision_tree = round(clf.score(train_features, train_labels), 6)
print(u&#39;score准确率为 %.4lf&#39; % acc_decision_tree)

# 使用K折交叉验证 统计决策树准确率
print(u&#39;cross_val_score准确率为 %.4lf&#39; % np.mean(cross_val_score(clf, train_features, train_labels, cv=10)))</div>2019-01-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/79/9a/4f907ad6.jpg" width="30px"><span>Python</span> 👍（3） 💬（1）<div>这两个函数最后得出的结果完全一样，但实际上用法有所不同。如果一定要两个一起用，那肯定是得先
fit_transforms,再transforms，不然就会报错。fit_transforms实际上是fit()和transforms（）这两个函数的集合</div>2019-01-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/ca/83/684361be.jpg" width="30px"><span>永降不息之雨</span> 👍（2） 💬（1）<div>自己写了一遍代码，说怎么一直报错，原来是测试集的fare有缺失啊！
</div>2019-06-28</li><br/><li><img src="" width="30px"><span>0256</span> 👍（1） 💬（1）<div>为什么最后算准确率的时候，是用的train，而不是test呢？</div>2020-09-12</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJYEdMwBDUC6gYrUoI7092ocWJPyw1aP8xNOFXxOv7LEw1xj5a4icDibV7pd9vN45lXicXYjB7oYXVqg/132" width="30px"><span>羊小看</span> 👍（1） 💬（1）<div>老师，你好！想问下，做分类时，样本的正负类别的数量对比，有限制吗？正类比较少，占比只有5%，可以吗？会出现什么问题吗？谢谢！</div>2019-05-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/1f/71/ad9de027.jpg" width="30px"><span>pythonzwd</span> 👍（1） 💬（1）<div>咨询一下，就是那个是否生存的结果如何生成出来</div>2019-03-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/56/65/22a37a8e.jpg" width="30px"><span>Yezhiwei</span> 👍（1） 💬（1）<div>是否可以将文章中的训练集分成训练集和测试集进行模型训练和评估，文章中的测试集模拟未知的真实数据？</div>2019-01-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1f/17/01/1c5309a3.jpg" width="30px"><span>McKee Chen</span> 👍（0） 💬（1）<div>这节课听完一遍之后，对整个解决决策树预测问题的流程有了清晰、明确的了解与认知，老师讲解的特别好，让我一下子就听明白了。准备开始上手写这节课的代码，好记性不如烂笔头。</div>2020-11-27</li><br/><li><img src="" width="30px"><span>freedomwl</span> 👍（0） 💬（1）<div>老师，k折交叉验证除了告诉我们实际的准确率，用这种方法训练模型是否能提高模型实际精度？</div>2020-07-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/eb/2f/baad4b9d.jpg" width="30px"><span>wei</span> 👍（0） 💬（1）<div>fit_transform是先fit再transform还是先transform再fit？我的理解是先transform，因为transform后的data才是用来fit的输入，这样理解对么？</div>2020-06-29</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/ziaN7rOONp15HJm6A9JoAYicJL8VA59x10DX4JZyvcfqmmpCnumXgAkNn37aFoALftyTaQNlUF7te54LibvVm20TQ/132" width="30px"><span>Geek_c9fa4e</span> 👍（0） 💬（1）<div>fit_transform是输入加转换，而transform表示转换。</div>2020-04-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/a0/b7/1327ae60.jpg" width="30px"><span>hellojd_gk</span> 👍（0） 💬（1）<div>结合老师的讲解，配合老师代码，再加上谷歌搜索。 对我python小白，看明白了。</div>2020-04-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/a0/b7/1327ae60.jpg" width="30px"><span>hellojd_gk</span> 👍（0） 💬（1）<div>通过数据探索，我们发现 Age、Fare 和 Cabin 这三个字段的数据有所缺失。 我看到结果是：Embarked，Cabin，Age这三个字段有所缺失。 不知道是我理解有问题，还是老师的内容有问题。求确认。</div>2020-04-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/e6/08/87a6f32b.jpg" width="30px"><span>qdnjqfch</span> 👍（0） 💬（1）<div>实际工作中以K 折交叉验证准确率为准吧？准确率一般多高才能算模型效果不错。</div>2020-03-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/e8/ba/6d318c08.jpg" width="30px"><span>GS</span> 👍（0） 💬（1）<div>先mark下，没太看懂。谢谢大家的留言，在留言吸取很多前人经验。
照着搬一遍先
https:&#47;&#47;github.com&#47;leledada&#47;jupyter&#47;blob&#47;master&#47;titanic_analysis.ipynb</div>2019-12-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/4d/62/0fe9cbb3.jpg" width="30px"><span>William～Zhang</span> 👍（0） 💬（1）<div>carbin的值无法补齐，那应该对这个属性如何处理？</div>2019-11-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/ad/e9/5c51e32c.jpg" width="30px"><span>eleven</span> 👍（0） 💬（1）<div>https:&#47;&#47;github.com&#47;cystanford&#47;Titanic_Data   无法访问
给的两个链接都打不开。</div>2019-11-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/43/dc/4f70c936.jpg" width="30px"><span>一只眼看世界</span> 👍（0） 💬（1）<div>老师， 训练出的模型要怎么使用呢？ 我理解的训练出的模型， 当我给它传一行数据 是不是就能判断出能活不能活了？ 要怎么传呢？ </div>2019-07-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/8e/76/6d55e26f.jpg" width="30px"><span>张晓辉</span> 👍（0） 💬（1）<div>fit_transform: 其实做了两件事情。一是学习所有的特征名字。二是把特征名字和特征值的Map关系转化为向量。
transform：只是把特征名字和特征值的Map关系转化为向量。

一般情况下，在调用transform之前需要调用fit 函数。否则，会有异常抛出。

#encoding:UTF-8
import pandas as pd 
import numpy as np 
from sklearn.model_selection import cross_val_score
from sklearn.feature_extraction import DictVectorizer
from sklearn.tree import DecisionTreeClassifier
import graphviz
from sklearn import tree

#Data Explore
train_data = pd.read_csv(&#39;train.csv&#39;)
test_data = pd.read_csv(&#39;test.csv&#39;)
print(train_data.info())

#Data clean
train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(), inplace=True)
test_data[&#39;Age&#39;].fillna(test_data[&#39;Age&#39;].mean(), inplace=True)
train_data[&#39;Fare&#39;].fillna(train_data[&#39;Fare&#39;].mean(), inplace=True)
test_data[&#39;Fare&#39;].fillna(test_data[&#39;Fare&#39;].mean(), inplace=True)
print(train_data[&#39;Embarked&#39;].value_counts())
train_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)
test_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)

#Feature Selection
features = [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]
train_features = train_data[features]
train_labels = train_data[&#39;Survived&#39;]
test_features = test_data[features]

dvec = DictVectorizer(sparse=False)
train_features = dvec.fit_transform(train_features.to_dict(orient=&#39;record&#39;))
test_features = dvec.transform(test_features.to_dict(orient=&#39;record&#39;))

#Construct and train ID3 clf
clf = DecisionTreeClassifier(criterion=&#39;entropy&#39;)
clf.fit(train_features, train_labels)

#Predict
pred_labels = clf.predict(test_features)

#Evaluate
acc_decision_tree = round(clf.score(train_features, train_labels), 6)
print(&#39;Score accuracy %.4lf&#39; % acc_decision_tree)
print(&#39;cross_val_score accuracy %.4lf&#39; % np.mean(cross_val_score(clf, train_features, train_labels, cv=10)))

#Visualize the tree
dot_data = tree.export_graphviz(clf, out_file=None)
graph = graphviz.Source(dot_data)
graph</div>2019-05-11</li><br/>
</ul>