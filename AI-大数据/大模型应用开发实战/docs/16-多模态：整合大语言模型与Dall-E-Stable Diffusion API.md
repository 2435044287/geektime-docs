你好，我是黄佳。从今天开始，我们进入一个新的应用开发领域——多模态开发实战。

AI 时代基本天天有惊喜，通常是小惊喜，偶尔有大惊喜。2024年5月，OpenAI的又一款语言模型让人眼前一亮，连连惊叹，这就是GPT-4o。

![图片](https://static001.geekbang.org/resource/image/e0/ac/e06a3eb9df41da28d9bae229b011a5ac.png?wh=2014x1125)

## GPT-4o 和多模态

OpenAI 官宣：GPT-4o（“o”代表“o​​mni”）是朝着更自然的人机交互迈出的一步——它接受文本、音频、图像和视频的任意组合作为输入，并生成文本、音频和图像的任意组合输出。它可以在最短 232 毫秒内响应音频输入，平均为 320 毫秒，这与人类在对话中的反应时间相似。它在英语和代码文本上的表现与 GPT-4 Turbo 相当，在非英语语言文本上的表现有显著改善，同时也更快。与现有模型相比，GPT-4o 在视觉和音频理解方面尤其出色，价格却便宜50%。

而且，GPT-4o并不仅仅是语言模型技术上的新突破，它也是一款非常出彩的AI产品，是产品设计和用户体验方面的王者。

只要你打开麦克风和摄像头，你的AI就真的拥有了耳朵、嘴和眼睛，能够接收实时信息，无缝地和你（甚至是和另一个AI）互动。你和它聊天，就像和另外一个人聊天一样，它能够观察到你的语气、表情、外部环境的样子和你当前的个人状态。它可以为你辅导数学，给你做旅游向导，帮助你准备面试，甚至是两个或者多个AI还可以相互对话、沟通。