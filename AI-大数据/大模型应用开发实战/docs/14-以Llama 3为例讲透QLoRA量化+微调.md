你好，我是黄佳。

在上节课中，我们以Qwen模型为例，探讨了一下大语言模型参数高效微调的基本方法，重点介绍了当下最热门的LoRA技术，并通过PEFT框架实际操作了一把用Alpaca风格的中文数据微调Qwen模型。

不过上节课还有些地方没有讲透彻，比如LoRA的数学原理究竟是什么？在微调的同时，还有哪些压缩大模型的技巧？这一次，我们就换一个模型——LLM开源之王Llama 3，来继续讨论一下微调和量化的话题。

## Llama 3 模型介绍

Llama是由Meta AI最新发布的一个大语言模型家族，其中Llama 3是截至目前（2024年7月）的最强开源模型。

Llama系列模型开启了大语言模型（真正能用的、具有商用价值这个级别的）开源的先河，它的发展历程简单总结如下：

![图片](https://static001.geekbang.org/resource/image/fb/ea/fb7cf22ae0e97e81af25d7841b0yyfea.jpg?wh=1474x578)

表中已经开源的模型，均可以在 [Meta 官网](https://llama.meta.com/)或者 [Hugging Face 模型库](https://huggingface.co/meta-llama/Meta-Llama-3-8B)中下载（需要先申请下载权限）。

Llama 3 拥有两个版本，一个是8B（80亿）参数模型，另一个是70B（700亿）参数模型。这两个版本都相较于之前的迭代有了显著的性能提升，在多项基准测试中表现出色，与GPT-4和Claude等领先模型相比同样具有竞争力。

- Llama 3-8B：具有非常好的性价比，性能在同级别的模型中很突出。测试显示它在问答、摘要和指令执行等任务中表现良好。
- Llama 3-70B：70B模型在更复杂的任务中表现卓越。在MMLU（一般知识）和HumanEval（编码）等基准测试中表现优异​。这个模型特别擅长理解和生成细致入微的回应，在某些任务中可以与GPT-4等更大模型竞争。