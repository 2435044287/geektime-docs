本周我们主要来分享“矩阵分解”的点点滴滴，这是过去10年里推荐系统中最流行的一类模型。周一我们讨论了这类方法中最基础的基于隐变量的矩阵分解。这类模型的优势是显式地对用户和物品信息中的隐含结构进行建模，从而能够挖掘更加深层次的用户和物品关系。矩阵分解的流行起源于10年前的Netflix大赛，当时各类矩阵分解模型都在实际数据中起到了很好的效果。

今天我们要分享的模型，叫作“**基于回归的隐变量模型**”（Regression-based Latent Factor Model）。这是在基本矩阵分解的基础上衍生出来的一类模型。

## 基本矩阵分解的问题

我们先来看看基本矩阵分解模型的潜在问题。

首先，我们来回顾一下矩阵分解的基本表达。假设我们在对用户和物品的评分进行建模。对于每一个用户，用一个向量来表达其对于所有可能物品的评分，把所有用户的向量堆积起来，就可以得到一个矩阵。这个矩阵的每一行代表一个用户，每一列代表一个物品，每一个交叉的元素代表某一个用户对于某一个商品的评分。对于每一个用户和物品的隐向量都要少于原有的物品数目，因此，我们也说矩阵分解的模型实现了“降维”（降低建模维度）的目的。

虽然矩阵分解的模型对于挖掘用户和物品的内在联系有比较强的作用，但是这类模型的劣势也十分明显。

**第一，矩阵分解的矩阵仅仅是对用户和物品的喜好进行了“编码”**（Encode）。我们之前在解释基于内容的推荐系统时说过，对于一个复杂的工业级推荐系统来说，有很多灵感或者直觉，都很难仅仅依赖用户和物品的喜好来捕捉到。有大量的信号，无法很好地被融合到这个矩阵分解的模式里。因此，矩阵分解虽然是不错的“独立”模型（Standalone），但在融合多种不同的推荐元素方面，表现却很一般。
<div><strong>精选留言（5）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/56/87/97541441.jpg" width="30px"><span>米乐乐果</span> 👍（3） 💬（0）<div>希望老师可以顺带指出比较经典的论文😬</div>2018-03-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/4b/46/717d5cb9.jpg" width="30px"><span>惜心（伟祺）</span> 👍（2） 💬（0）<div>相当于先用矩阵分解得到隐变量向量 作为y
用用户和物品的显变量作为数据x
拟合求出 theta
在用的时候 利用新来用户武品显变量信息点乘参赛求出一个向量
后面求相似度就和矩阵分界出的物品矩阵和用户矩阵类似了</div>2018-04-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（0） 💬（0）<div>基于回归的矩阵分解需要在实践中尝试不同的回归模型。回归模型中的特征工程在实践中要结合领域知识，这里要推断和隐式特征的更好的关系不太容易。在大规模数据中这种方法的收敛性，找到最优解是不是有一定难度。</div>2018-03-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/22/69/c85fdb98.jpg" width="30px"><span>微微一笑</span> 👍（0） 💬（0）<div>文章的更新顺序不是按照目录来的吧？推荐系统架构剖析的文章找不到</div>2018-03-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/1b/1a/0ca7fe25.jpg" width="30px"><span>Peter</span> 👍（0） 💬（0）<div>老师，请问这块有没有一些资料或者论文可以看呀</div>2018-03-07</li><br/>
</ul>