机器学习与人工智能领域的顶级会议NIPS（Conference on Neural Information Processing Systems，神经信息处理系统大会）从1987年开始举办，已经有30多年的历史。NIPS 2017大会于2017年12月4日到9日在美国加利福尼亚州的长滩（Long Beach）举行。

每年大会都会在众多的学术论文中挑选出几篇最有新意和价值的论文作为最佳研究论文。在NIPS 2017上，一共有三篇论文获得了最佳论文的称号。今天，我就来带你认真剖析一下其中的一篇《具有凸目标的基于方差的正则化》（[Variance-based Regularization with Convex Objectives](https://papers.nips.cc/paper/6890-variance-based-regularization-with-convex-objectives.pdf)）。这篇论文的两位作者都是来自斯坦福大学的学者。

这篇文章理论性很强，主要研究的是一种“健壮的优化问题”（Robust Optimization），也就是说我们在优化一个“损失函数”（Loss Function）的时候，不仅要考虑损失函数的“均值”（Mean），还要考虑损失函数的“方差”（Variance）。然而，一个既要考虑均值又要考虑方差的综合的损失函数，往往是一个“非凸”（Non Convex）的问题。对于一般的非凸优化问题来说，我们往往不能找到一个全局的最优解，甚至是找到局部最优解也很困难。这篇文章就是要来解决这么一个问题。

## 作者群信息介绍

第一作者洪升⋅南空（Hongseok Namkoong）是斯坦福大学“运筹学”（Operations Research）的一名在读博士研究生。他的导师分别是约翰⋅达齐（John C. Duchi）和彼得⋅格林（Peter W. Glynn）。2013年到斯坦福之前，南空在韩国的韩国科学与技术高级研究所（Korea Advanced Institute of Science and Technology），有时候又称为KAIST，获得工业工程和数学学士学位。最近两三年，南空已经在发表了两篇NIPS的文章（包括这篇最佳论文），以及一篇ICML的论文。

第二作者约翰⋅达齐（John C. Duchi）是南空的导师之一。达奇可以说是师出名门，他于2007年从斯坦福本科毕业，接着在斯坦福跟随机器学习权威达菲⋅科勒（Daphne Koller），拿到了计算机科学的硕士学位；然后又到加州大学伯克利分校跟随统计学习权威迈克尔⋅乔丹（Michael Jordan）拿到了计算机科学的博士学位。在博士阶段的暑假里，达奇还到Google研究院中追随约然⋅辛格（Yoram Singer）积累了非常有价值的实习经验。之后，他来到了斯坦福大学担任统计和电气电子工程系的助理教授。

有了这些良好的基础，达奇的学术成绩也是非常扎实。他于2010年获得了ICML最佳论文奖。紧接着，2011年在Google实习期间的工作AdaGrad，成为了现在机器学习优化领域的经典算法，这个工作的论文有超过2500次的引用，而且也是深度学习优化算法的一个重要基础。目前，达奇所有论文的引用数超过6千次。

## 论文的主要贡献

我们首先来看一下这篇文章的主要贡献，理解文章主要解决了一个什么场景下的问题。

**很多机器学习问题其实都可以最终归结于优化一个目标函数（Objective Function）或者有时候叫做损失函数（Loss Function）的问题**。针对训练数据集上损失函数的优化（即最大化或最小化）并且在测试集上表现优异，是可以被证明为最终能够较好“泛化”（Generalization）的一种体现。

那么，**通常情况下，这个损失函数都是针对均值的一个描述，比如在整个训练数据集上的平均误差，或者说在整个训练数据集上的平均准确度**。然而，我们都知道，在一些很“偏斜”（Skewed）的数据分布上，均值并不是很好的一个数据描述。即便我们的函数能够在“平均”的情况下优化一个损失函数，这个函数也有可能在一些，甚至大部分数据点上表现得不尽如人意。
<div><strong>精选留言（2）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（9） 💬（0）<div>常规的降低方差，也就是减少过拟合的方法有
1) 找到一个合适的复杂度，比如目标函数多项式的次数，降低次数可以降低方差；
2）引入合适的正则化参数lambda，lambda越大过拟合程度越低；
3）集成学习中的bagging（传统视角）；
4）减少特征数量；
5）获取更多数据。

直接在目标函数里评估误差的方差对我来说是一种新的思路。

P.S. 极客时间能提供桌面版写留言吗？没有平板，4英寸的手机屏幕上输入大段中文对我来说有点费时。</div>2018-01-29</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTI5uDOruARAmM91UYKs8yyXZpkdXkXF96AaZSib3dUNRah6SjY4eHbLJiczlrnsPXCvvax3icd8w9JJQ/132" width="30px"><span>yy</span> 👍（0） 💬（0）<div>非常感谢！分享的真好！</div>2018-06-29</li><br/>
</ul>