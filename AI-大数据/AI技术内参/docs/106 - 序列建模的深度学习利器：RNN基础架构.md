前面我们介绍了一个重要的文本模型，Word2Vec，我们聊了这个模型的基本假设，模型实现，一些重要的扩展，以及其在自然语言处理各个领域的应用。

接下来，我们来讨论更加复杂的\*\*基于深度学习的文本分析模型**。这些模型的一大特点就是更加丰富地利用了**文字的序列信息\*\*，从而能够对文本进行大规模建模。

今天，我们首先来看一看，序列建模的深度学习利器 **RNN**（Recurrent Neural Network，递归神经网络）的基本架构。

## 文本信息中的序列数据

我们在之前介绍Word2Vec的时候，讲了为什么希望能够把上下文信息给融入到模型当中去。一个非常重要的原因，就是在最早的利用“词包”（Bag of Word）的形式下，离散的词向量无法表达更多的语义信息。那么，从文本的角度来讲，很多研究人员都面对的困扰是，如何对有序列信息的文本进行有效的建模？同时，对于广大文本挖掘的科研工作者来说，这也是大家心中一直深信不疑的一个假设，那就是**对文字的深层次的理解一定是建立在对序列、对上下文的建模之中**。

你可能有一个疑问，文字信息中真的有那么多序列数据吗？

其实，从最简单的语义单元“句子”出发，到“段落”，到“章节”，再到整个“文章”。这些文字的组成部分都依赖于对更小单元的序列组合。例如，句子就是词语的序列，段落就是句子的序列，章节就是段落的序列等等。不仅是“词包假设”无法对这样的序列进行建模，就算是我们之前提到的Word2Vec等一系列学习词向量或者段落向量的方法，也仅仅能考虑到一部分的上下文信息。

还有更加复杂的文字序列，比如对话。人与人的对话很明显是有顺序的。两个人之间进行对话，当前所说的字句都是根据对方的回应以及整个对话的上下文所做出的选择。如果要对这样复杂的文字序列进行建模，传统的不考虑序列的模型方法是肯定不能奏效的。

那么，传统的机器学习领域，有没有能够对时序信息建模的工具或者模型呢？
<div><strong>精选留言（2）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/1e/96/6a/7c630e77.jpg" width="30px"><span>rz</span> 👍（1） 💬（0）<div>hmm的隐含状态通常都是离散并且有限的，而rnn中的隐含状态是一种连续的变量，也许能够代表着更多的信息</div>2020-11-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/71/45/abb7bfe3.jpg" width="30px"><span>Andy</span> 👍（0） 💬（0）<div>我感觉RNN比起HMM最大的一个有点就是可以建立某个状态与之前的很多个状态的联系，而HMM只能建立与前一个状态的关系，然而人类处理序列模型的时候，更加能参考上下文，所以RNN更接近人类的处理方式</div>2018-06-01</li><br/>
</ul>