周一我们进一步展开了RNN这个基本框架，讨论了几个流行的RNN模型实现，从最简单的RNN模型到为什么需要“门机制”，再到流行的LSTM和GRU框架的核心思想。

今天，我们就来看一看RNN究竟在自然语言处理的哪些任务和场景中有所应用。

## 简单分类场景

我们首先来聊一种**简单的分类场景**。在这种场景下，RNN输入一个序列的文字，然后根据所有这些文字，做一个决策，或者叫作输出一个符号。这类应用是文本挖掘和分析中最基本的一个场景。

在绝大多数的“简单分类”任务中，传统的文字表达，例如“**词包**”（Bag of Word）或者“**N元语法**”（Ngram），经常都能有不错的表现。也就是说，在很多这类任务中，文字的顺序其实并不是很重要，或者说词序并没有携带更多的语义信息。

然而，实践者们发现，在一些场景中，如果利用RNN来对文字序列进行建模，会获得额外的效果提升。比如有一类任务叫作“**句子级别的情感分类**”（Sentence-Level Sentiment Classification），这类任务常常出现在分析商品的评论文本（Review）这个场景。这时候，我们需要对每一个句子输出至少两种感情色彩的判断，褒义或者贬义，正面或者负面。比如，我们在分析电影评价的时候，就希望知道用户在某一个句子中是否表达了对电影“喜爱”或者“不喜爱”的情绪。

面对这样句子级别的情感分析，一种比较通行的利用RNN建模的方式是：**把每一个单词作为一个输入单元，然后把一个句子当作一个序列输入到一个RNN中去，RNN来维持一个隐含的状态**。

对于这类应用，不是每一个隐含状态都有一个输出，而是在句子结束的时候，利用最后的隐含状态来产生输出。对于这类任务而言，输出的状态就是一个二元判断，那么我们需要利用最后的隐含状态来实现这个目的。一般来说，在深度模型的架构中，这个步骤是利用最后的隐含状态，然后经过多层感知网络，最后进行一个二元或者多元的分类。这其实是一个标准的分类问题的构建。

在有的应用中，研究者们发现可以**利用两个RNN建立起来的链条**，从而能够更进一步地提升最后的分类效果。在我们刚才描述的建模步骤里，RNN把一个句子从头到尾按照正常顺序进行了输入并归纳。另外一种建模方式是利用RNN去建模**句子的逆序**，也就是把整个句子倒过来，学习到一个逆序的隐含状态。接下来，我们把顺序的最后隐含状态和逆序的最后隐含状态串联起来，成为最终放入分类器需要学习的特性。这种架构有时候被称作“**双向模型**”。
<div><strong>精选留言（1）</strong></div><ul>
<li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTL9G1CvS4VrqeE5MZPuWtf7qCQYx0RAiaXiakNRG3lFCgt7eeiaSYuAJmVb6o2kDROoFfeGibnP3yCPxQ/132" width="30px"><span>humanchao</span> 👍（0） 💬（0）<div>rnn很难捕捉文档整体内容，1.多文档长度不一，网络参数不好确定 2. 文档长度过长，面向整体建模而不是面向标注目标建模，长短特征兼顾导致训练困难。</div>2018-05-12</li><br/>
</ul>