这周，我们继续来讨论基于深度学习的文本分析模型。这些模型的一大特点就是更加丰富地利用了文字的序列信息，从而能够对文本进行大规模的建模。在上一次的分享里，我们聊了对序列建模的深度学习利器“递归神经网络”，或简称RNN。我们分析了文本信息中的序列数据，了解了如何对文本信息中最复杂的一部分进行建模，同时还讲了在传统机器学习中非常有代表性的“隐马尔科夫模型”（HMM）的基本原理以及RNN和HMM的异同。

今天我们进一步展开RNN这个基本框架，看一看在当下都有哪些流行的RNN模型实现。

## 简单的RNN模型

为了能让你对今天要进一步介绍的RNN模型有更加深入的了解，我们先来回顾一下RNN的基本框架。

一个RNN通常有一个输入序列X和一个输出序列Y，这两个序列都随着时间的变化而变化。也就是说，每一个时间点，我们都对应着一个X和一个Y。RNN假定X和Y都不独立发生变化，它们的变化和关系都是通过一组隐含状态来控制的。具体来说，时间T时刻的隐含状态有两个输入，一个输入是时间T时刻之前的所有隐含状态，一个输入是当前时刻，也就是时间T时刻的输入变量X。时间T时刻的隐含状态根据这两个输入，会产生一个输出，这个输出就是T时刻的Y值。

那么，在这样的一个框架下，一个最简单的RNN模型是什么样子的呢？我们需要确定两个元素。第一个元素就是在时刻T，究竟如何处理过去的隐含状态和现在的输入，从而得到当前时刻的隐含状态，这是一个需要建模的元素。第二，如何从当前的隐含状态到输出变量Y，这是另外一个需要建模的元素。

最简单的RNN模型对这两个建模元素是这样选择的。通常情况下，在时间T-1时刻的隐含状态是一个向量，我们假设叫St-1，那么这个时候，我们有两种选择。

第一种选择是用一个线性模型来表达对当前时刻的隐含状态St的建模，也就是把St-1和Xt当作特性串联起来，然后用一个矩阵W当作是线性变换的参数。有时候，我们还会加上一个“偏差项”（Bias Term），比如用b来表示。那么在这样的情况下，当前的隐含状态可以认为是“所有过去隐含状态以及输入”的一阶线性变换结果。可以说，这基本上就是最简单直观的建模选择了。

第二种选择是如何从St变换成为Y。这一步可以更加简化，那就是认为St直接就是输出的变量Y。这也就是选择了隐含状态和输出变量的一种一一对应的情况。

在这个最简单的RNN模型基础上，我们可以把第一个转换从**线性转换**变为任何的深度模型的**非线性转换**，这就构成了更加标准的RNN模型。

## LSTM与GRU模型

我们刚刚介绍的RNN模型看上去简单直观，但在实际应用中，这类模型有一个致命的缺陷，那就是实践者们发现，在现实数据面前根本没法有效地学习这类模型。什么意思呢？
<div><strong>精选留言（2）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/6c/9f/0343d633.jpg" width="30px"><span>黄德平</span> 👍（2） 💬（0）<div>个人认为是解决梯度异常的需要催生了门机制，然后发现门机制可以进行长时序信息的选择性提取</div>2018-12-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/f3/b5/5db57375.jpg" width="30px"><span>离忧</span> 👍（0） 💬（0）<div>rnn需要门机制，应该是为了防止剃度消失等情况，所以为了防止这样情况，应该建模的时候，就需要。</div>2018-07-01</li><br/>
</ul>