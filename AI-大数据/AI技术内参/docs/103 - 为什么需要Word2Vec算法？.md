至此，关于文本分析这个方向，我们已经介绍了 **LDA**（Latent Diriclet Allocation），这是一个出色的无监督学习的文本挖掘模型。还有“**隐语义分析**”（Latent Semantic Indexing），其核心是基于矩阵分解的代数方法。接着，我们分享了“**概率隐语义分析**”（Probabilistic Latent Semantic Indexing），这类模型有效地弥补了隐语义分析的不足，成为了在LDA兴起之前的有力的文本分析工具。我们还介绍了**EM**（Expectation Maximization）算法，这是针对隐参数模型最直接有效的训练方法之一。

今天，我们进入文本分析的另外一个环节，介绍一个最近几年兴起的重要文本模型，**Word2Vec**。可以说，这个模型对文本挖掘、自然语言处理、乃至很多其他领域比如网络结构分析（Network Analysis）等都有很重要的影响。

我们先来看Word2Vec的一个最基本的形式。

## Word2Vec背景

了解任何一种模型或者算法都需要了解这种方法背后被提出的动机，这是一种**能够拨开繁复的数学公式从而了解模型本质的方法**。

那么，Word2Vec的提出有什么背景呢？我们从两个方面来进行解读。

首先，我们之前在介绍LDA和PLSA等隐变量模型的时候就提到过，这些模型的一大优势就是在文档信息没有任何监督标签的情况下，能够学习到文档的“隐含特性”。这也是文档领域“**表征学习**”（Representation Learning）的重要工具。遗憾的是，不管是LDA还是PLSA其实都是把文档当作“**词包**”（Bag Of Word），然后从中学习到语言的特征。

这样做当然可以产生不小的效果，不过，从自然语言处理，或者是文档建模的角度来说，人们一直都在探讨，如何能够把**单词的顺序**利用到学习表征里。什么意思呢？文档中很重要的信息是单词的顺序，某一个特定单词组合代表了一个词组或者是一个句子，然后句子自然也就代表着某种语义。词包的表达方式打破了所有词组顺序以及高维度的语义表达，因此长期以来被认为并不能真正学习到语言的精华。

然而，在主题模型这个大旗帜下，已经有不少学者和研究员试图把词序和语义给加入到模型中，这些尝试都没有得到很好的效果，或者模型过于复杂变得不适用。于是，大家都期待着新的工具能够解决这方面的问题。

另外一个思路也是从词包发展来的。词包本身要求把一个词表达成为一个向量。这个向量里只有一个维度是1，其他的维度都是0。因为每个词都表达成为这样离散的向量，因此词与词之间没有任何的重叠。既然两个离散的向量没有重叠，我们自然也就无法从这个离散的词包表达来推断任何词语的高维度语义。这也是为什么大家会利用主题模型从这个离散的词包中抽取主题信息，从而达到理解高维度语义的目的。
<div><strong>精选留言（2）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/11/92/64/e1fe7ad3.jpg" width="30px"><span>hayley</span> 👍（0） 💬（0）<div>对于短文本，怎么提升word2vec的效果？</div>2018-07-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/23/c3/625eef99.jpg" width="30px"><span>arfa</span> 👍（0） 💬（0）<div>LDA得到的是主题及主题的词分布，word2vec计算的是是词向量</div>2018-05-11</li><br/>
</ul>