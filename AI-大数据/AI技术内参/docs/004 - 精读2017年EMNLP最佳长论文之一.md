自然语言处理实证方法会议**EMNLP**（Conference on Empirical Methods in Natural Language Processing），是由国际计算语言学协会**ACL**（Association for Computational Linguistics）的专委会**SIGDAT**（Special Interest Group on Linguistic Data and Corpus-based Approaches to NLP）主办，每年召开一次，颇具影响力和规模，是自然语言处理类的顶级国际会议。从1996年开始举办，已经有20多年的历史。2017年的EMNLP大会于9月7日到11日在丹麦的哥本哈根举行。

每年大会都会在众多的学术论文中挑选出两篇最具价值的论文作为最佳长论文（Best Long Paper Award）。 今天，我就带你认真剖析一下EMNLP今年的最佳长论文，题目是《男性也喜欢购物：使用语料库级别的约束条件减少性别偏见的放大程度》（Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints） 。这篇文章也是很应景，近期学术圈对于数据和机器学习算法有可能带来的“**偏见**”（Bias）感到关切，有不少学者都在研究如何能对这些偏见进行评估、检测，进而可以改进甚至消除。

## 作者群信息介绍

第一作者赵洁玉（Jieyu Zhao），论文发表的时候在弗吉尼亚大学计算机系攻读博士学位，目前，已转学到加州大学洛杉矶分校，从事如何从机器学习算法中探测和消除偏见的研究。之前她从北京航空航天大学获得学士和硕士学位，曾于2016年在滴滴研究院实习。

第二作者王天露（Tianlu Wang）也是来自弗吉尼亚大学计算机系的博士生，之前在浙江大学获得计算机学士学位。第三作者马克·雅茨卡尔（Mark Yatskar）是来自华盛顿大学的计算机系博士生，已在自然语言处理以及图像处理领域发表过多篇高质量论文。

第四作者文森特（Vicente Ordóñez）目前在弗吉尼亚大学计算机系任助理教授。他的研究方向是自然语言处理以及计算机视觉的交叉学科。他于2015年从北卡罗来纳大学教堂山分校计算机系博士毕业。博士期间，他在微软研究院、eBay研究院以及谷歌都有过实习经历。他是第二作者王天露的博士导师。

文章最后一位作者是Kai-Wei Chang，也是第一作者赵洁玉的导师。他目前在加州大学洛杉矶分校任助理教授，之前在弗吉尼亚大学任职。他于2015年从伊利诺伊大学香槟分校博士毕业，师从著名教授丹·罗斯（Dan Roth）。在之前的研究生涯中，曾先后3次在微软研究院实习，也在谷歌研究院实习过。在他研究的早期，曾参与了LibLinear这个著名支持向量机软件的研发工作。

## 论文的主要贡献

机器学习的一个重要任务就是通过数据来学习某些具体事项。最近机器学习的研究人员发现，数据中可能蕴含着一些社会赋予的偏见，而机器学习算法很有可能会放大这些偏见。这种情况在自然语言处理的相关任务中可能更为明显。比如，在一些数据集里，“做饭”这个词和“女性”这个词一起出现的比例可能要比和“男性”一起出现的比例高30%，经过机器学习算法在这个数据集训练之后，这个比例在测试数据集上可能就高达68%了。因此，虽然在数据集里，社会偏见已经有所呈现，但是这种偏见被机器学习算法放大了。

因此，**这篇文章的核心思想就是，如何设计出算法能够消除这种放大的偏见，使得机器学习算法能够更加“公平”**。注意，这里说的是消除放大的偏见，而不是追求绝对的平衡。比如，我们刚才提到的数据集，训练集里已经表现出“女性”和“做饭”一起出现的频率要高于“男性”和“做饭”一起出现的频率。那么，算法需要做的是使这个频率不会进一步在测试集里升高，也就是说，保持之前的30%的差距，而不把这个差距扩大。这篇文章并不是追求把这个差距人为地调整到相同的状态。

文章提出了一个**限制优化（Constrained Optimization）算法**，为测试数据建立限制条件，使机器学习算法的结果在测试集上能够得到和训练集上相似的偏见比例。注意，这是对已有测试结果的一个调整（Calibration），因此可以应用在多种不同的算法上。

作者们使用提出的算法在两个数据集上做了实验，得到的结果是，新的测试结果不但能够大幅度（高达30%至40%）地减小偏见，还能基本保持原来的测试准确度。可见，提出的算法效果显著。

## 论文的核心方法

那么，作者们提出的究竟是一种什么方法呢？
<div><strong>精选留言（4）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/72/3e/534db55d.jpg" width="30px"><span>huan</span> 👍（3） 💬（1）<div>“偏见”也是一种模式，但并不是训练集上的被测量对象，从而容易出现“过拟合”；更进一步的，现在常用的训练方法中，即使已经完成被测量对象的模型参数的筛选，但是仍然需要跑相当多的全训练集Epoch，才“放心”的输出模型参数，而这可能更加增强了偏见的“过拟合”。我觉得现在的DNN的训练方法更容易造成偏见过度增强。
我的一个问题是，注意到老师解读的论文中是偏见被修正的同时并没有减少准确率，如果能有效的察觉并发现这些偏见那？有没有相关的文献？或者相关的偏见的dataset?</div>2017-11-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/43/c3/62eb8c16.jpg" width="30px"><span>高锋</span> 👍（3） 💬（0）<div>也感觉是过拟合了。文中“限制优化”、数学上似乎更长用的应该是 “约束优化”这个词吧，😄</div>2019-08-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/8f/98/7d1287d9.jpg" width="30px"><span>韩 * *</span> 👍（2） 💬（0）<div>我觉得就是过拟合了，然后加正则限制的事儿，不过给了个消除“性别偏见”的说法</div>2019-08-12</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLN7k1Sg2m5LXia9wiay1S5APdN9HcTfPHTjibrsiaNYiaTff0kUVbnWLgE66r9px1JeRv06lLB8QLJVCg/132" width="30px"><span>胡杰</span> 👍（1） 💬（0）<div>我猜想是不是会和数据降维的一些算法有关系？在降维的过程中测试样本的某些特征被映射的过程中放大了。</div>2020-03-15</li><br/>
</ul>