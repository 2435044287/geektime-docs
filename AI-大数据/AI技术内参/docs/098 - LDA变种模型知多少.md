我们在之前的分享中曾经介绍过文本挖掘（Text Mining）中的重要工具LDA（Latent Diriclet Allocation）的基本原理。在文本挖掘中，有一项重要的工作就是分析和挖掘出文本中隐含的结构信息，而不依赖任何提前标注（Labeled）的信息。也就是说，我们希望能够利用文本挖掘技术来对无标签的数据进行挖掘，这是典型的无监督学习。

LDA就是一个出色的无监督学习的文本挖掘模型。这个模型在过去的十年里开启了主题模型（Topic Model）这个领域。不少学者都利用LDA来分析各式各样的文档数据，从新闻数据到医药文档，从考古文献到政府公文。在一段时间内，LDA成为了分析文本信息的标准工具。而从最原始的LDA发展出来的各类模型变种，则被应用到了多种数据类型上，包括图像、音频、混合信息、推荐系统、文档检索等等，可以说各类主题模型变种层出不穷。

今天我们就结合几篇经典论文，来看一看**LDA的各种扩展模型**。当然，在介绍今天的内容之前，我们首先来回顾一下LDA模型的一些基本信息。

## LDA模型的回顾

LDA模型是一个典型的**产生式模型**（Generative Model）。产生式模型的一大特点就是通过一组概率语言，对数据的产生过程进行描述，从而对现实数据建立一个模型。注意，这个产生过程的本质是描述的一个**联合概率分布**（Joint Distribution）的分解过程。也就是说，这个过程是一个虚拟的过程，真实的数据往往并不是这样产生的。这样的产生过程是模型的一个假设，一种描述。任何一个产生过程都可以在数学上完全等价一个联合概率分布。

LDA的产生过程描述了文档以及文档中文字的产生过程。在原始的LDA论文中，作者们描述了对于每一个文档而言的产生过程。

[LDA模型的前世今生](https://time.geekbang.org/column/article/376)

相比于传统的文本聚类方法，LDA对于每个文档的每一个字都有一个主题下标，也就是说，LDA是没有一个文档统一的聚类标签，而是每个字有一个聚类标签，在这里就是主题。

LDA模型的训练一直是一个难点。传统上，LDA的学习属于**贝叶斯推断**（Bayesian Inference），而在2000年初期，只有**MCMC算法**（Markov chain Monte Carlo，马尔科夫链蒙特卡洛）以及 **VI**（Variational Inference，变分推断）作为工具可以解决。在最初的LDA论文里，作者们采用了VI；后续大多数LDA相关的论文都选择了MCMC为主的吉布斯采样（Gibbs Sampling）来作为学习算法。

## LDA的扩展

当LDA被提出以后，不少学者看到了这个模型的潜力，于是开始思考怎么把更多的信息融入到LDA里面去。通过我们上面的讲解，你可以看到，LDA只是对文档的文字信息本身进行建模。但是绝大多数的文档数据集还有很多额外的信息，如何利用这些额外信息，就成为了日后对LDA扩展的最重要的工作。

第一个很容易想到的需要扩展的信息就是作者信息。特别是LDA最早期的应用，对于一般的文档来说，比如科学文档或者新闻文档，都有作者信息。很多时候我们希望借用作者在写文档时的遣词造句风格来分析作者的一些写作信息。那么，如何让LDA能够分析作者的信息呢？
<div><strong>精选留言（2）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/10/25/ad/d9757d31.jpg" width="30px"><span>Jack_Sainity</span> 👍（2） 💬（0）<div>每个用户看做一篇文档，用户选择的商品视作文档中的每个词。</div>2018-04-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（1） 💬（0）<div>通过LDA生成用户的兴趣主题(商品的语义标签是一种数据来源)，这个过程有些类似于生成文档。然后根据这些用户兴趣主题来寻找匹配的商品，比如计算和商品主题的相似度。</div>2018-04-18</li><br/>
</ul>