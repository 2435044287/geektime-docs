在上一期的介绍里，我们讨论了以深度学习为背景的计算机视觉技术，重点讲解了为什么需要深度学习，特别是从传统模型的眼光来看深度学习模型的特点。

今天，我们来聊一聊应用到图像上的一些最基本的深度学习模型。

## 前馈神经网络

**前馈神经网络**（Feedforward Networks）应该算是最基本的神经网络架构了。这种架构是理解其他神经网络结构的基础。

我们可以从最基本的线性模型（Linear Model）入手，来理解前馈神经网络。线性模型说的是有一组输入x，然后有一个输出y，我们学习到一组向量，有的时候也叫作系数w，来通过x预测y。这种线性模型可以算是最简单的机器学习模型了。在图像的情况下，输入往往是一个向量，输出也是一个向量，这个时候，我们需要学习的系数就从一个向量变为一个**矩阵**了。

那么，试想一下，如果我们把多层的线性模型进行叠加，能否得到多层的神经网络结构呢？答案是否定的。即便是多层的线性模型，只要每一层的变换是线性的，那么最后的结果一定也是线性的。因此，要想构建多层的非线性模型，每一层的变换也一定要是非线性的。

那么，如何在线性模型的基础上，我们只进行一些最小的改动，就能引入非线性的因素呢？

在这里，我们引入一个叫“**激活函数**”（Activation Function）的概念。直观地理解，激活函数就是在线性模型输出的基础上进行非线性变换。一个最简单的激活函数就是**Sigmoid函数**，也就是把负无穷到正无穷的实数给映射到0～1这个范围内。我们经常提到的对数几率回归，其实就是对这种变换的另一种称呼。
<div><strong>精选留言（5）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/12/8d/7a/4c187d0d.jpg" width="30px"><span>医用地狗精灵</span> 👍（6） 💬（0）<div>池化是为了让学习不过于注重不必要讯息，比如说辨认一张脸，知道有眼睛就可以了，不需要知道眼睛是在图的哪一个地方。通过池化还可以降低计算成本，提高效率。</div>2018-09-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/e9/28/117faa64.jpg" width="30px"><span>嘟嘟科技说</span> 👍（3） 💬（0）<div>池化从一定程度上讲，有防止过拟合的作用</div>2018-09-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/85/49/585c69c4.jpg" width="30px"><span>皮特尔</span> 👍（1） 💬（0）<div>池化层可以用于减少计算参数、防止过拟合。

池化有两个主要作用：
1. 特征不变性：经过池化，去掉的只是一些无关紧要的信息，而留下的信息则具有尺度不变性的特征，是最能表达图像的特征。
2. 特征降维：一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来。</div>2020-05-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1a/52/76/7fb4a7a9.jpg" width="30px"><span>Alice</span> 👍（0） 💬（0）<div>应用于图像的基本深度学习模型
1前馈神经网络：用激活函数来加入非线性元素
2卷积神经网络：
卷积操作是为了提取图像局部特征；
池化是为了进一步更高层次地总结概括数据，是不是也能叫做进一步提取更抽象的特征呢？

讲解说卷积神经网络，核心是用向量来描述一个矩阵的信息，不理解</div>2019-11-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/e9/28/117faa64.jpg" width="30px"><span>嘟嘟科技说</span> 👍（0） 💬（0）<div>池化从一定程度上说，起到防止过拟合的作用</div>2018-09-09</li><br/>
</ul>