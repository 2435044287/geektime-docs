上周我们讨论了EE算法，介绍了UCB（Upper Confidence Bound）算法和“汤普森采样”（Thompson Sampling）。

这周，我们回归到一个更加传统的话题，那就是**如何评测推荐系统**。这个话题非常重要，牵涉到如何持续对一个推荐系统进行评价，从而能够提高推荐系统的精度。

今天，我们先来看一看**推荐系统的线下评测**。

## 基于评分的线下评测

在过去10年里，随着Netflix大奖赛的举行，很多研究人员和工程人员往往把推荐系统的模型学习简化为对用户评分的一种估计。同时，在模型上面来说，对用户物品评分矩阵进行分解成为了一种主流的方法。

在这样的场景下，如何对模型进行评测呢？

一种简单且直观的办法，就是**衡量评分的准确性**，换句话说，也就是看我们预测的评分和真实评分之间有多大的差距。

那么，有哪些方法可以用来衡量两个数值之间的差异呢？

在机器学习中，一个经常使用的测度叫“**均方差**”（Mean Square Error），或**MSE**。有时候，我们也会使用它的根号后的结果，叫作“**方差**”（Rooted Mean Square Error），或**RMSE**。

MSE是这么定义的。首先，如果我们知道一个用户i和物品j的真实评分，假设叫$ Y\_{ij} $ ，那么我们的一个估计值是$ Z\_{ij} $，MSE计算的就是$ Y\_{ij} $ 和$ Z\_{ij} $的差值，然后取平方。平方以后的结果肯定是一个正数，也就是说这样做的好处是整个计算不会出现负数，我们的估计值比真实值小了或者大了，MSE都可以处理。当我们对于每一个用户和物品都计算了这个差值以后，再对所有的差值的平方取一个平均值，就得到了我们所要的MSE。

从计算上来讲，RMSE就是在MSE的基础上再取一个根号。我们在很多实际应用中，往往使用RMSE来汇报模型的评测结果。同时，RMSE也经常用在大多数的学术论文中，但这个评测有没有什么问题呢？
<div><strong>精选留言（2）</strong></div><ul>
<li><img src="" width="30px"><span>ninenight</span> 👍（0） 💬（0）<div>利用ndcg在做线下评测的时候怎么具体操作呢，是先标注点击相关吗，但是线下评测的时候还没上到线上呢，也不知道点击数据，这个时候怎么在线下评测呢，比如我要现在线下评测下效果，然后再上线，这个时候怎么能评测效果呢，请指教</div>2018-11-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（0） 💬（0）<div>基于排序的推荐系统会一直推荐用户有交互行为的物品，发掘新物品和保持多样性的能力会降低</div>2018-03-27</li><br/>
</ul>