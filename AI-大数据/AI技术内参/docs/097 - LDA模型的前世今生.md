在文本挖掘中，有一项重要的工作就是分析和挖掘出文本中隐含的结构信息，而不依赖任何提前标注的信息。今天我要介绍的是一个叫做**LDA**（Latent Dirichlet Allocation）的模型，它在过去十年里开启了一个领域叫**主题模型**。

从LDA提出后，不少学者都利用它来分析各式各样的文档数据，从新闻数据到医药文档，从考古文献到政府公文。一段时间内，LDA成了分析文本信息的标准工具。从最原始的LDA发展出来的各类模型变种，则被应用到了多种数据类型上，包括图像、音频、混合信息、推荐系统、文档检索等等，各类主题模型变种层出不穷。下面我来简单剖析一下LDA这个模型，聊聊它的模型描述以及训练方法等基础知识。

## LDA的背景介绍

LDA的论文作者是戴维·布雷（David Blei）、吴恩达和迈克尔·乔丹（Michael Jordan）。这三位都是今天机器学习界炙手可热的人物。论文最早发表在2002年的神经信息处理系统大会（Neural Information Processing Systems，简称NIPS）上，然后长文章（Long Paper）于2003年在机器学习顶级期刊《机器学习研究杂志》（Journal of Machine Learning Research）上发表。迄今为止，这篇论文已经有超过1万9千次的引用数，也成了机器学习史上的重要文献之一。

论文发表的时候，戴维·布雷还在加州大学伯克利分校迈克尔手下攻读博士。吴恩达当时刚刚从迈克尔手下博士毕业来到斯坦福大学任教。戴维 2004年从伯克利毕业后，先到卡内基梅隆大学跟随统计学权威教授约翰·拉弗蒂（John Lafferty）做了两年的博士后学者，然后又到东部普林斯顿大学任教职，先后担任助理教授和副教授。2014年转到纽约哥伦比亚大学任统计系和计算机系的正教授。戴维在2010年获得斯隆奖（Alfred P. Sloan Fellowship，美国声誉极高的奖励研究人员的奖项，不少诺贝尔奖获得者均在获得诺贝尔奖多年之前获得过此奖），紧接着又在2011年获得总统青年科学家和工程师早期成就奖（Presidential Early Career Award for Scientists and Engineers，简称PECASE）。目前他所有论文的引用数超过了4万9千次 。

吴恩达在斯坦福晋升到副教授后，2011年到2012年在Google工作，开启了谷歌大脑（Google Brain）的项目来训练大规模的深度学习模型，是深度学习的重要人物和推动者之一。2012年他合作创建了在线学习平台Coursera，可以说是打开了慕课（Massive Open Online Course，简称MOOC）运动的大门。之后吴恩达从2014年到2017年间担任百度首席科学家，并创建和运行了百度在北美的研究机构。目前他所有论文的引用数超过8万3千次。

文章的第三作者迈克尔·乔丹是机器学习界的泰斗人物。他自1998年在加州大学伯克利任教至今，是美国三个科学院院士（American Academy of Arts and Sciences、National Academy of Engineering以及National Academy of Sciences），是诸多学术和专业组织的院士（比如ACM、IEEE、AAAI、SIAM等）。迈克尔可以说是桃李满天下，而且其徒子徒孙也已经遍布整个机器学习领域，不少都是学术权威。他的所有论文有多达12万次以上的引用量。

值得注意的是，对于三位作者来说，LDA论文都是他们单篇论文引用次数最多的文章。

## LDA模型

要描述LDA模型，就要说一下LDA模型所属的**产生式模型**（Generative Model）的背景。产生式模型是相对于**判别式模型**（Discriminative Model）而说的。这里，我们假设需要建模的数据有特征信息，也就是通常说的X，以及标签信息，也就是通常所说的Y。

判别式模型常常直接对Y的产生过程（Generative Process)进行描述，而对特征信息本身不予建模。这使得判别式模型天生就成为构建分类器或者回归分析的有利工具。而产生式模型则要同时对X和Y建模，这使得产生式模型更适合做无标签的数据分析，比如聚类。当然，因为产生式模型要对比较多的信息进行建模，所以一般认为对于同一个数据而言，产生式模型要比判别式模型更难以学习。

一般来说，产生式模型希望通过一个产生过程来帮助读者理解一个模型。注意，这个产生过程本质是描述一个**联合概率分布**（Joint Distribution）的分解过程。也就是说，这个过程是一个虚拟过程，真实的数据往往并不是这样产生的。这样的产生过程是模型的一个假设，一种描述。任何一个产生过程都可以在数学上完全等价一个联合概率分布。

LDA的产生过程描述了文档以及文档中文字的生成过程。在原始的LDA论文中，作者们描述了对于每一个文档而言有这么一种生成过程：
<div><strong>精选留言（8）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/45/3b/075d5e14.jpg" width="30px"><span>Geek_vidmje</span> 👍（5） 💬（0）<div>对于非统计学或cs专业的同学，上一期和这一期似乎有点深奥。但是还是十分感谢老师的辛苦付出！</div>2017-10-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/4f/1c/abb7bfe3.jpg" width="30px"><span>沉</span> 👍（2） 💬（0）<div>如果能在文中给出一些可以详细参考的资料就更好了～</div>2017-10-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/57/44/abb7bfe3.jpg" width="30px"><span>兔子ORZ</span> 👍（1） 💬（0）<div>有监督的LDA比较好的形式就是LLDA，在plate notation上的theta上再加上一个观察变量表示主题标签，而这个观察变量也是基于狄利克雷超参的</div>2018-04-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/e5/28/bb7eda4b.jpg" width="30px"><span>unicornmm</span> 👍（1） 💬（0）<div>希望能够给出每篇文章的链接，非常感谢老师的讲解</div>2018-01-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1e/f2/ce/791d0f5e.jpg" width="30px"><span>张开元</span> 👍（0） 💬（0）<div>如果我们希望在LDA模型中对某一个段落中的所有文字都来自同一个主题，我们可以进行以下考虑的修改：

调整模型假设：在原始的LDA模型中，每个词独立地从文档的主题分布中选取主题。如果我们希望一个段落中的所有词来自同一个主题，那么可以将模型修改为在段落级别选择主题，而非在单词级别。这需要我们定义段落级别的主题分布，并且在生成单词之前首先为每个段落抽取一个主题。

修改概率图模型：在概率图中，每个段落可以有一个与之相关联的潜在主题变量，而不是每个单词。这样，在生成词汇时，所有的词将共享相同的段落级主题变量，而不是各自独立的主题。

修改推理过程：在采样或变分推断算法中，现在需要针对段落而非单个词来更新主题分配。这意味着在计算每个段落的主题分布时，我们需要考虑段落内所有词的联合分布。

调整先验分布：可以对段落级别的主题分布引入一个狄利克雷先验，这样可以控制段落中词的主题多样性。如果希望段落中的词更可能来自同一个主题，可以选择较小的先验参数。</div>2024-01-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/70/81/ba166574.jpg" width="30px"><span>昉</span> 👍（0） 💬（0）<div>长期基础研究，和短期工程实现本质上有传统的。有种可能性，就是给研究团队配备专门的专门转化和monetization团队，相互配合，各司其职。</div>2019-07-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/73/3d/287d2fdd.jpg" width="30px"><span>潜行</span> 👍（0） 💬（0）<div>希望老师给文章链接，另外提到很多统计学的知识，还看不懂。。得好好消化</div>2018-08-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/4b/46/717d5cb9.jpg" width="30px"><span>惜心（伟祺）</span> 👍（0） 💬（0）<div>文章后的问题一段属于同一主题 是不是可以在生成这段字时候公用生成主题的theta参数值 就相当于同一段落同一主题了</div>2018-03-27</li><br/>
</ul>