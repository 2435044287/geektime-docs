在信息检索和文本挖掘领域，我们之前已经讲过了TF-IDF算法和BM25算法。TF-IDF因其简单和实用常常成为很多信息检索任务的第一选择，BM25则以其坚实的经验公式成了很多工业界实际系统的重要基石。

然而，在信息检索研究者的心里，一直都在寻找一种既容易解释，又能自由扩展，并且在实际使用中效果显著的检索模型。这种情况一直到20世纪90年代末、21世纪初才得到了突破，一种叫“语言模型”（Language Model）的新模型得到发展。其后10多年的时间里，以语言模型为基础的各类变种可谓层出不穷，成了信息检索和搜索领域的重要研究方向。

今天我就来谈谈语言模型的历史，算法细节和语言模型的重要变种，帮助初学者快速掌握这一模型。

## 语言模型的历史

语言模型在信息检索中的应用开始于1998年的SIGIR大会（International ACM SIGIR Conference on Research and Development in Information Retrieval，国际信息检索大会）。来自马萨诸塞州大学阿姆赫斯特分校（UMass Amherst）的信息检索学者杰·庞特（Jay M. Ponte）和布鲁斯·夸夫特（W. Bruce Croft）发表了第一篇应用语言模型的论文，从此开启了一个新的时代。

布鲁斯是信息检索的学术权威。早年他在英国的剑桥大学获得博士学位，之后一直在马萨诸塞州大学阿姆赫斯特分校任教。他于2003年获得美国计算机协会ACM颁发的“杰拉德·索尔顿奖”，表彰他在信息检索领域所作出的突出贡献。另外，布鲁斯也是ACM院士。

从那篇论文发表之后，华人学者翟成祥对于语言模型的贡献也是当仁不让。他的博士论文就是系统性论述语言模型的平滑技术以及各类语言模型的深刻理论内涵。

翟成祥来自中国的南京大学计算机系，并于1984年、1987年和1990年分别获得南京大学的学士、硕士和博士学位，2002年他从美国卡内基梅隆大学计算机系的语言与信息技术研究所获得另外一个博士学位。

翟成祥曾经获得过2004年的美国国家科学基金会职业生涯奖（NSF CAREER Award）和2004年ACM SIGIR最佳论文奖。另外，2004年翟成祥还获得了著名的美国总统奖（PECASE，Presidential Early Career Award for Scientists and Engineers）。

## 语言模型详解

**语言模型的核心思想是希望用概率模型（Probabilistic Model）来描述查询关键字和目标文档之间的关系**。语言模型有很多的类型，最简单的、也是最基础的叫做“**查询关键字似然检索模型**”（Query Likelihood Retrieval Model）。下面我就来聊一聊这个模型的一些细节。
<div><strong>精选留言（10）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/4f/1c/abb7bfe3.jpg" width="30px"><span>沉</span> 👍（1） 💬（1）<div>Topic modelling 常用的 LDA 是不是就是基是于语言模型发展而来的呢？
</div>2017-11-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/66/a6/599551bd.jpg" width="30px"><span>Qi</span> 👍（12） 💬（0）<div>这样全用文字讲算法有点似懂非懂，建议举点例子。</div>2018-12-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/26/55/e72a671e.jpg" width="30px"><span>rookie</span> 👍（4） 💬（0）<div>EM算法、变分推断、MCMC等</div>2019-05-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/5c/c5/1231d633.jpg" width="30px"><span>梁中华</span> 👍（3） 💬（0）<div>感觉有点抽象，对没有基础和背景知识的同学理解起来很累，每句话都懂，但串起来还是把握不了这个知识点</div>2018-11-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1a/37/b4/c3354997.jpg" width="30px"><span>巧克力芭菲</span> 👍（1） 💬（0）<div>整体看下来。。。讲了等于没讲，太上层概括了，出去跟人聊天用得到。。。</div>2022-11-07</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLN7k1Sg2m5LXia9wiay1S5APdN9HcTfPHTjibrsiaNYiaTff0kUVbnWLgE66r9px1JeRv06lLB8QLJVCg/132" width="30px"><span>胡杰</span> 👍（0） 💬（0）<div>概率分布函数没有解析解，是否表示求解过程就是一个非多项式复杂度的计算？如果是这样那么就是一个NP完备问题，用神经网络去逼近的话应该也求不出吧？</div>2020-03-19</li><br/><li><img src="" width="30px"><span>hiee</span> 👍（0） 💬（0）<div>如果能将理论和实践结合起来，这些理论如何解决当前搜索遇到的常见问题，这样会让我们更受益</div>2019-08-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/6c/9f/0343d633.jpg" width="30px"><span>黄德平</span> 👍（0） 💬（0）<div>可以用神经网络代替解析形式，通过神经网络的参数拟合这些&quot;参数&quot;，梯度下降方法就可以派上用场了</div>2018-12-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/9d/e3/9ebf3856.jpg" width="30px"><span>淳韵</span> 👍（0） 💬（0）<div>没有解析解，就得用梯度下降等优化方法逼近吧？</div>2018-10-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/9d/e3/9ebf3856.jpg" width="30px"><span>淳韵</span> 👍（0） 💬（0）<div>没有解析解，就得用梯度下降等优化方法逼近吧？</div>2018-10-11</li><br/>
</ul>