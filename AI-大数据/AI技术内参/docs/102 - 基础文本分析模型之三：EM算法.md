周一我们分享的模型是“概率隐语义分析”（Probabilistic Latent Semantic Indexing），或者简称为PLSA，这类模型有效地弥补了隐语义分析的不足，在LDA兴起之前，成为了有力的文本分析工具。

不管是PLSA，还是LDA，其模型的训练过程都直接或者间接地依赖一个算法，这个算法叫作“**期望最大化”**（Expectation Maximization），或简称为 **EM算法**。实际上，EM算法是针对隐参数模型（Latent Variable Model）最直接有效的训练方法之一。既然这些模型都需要EM算法，我们今天就来谈一谈这个算法的一些核心思想。

## EM和MLE的关系

EM算法深深根植于一种更加传统的统计参数方法：**最大似然估计**（Maximum Likelihood Estimation），有时候简称为 **MLE**。**绝大多数的机器学习都可以表达成为某种概率模型的MLE求解过程**。

具体来说，MLE是这样构造的。首先，我们通过概率模型写出当前数据的“似然表达”。所谓的“似然”表达，其实也就是在当前模型的参数值的情况下，看整个数据出现的可能性有多少。可能性越低，表明参数越无法解释当前的数据。反之，如果可能性非常高，则表明参数可以比较准确地解释当前的数据。因此，**MLE的思想其实就是找到一组参数的取值，使其可以最好地解释现在的数据**。

针对某一个模型写出这个MLE以后，就是一个具体的式子，然后看我们能否找到这个式子最大值下的参数取值。这个时候，整个问题往往就已经变成了一个优化问题。从优化的角度来说，那就是针对参数求导，然后尝试把整个式子置零，从而求出在这个时候的参数值。

对绝大多数相对比较简单的模型来说，我们都可以根据这个流程求出参数的取值。比如，我们熟悉的利用高斯分布来对数据进行建模，其实就可以通过MLE的形式，写出用高斯建模的似然表达式，然后通过求解最优函数解的方式得到最佳的参数表达。而正好，这个最优的参数就是样本的均值和样本的方差。

然而，并不是所有的MLE表达都能够得到一个“**解析解**”（Closed Form Solution），有不少的模型甚至无法优化MLE的表达式，那么这个时候，我们就需要一个新的工具来求解MLE。

**EM算法的提出就是为了简化那些求解相对比较困难模型的MLE解。**

有一点需要说明的是，EM算法并不能直接求到MLE，而只能提供一种近似。多数无法直接求解的MLE问题都属于**非凸（Non-Convex）问题**。因此，**EM能够提供的仅仅是一个局部的最优解，而不是全局的最优解**。

## EM算法的核心思想

理解了EM和MLE的关系后，我们来看一看EM的一些核心思想。因为EM算法是技术性比较强的算法，我建议你一定要亲自去推演公式，从而能够真正理解算法的精髓。我们在这里主要提供一种大体的思路。
<div><strong>精选留言（3）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/11/8d/11/93f6df95.jpg" width="30px"><span>马勇(Daniel)</span> 👍（1） 💬（0）<div>最好有公式</div>2018-10-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c1/a7/5e66d331.jpg" width="30px"><span>林彦</span> 👍（0） 💬（0）<div>EM算法是不是有收敛速度慢，每一步的计算比较复杂的问题？</div>2018-04-27</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erFY9H3mxyTpf4BGmaWZvUdJkE9Jicm7b4082qg6vEqtiau3cQ5TW7HRI3asTocw7oWTLyvAsXIiaJjg/132" width="30px"><span>罗马工匠</span> 👍（0） 💬（0）<div>还是有公式好理解一点。另外问题的答案能否放评论区呢？em算法除了局部最优，还有其他问题么？</div>2018-04-25</li><br/>
</ul>