我在之前几周的专栏文章里主要讲解了最经典的信息检索（Information Retrieval）技术和基于机器学习的排序学习算法（Learning to Rank），以及如何对查询关键字（Query）进行理解，包括查询关键字分类、查询关键字解析以及查询关键字扩展。这些经典的技术是2000年后开始流行的各类搜索引擎的核心技术。

在进一步介绍更多的搜索引擎技术前，我觉得有必要专门抽出一周时间，来好好地看一下搜索系统的评测（Evaluation）以及我们经常使用的各类指标（Metric）。俗话说得好，“如果你不能衡量它，你就不能改进它”（If You Can't Measure It, You Can't Improve It）。意思其实就是说，对待一个系统，如果我们无法去衡量这个系统的好坏，没有相应的评测指标，那就很难真正地去琢磨怎么改进这些指标，从而达到提升系统的目的。

虽然我们这里是在搜索系统这个重要场景中讨论评测和指标，但实际上我们这周要讨论的很多细节都可以应用到很多类似的场景。比如，我们后面要讨论的推荐系统、广告系统等，在这些场景中几乎就可以无缝地使用这周要讲的很多内容。

## 线下评测

假设你今天开发了一个新软件，比如说是一个最新的手机软件，你怎么知道你的用户是不是喜欢你的软件呢？你怎么知道你的用户是不是愿意为你的软件掏钱呢？

**评测的核心其实就是了解用户的喜好**。最直接的方法，当然是直接询问用户来获得反馈。例如你可以对每一个下载了你手机软件的用户强行进行问卷调查，询问他们对待新软件的态度。

然而，我们很快就会发现这样的方法是行不通的。姑且不说用户是否会因为这样强行的方式产生反感，我们是不是能通过这些调查问卷获得用户的真实反馈，这本身就是一个问题。这里面涉及到调查问卷设计的科学性问题。

即便这些调查问卷都能完整准确地反映出用户对手机软件的看法，真正实施起来也会面临种种困难。如果这款手机软件的用户数量有百万甚至千万，那我们就要进行如此大规模的问卷调查，还要处理调查后的数据，显然这样做的工作量非常大。而这些调查问卷是没法反复使用的，因为下一个版本的软件更新后，用户的态度就会发生改变，这样的方式就没法系统地来帮助软件迭代。

那么如何才能形成一组数据来帮助系统反复迭代，并且还能够减少人工成本，这就成了一个核心问题。

在信息检索系统开发的早年，研究人员和工程师们就意识到了这个核心问题的重要性。英国人赛利尔·克莱温顿（Cyril Cleverdon）可以算是最早开发线下测试集的计算机科学家。

赛利尔生于1914年，在英国的布里斯托（Bristol）图书馆工作了很长时间。从1950年开始，赛利尔就致力于开发信息检索系统，以提高图书馆查询的效率。1953年他尝试建立了一个小型的测试数据集，用于检测图书管理员查找文档的快慢。这个工作最早发表于1955年的一篇论文（参考文献\[1]）。

这之后，英美的一些早期信息检索系统的研发都开始顺应这个思路，那就是为了比较多个系统，首先构造一个线下的测试数据集，然后利用这个测试集对现有的系统反复进行改进和提升。如果你想对早期测试集的构造以及信息有所了解，建议阅读文末的参考文献\[2]。

那么，当时构造的这些测试数据集有些什么特点呢？
<div><strong>精选留言（7）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/8e/c7/3ad139d6.jpg" width="30px"><span>黑翼天佑</span> 👍（2） 💬（1）<div>可以通过选择不同的阈值来画出roc曲线，然后计算auc来评价</div>2017-12-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/9b/28/fec0aaf4.jpg" width="30px"><span>老敖</span> 👍（0） 💬（1）<div>在计算召回或者精确率的时候，不是只计算文档个数，而是把它变成一个浮点数，把排序的相关性也考虑进去。就是相关性大的，查出来一篇顶两篇。是这样吗？</div>2017-12-04</li><br/><li><img src="" width="30px"><span>xinfeng1i</span> 👍（2） 💬（0）<div>依次查看top1,top3,top5等不同提取数量的精度和召回，可以估计排序的好坏。</div>2019-05-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1e/40/b0/fe2802ef.jpg" width="30px"><span>BigMoyan</span> 👍（0） 💬（0）<div>抓个虫，如果我们返回所有的文档，那召回是1，精度则是相关文档在全集中的比例，应该笔误了。</div>2021-03-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/29/8d/abb7bfe3.jpg" width="30px"><span>牧云</span> 👍（0） 💬（0）<div>通过用户对搜索结果的选择，选择与不选择，构建二分，来判断排序的好坏</div>2019-03-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/fe/4f/3a7b8033.jpg" width="30px"><span>幻大米</span> 👍（0） 💬（1）<div>「如果我们返回所有的文档，“精度”和“召回”都将成为 1」精度不是 1 吧？</div>2018-12-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/80/6e/7f78292e.jpg" width="30px"><span>无</span> 👍（0） 💬（0）<div>您好！请问如果正负样本比差别很大，比如1&#47;1000的情况下，上述这些指标以及AUC是否就不准导致无法作为参考了？如果这样，应该如何应对正负样本比悬殊时模型评估的问题呢？ 谢谢！</div>2018-03-03</li><br/>
</ul>