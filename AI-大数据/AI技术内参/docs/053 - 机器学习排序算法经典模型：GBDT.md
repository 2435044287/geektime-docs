这周我们讨论机器学习排序算法中几个经典的模型，周一分享了配对法排序中的一个经典算法，即排序支持向量机（RankSVM），这个算法的核心思想是把支持向量机应用到有序数据中，试图对数据间的顺序进行直接建模。

今天，我们来聊一聊利用机器学习进行排序的一个重要算法：**“梯度增强决策树”（Gradient Boosted Decision Tree）**。长期以来，包括雅虎在内的很多商业搜索引擎都利用这种算法作为排序算法。

## 梯度增强决策树的历史

梯度回归决策树的思想来源于两个地方。首先是“**增强算法**”（Boosting），一种试图用弱学习器提升为强学习器的算法。这种算法中比较成熟的、有代表性的算法是由罗伯特⋅施派尔（Robert Schapire）和约阿夫⋅福伦德（Yoav Freund）所提出的**AdaBoost算法**\[1]。因为这个算法两人于2003年获得理论计算机界的重要奖项“哥德尔奖”（Gödel Prize）。罗伯特之前在普林斯顿大学任计算机系教授，目前在微软研究院的纽约实验室工作。约阿夫一直在加州大学圣地亚哥分校任计算机系教授。

增强算法的工作机制都比较类似，那就是先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器。如此重复进行，直到基学习器数目达到事先制定的值，最终将所有的基学习器进行加权结合。如果你对“偏差-方差分解”（Bias-Variance Decomposition）有耳闻的话，那么，Boosting主要关注降低偏差。**在实际效果中，增强算法往往能基于泛化性能相当弱的学习器构建出很强的集成结果**。

AdaBoost提出后不久，机器学习学者和统计学家杰罗姆⋅弗赖德曼（Jerome H. Friedman）等人发表了一篇论文\[2]，**从“统计视角”解释AdaBoost实质上是基于加性模型（Additive Model）以类似牛顿迭代法来优化指数损失函数（Loss Function）**。于是受此启发，杰米姆提出了“**梯度增强**”（Gradient Boosting）的想法。这也就是梯度回归决策树思想来源的第二个地方，也是直接根源。如果你希望对“梯度增强”有进一步的了解，可以见参考文献\[3]。

最早把“梯度增强”的想法应用到搜索中，是雅虎研究院的学者于2007年左右提出的\[4]&amp;\[5]。之后，Facebook把梯度增强决策树应用于新闻推荐中\[6]。

## 梯度增强的思想核心

我们刚才简单讲了增强算法的思路，那么要想理解梯度增强决策树，就必须理解梯度增强的想法。

**梯度增强首先还是增强算法的一个扩展，也是希望能用一系列的弱学习器来达到一个强学习器的效果，从而逼近目标变量的值，也就是我们常说的标签值**。而根据加性模型的假设，这种逼近效果是这些弱学习器的一个加权平均。也就是说，最终的预测效果，是所有单个弱学习器的一个平均效果，只不过这个平均不是简单的平均，而是一个加权的效果。

那么如何来构造这些弱学习器和加权平均的权重呢？
<div><strong>精选留言（2）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/6c/9f/0343d633.jpg" width="30px"><span>黄德平</span> 👍（0） 💬（0）<div>残差网络估计是受到GBDT的启发</div>2018-12-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/68/5a/4e7754d3.jpg" width="30px"><span>范深</span> 👍（0） 💬（0）<div>神经网络与增强梯度最简单的结合，就是把多个神经网络作为弱分类器串联起来？我相信还有更妙的结合点：）</div>2018-01-17</li><br/>
</ul>