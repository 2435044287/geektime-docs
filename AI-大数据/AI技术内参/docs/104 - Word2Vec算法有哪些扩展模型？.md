从上一期的分享开始，我们进入到文本分析的另外一个环节，那就是介绍一个最近几年兴起的重要文本模型，Word2Vec。这个模型对文本挖掘、自然语言处理等很多领域都有重要影响。我们讨论了Word2Vec模型的基本假设，主要是如何从离散的词包输入获得连续的词的表达，以及如何能够利用上下文从而学习到词的隐含特性。我们还聊了两个Word2Vec模型，SG（SkipGram）模型和CBOW（Continuous-Bag-of-Word）模型，讨论了它们都有什么特性以及如何实现。

今天，我们就来看一看**Word2Vec的一些扩展模型**。

## Word2Vec的扩展思路

在列举几个比较知名的Word2Vec扩展模型之前，我们首先来看看这个模型怎么进行扩展。

首先，我们来回忆一下Word2Vec的一个基本的性质，那就是这是一个语言模型。而语言模型本身其实是一个**离散分布模型**。我们一起来想一想，什么是语言模型？语言模型就是针对某一个词库（这里其实就是一个语言的所有单词），然后在某种语境下，产生下一个单词的模型。也就是说，语言模型是一个**产生式模型**，而且这个产生式模型是产生单词这一离散数据的。

既然是这样，如果我们更改这个词库，变成任何的离散数据，那么，Word2Vec这个模型依然能够输出在新词库下的离散数据。比如，如果我们把词汇库从英语单词换成物品的下标，那Word2Vec就变成了一个对物品的序列进行建模的工具。**这其实就是扩展Word2Vec的一大思路，那就是如何把Word2Vec应用到其他的离散数据上**。

**扩展Word2Vec的第二大思路，则是从Word2Vec的另外一个特性入手：上下文的语境信息**。我们在之前的介绍中也讲过，这个上下文信息是Word2Vec成功的一个关键因素，因为这样就使得我们学习到的词向量能够表达上下文的关联所带来的语义信息。这也是传统的主题模型（Topic Model）例如LDA或者PLSA所不具备的。那么，我们能不能对这个上下文进行更换，从而使得Word2Vec能够产生完全不一样的词向量呢？答案是肯定的，这也是Word2Vec扩展的重要思路。

除此以外，还有一个重要的分支，那就是很多研究者都希望往Word2Vec里增加更多的信息，比如文档本身的信息，段落的信息以及其他的辅助信息。**如何能够让Word2Vec对更多信息建模也是一个重要的扩展思路**。

## Word2Vec的三个扩展
<div><strong>精选留言（1）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/18/8f/98/7d1287d9.jpg" width="30px"><span>韩 * *</span> 👍（0） 💬（0）<div>能想到的一个就是将数据离散化，并同时用出不同的离散粒度来捕捉不同层级的内在联系，最终将结果拼接起来。反过来想，如果是用户id等有规律增长离散数据，类似将前n位分别做嵌入再合并是否也会有些收益？</div>2019-08-05</li><br/>
</ul>