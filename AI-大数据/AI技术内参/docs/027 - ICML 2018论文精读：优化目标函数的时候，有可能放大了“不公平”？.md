今天我们要分享的是ICML 2018的一篇最佳论文提名，题目是Fairness Without Demographics in Repeated Loss Minimization。

这篇论文讨论了这样一个话题，在优化目标函数的时候，如何能够做到针对不同的子群体，准确率是相当的，从而避免优化的过程中过分重视多数群体。这篇论文的作者都来自斯坦福大学。

## 论文的主要贡献

这篇论文其实也是希望讨论算法带来的“公平性”问题，但是出发的角度和我们上一篇讨论公平性的论文非常不一样。这篇论文的核心思想，是希望通过机器学习目标函数优化的原理，来讨论机器学习和公平性的关系。

作者们发现，基于“平均损失”（Average Loss）优化的机器学习算法，常常会给某一些少数群体带来巨大的不准确性。这其实并不是模型本身的问题，而是优化的目标函数的问题。在这样的情况下，目标函数主要是关注有较多数据的群体，保证这些群体的损失最小化，而可能忽略了在数量上不占优势的少数群体。

在此基础上，还带来了另外一个用户“**留存度**”（Retention）的问题。因为少数群体忍受了比较大的优化损失，因此这些群体有可能离开或者被这个系统剔除。所以，长期下去，少数群体的数目就可能逐渐变少。这也许是目标函数的设计者们无从想到的一个平均损失函数的副产品。作者们还把这个现象命名为“**不公平的放大**”（Disparity Amplification）。
<div><strong>精选留言（2）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/fe/4f/3a7b8033.jpg" width="30px"><span>幻大米</span> 👍（0） 💬（0）<div>没看原始论文前会有些疑问：关照了少数人群，多数人群会不会有损失呢？损失大于少数人群的提升吗？</div>2018-08-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/28/f9/7faaa517.jpg" width="30px"><span>刘洋</span> 👍（0） 💬（0）<div>通过对少数群体补充训练样本的方式，采用erm方式来进行优化，应该也可以吧？</div>2018-08-09</li><br/>
</ul>