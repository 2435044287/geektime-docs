你好，我是金伟。

上节课说的数字孪生的数据质量好坏判断，是一个典型的符合预期/不符合预期的二值评估场景，那在大模型最核心的结果测评步骤里，基于私有task结合评估专家的评估则可以转为一个典型的二值倾向性评估。这个过程如何实现完全自动化呢？这就是这节课的主要内容了。

我们知道，一个DPO的倾向性评估意味着需要在两个输出之间选择倾向性，那对大模型结果而言，可以用前后两个版本的大模型输出比较，也可以在大模型输出和人类答案之间比较。这个问题的核心还是在测评算法的选择上。

## 评分算法

我们先来看一下不同的评估方法的核心原理。之前的课程提到，传统的大模型评估主要依赖人工评估的评分，人工评估就是对大模型训练完成后形成输出做出好和不好的打分，具体分值是1分和0分。

如果是评估专家来做评估，则是大模型替代人类评估判断，得出评分1还是0。私有task传统的方式是用语义距离等算法来判断评分 1 还是 0，如果将语义距离算法改为评估专家算法，还是判断评分1还是0。

这里的评分1就是正反馈，评分0就是负反馈。

![图片](https://static001.geekbang.org/resource/image/90/d7/904bdf12fb0b9300c51c9aec13caccd7.png?wh=1920x710)

当然，具体项目里的大模型评估要根据实际需求来做评分算法，也不都是1和0这样的二值评分。比如我们要微调的大模型能力是对文章的分类，那大模型的输出是一个明确的值，可以用KTO类的输出判断方法，也就是输出符合预期/不符合预期。