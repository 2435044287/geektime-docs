你好，我是金伟。

总体而言，大模型测评才是整个项目的核心。基于电商客服对问答准确性的要求，只有发布之前测试充分，才能保证发布后不出现bug。问题是，怎么才能通过测评保证电商客服模型的准确性呢？

很多人可能会想，直接用程序自动测试？或者用其他能力更强的大模型来测评？**我的实际经验表明，最重要的是人工测评。**

在真实电商客服项目中，我们采用人工测评+自动化测评结合的方案。

我们先从测评和训练之间的关系开始说。

## 测评过程

首先需要注意的是，为了保证测评效果，测评数据集和训练数据集要完全隔离。真实项目里，测评分为3类测评，分别是人工测评，通用测评和私有task测评。

测评和训练的关系不是先训练再测评，而是**在模型训练过程中分别评分，随时根据结果调整模型训练过程。**

![图片](https://static001.geekbang.org/resource/image/90/21/9092743f0a6b83dd37a41c69c7aa8c21.png?wh=1920x1117)

如果这3个测评非要做一个重要性排序的话，那么人工测评其实是最重要的。人工测评的方法是**将智能客服项目最关心的能力设计为一系列问题，把问题和期望的回答写到Excel表格中，由人工挨个根据微调后的大模型进行评分。**

![图片](https://static001.geekbang.org/resource/image/70/95/70b0abb8e5bf9f6757df43dcb1073595.png?wh=1920x804)

需要注意，在智能客服上线之后遇到的新问题也要补充到表格里。

如果说人工测评是测试大模型微调后有多少新能力，那么**通用测试集测评则是保证大模型基础和通用能力不下降。**通用测试集一般是公开的，我们只要用测试集对微调后的大模型测评，结果和原有大模型的评分对比即可。
<div><strong>精选留言（1）</strong></div><ul>
<li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/I38GzuHkWp5LEEEdkBgbfM1ctuX23oiayhsJ0xbcHUotNUkSmpppk7mRVzhWxG8m67T71YA0kDzVeKTYmibp926w/132" width="30px"><span>Geek_a7be42</span> 👍（0） 💬（2）<div>真的会有电商用AI客服吗，感觉也太复杂了，市场变化如果太快的话且不是白搞了</div>2024-09-16</li><br/>
</ul>