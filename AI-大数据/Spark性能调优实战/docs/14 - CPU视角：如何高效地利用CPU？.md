你好，我是吴磊。

在日常的开发与调优工作中，总有同学向我抱怨：“为什么我的应用CPU利用率这么低？偌大的集群，CPU利用率才10%！”确实，较低的CPU利用率不仅对宝贵的硬件资源来说是一种非常大的浪费，也会让应用端到端的执行性能很难达到令人满意的效果。那么，在分布式应用开发中，我们到底该如何高效地利用CPU？

我们说过，性能调优的最终目的，是在**所有参与计算的硬件资源之间寻求协同与平衡**，让硬件资源达到一种平衡、无瓶颈的状态。对于CPU来说，最需要协同和平衡的硬件资源非内存莫属。原因主要有两方面：一方面，在处理延迟方面，只有内存能望其项背；另一方面，在主板上内存通过数据总线直接向CPU寄存器供给数据。因此，理顺它们之间的关系，可以为性能调优奠定更好的基础。

那么，今天这一讲，我们就从硬件资源平衡的角度入手，去分析CPU与内存到底该如何合作。

## CPU与内存的平衡本质上是什么？

我们知道，Spark将内存分成了Execution Memory和Storage Memory两类，分别用于分布式任务执行和RDD缓存。其中，RDD缓存虽然最终占用的是Storage Memory，但在RDD展开（Unroll）之前，计算任务消耗的还是Execution Memory。**因此，Spark中CPU与内存的平衡，其实就是CPU与执行内存之间的协同与配比。**
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132" width="30px"><span>Geek_d794f8</span> 👍（28） 💬（6）<div>老师，在考虑并行度,内存,线程数三者之间的平衡时，spark.sql.shuffle.partitions的值是shuffle的reducer阶段的并行度，那么对于从数据源读取(比如读hive表)这个起始的map阶段的并行度是否需要考虑？这个阶段spark底层有某种默认的切片规则吗，需要在代码中人为的干预吗(比如coalesce)？ 我使用的是DataFrame和DataSet的api。</div>2021-04-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/a2/4b/b72f724f.jpg" width="30px"><span>zxk</span> 👍（16） 💬（3）<div>问题一：并发度决定了数据分片的大小：
- 在每个线程都分配到了最大内存，即 M&#47;N 的内存时，如果 task 还需要更多的内存，那么就会发生 OOM。
- 在每个线程都分配到了最少内存，即 M&#47;2N的内存时，如果 task 还需要更多的内存，此时又没有其他线程释放内存供其使用，那么也会导致OOM。</div>2021-04-17</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIibtTdDicknUHXgxmEAs8ib88kGhcjOzeHo5GBs6RfFHjGypRicKYcMPFZ5dm5edlpqALoibPK90icuwwQ/132" width="30px"><span>qconljk</span> 👍（9） 💬（5）<div>首先，在一个 Executor 中，每个 CPU 线程能够申请到的内存比例是有上下限的，最高不超过 1&#47;N，最低不少于 1&#47;N&#47;2，其中 N 代表线程池大小。这个除以2，2代表什么？</div>2021-04-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg" width="30px"><span>斯盖丸</span> 👍（9） 💬（1）<div>这讲看得有些迷，想请问下老师如何从UI角度看出来我任务的并行度是否合适呢？</div>2021-04-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/11/20/9f31c4f4.jpg" width="30px"><span>wow_xiaodi</span> 👍（8） 💬（1）<div>最佳并行度 P，计算方法是让数据分片的平均大小 D&#47;P 坐落在（M&#47;N&#47;2, M&#47;N）区间
这里很不解为何内存的大小和分片大小有直接联系，无论是计算过程还是shuffle过程，都是用到一些内存占用较小的数据结构去做的，就算内存不够用，也会有gc去保证。这个公式感觉就是让数据分片大小和执行内存等价了，让所有数据都在待在内存中一次性批处理，而不是处理一部分溢出落盘再继续处理？请老师指正</div>2021-08-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1f/9b/3b/dc3f819f.jpg" width="30px"><span>小灵芝</span> 👍（6） 💬（3）<div>“在给定执行内存 M、线程池大小 N 和数据总量 D 的时候，想要有效地提升 CPU 利用率，我们就要计算出最佳并行度 P，计算方法是让数据分片的平均大小 D&#47;P 坐落在（M&#47;N&#47;2, M&#47;N）区间。这样，在运行时，我们的 CPU 利用率往往不会太差。”

请问老师，这里的M, N， D 都是针对一个executor而言的对吧？</div>2021-04-30</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132" width="30px"><span>Geek_d794f8</span> 👍（5） 💬（2）<div>我的理解是每个线程申请的内存上限是M&#47;N,那么当数据分片过少，某个task需要处理的数据量较大，M&#47;N的上限执行内存也不够时，就会出现OOM。
不知道这么理解对不对？</div>2021-04-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/a3/f2/ab8c5183.jpg" width="30px"><span>Sampson</span> 👍（2） 💬（2）<div>老师您好，请教一下，在上文中有提到spill机制可以保护oom 的溢出，这个是怎么判断的呢 </div>2022-01-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg" width="30px"><span>斯盖丸</span> 👍（2） 💬（3）<div>老师，顺着王天雨同学的问题我接着问。我的executor memory为9G，executor的off heap memory也为9G，executor cores为5个，executor instances为30个。
以上是我的配置。
照您的公式，我的数据分片的大小就应该在(9G+9G)&#47;5&#47;2=1.8G到(9G+9G)&#47;5=3.6，即数据分片在（1.8G，3.6G）的范围内吗，那进一步说，我在Spark UI里，找到第一个读取parquet的任务，看shuffle read size这个指标，如果在（1.8G，3.6G）这个区间之内，说明就是可以的，是这样吗?

感觉这个分片好大哦~~</div>2021-05-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/20/d6/b9513db0.jpg" width="30px"><span>kingcall</span> 👍（2） 💬（3）<div>M 的下限是 Execution Memory 初始值，上限是 spark.executor.memory * spark.memory.fraction 划定的所有内存区域。这个老师笔误，写错了吧！</div>2021-04-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg" width="30px"><span>Unknown element</span> 👍（1） 💬（1）<div>老师您好 spark在计算过程中会输出类似这样的日志：
[Stage 1:==================================================&gt;(11765 + 6) &#47; 11769]
想问下这里的数字是 task 数吗？为什么括号里的两个task数加起来不等于外面task数呢？
另外这里的task是不是偏多？应该从哪些方面去排查问题呢？
谢谢老师</div>2021-12-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2a/39/68/56dfc8c0.jpg" width="30px"><span>子兮</span> 👍（1） 💬（1）<div>老师，有没有可能：M&#47;N =45M&#47;3core=15M， 有三个分片 5M ，12M，18M,  理论来讲，18M 的task 会出现oom，三个task 同时执行，但在执行过程中，由于5M 的task 迅速执行完成，使得内存释放，这时18M 在执行时获得45&#47;2=22.5M，没有报oom ？
2 spark graphx 在执行时每个job 所需要处理的数据量都不同，没有办法实时评估更改，这时的设置应该怎样思考呢？</div>2021-11-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1b/4e/29/adcb78e7.jpg" width="30px"><span>静心</span> 👍（1） 💬（1）<div>老师，这一讲太理论范了，能不能举一些实际的例子，供大家学习加深理解，谢谢老师</div>2021-10-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1b/4e/29/adcb78e7.jpg" width="30px"><span>静心</span> 👍（1） 💬（2）<div>老师关于cpu线程挂起，描述的是有cpu资源而没执行内存使用而挂起的情况，主要还是动态变化的M（执行内存）与动态变化N（当前使用的cores）的问题，文中提到线程申请内存的大小也是通过M和N计算的。但文中也提到和分布式数据集的数据分布也有关系，这里我不太理解，线程可使用执行内存大小是通过M和N计算的，这和分布式数据集的数据分布有什么关系呢？</div>2021-10-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/21/00/3f/a0f84788.jpg" width="30px"><span>Sean</span> 👍（1） 💬（1）<div>老师提到的从分片大小200M反推,来配置spark.sql.shuffle.partitions,spark.executor.memory,spark.executor.cores参数,不知道理解的对不对,在不考虑数据压缩的情况下,比如有1T的数据,那么按200分片考虑,最佳并行度计算按1048576M&#47;200M≈5243计算吗,set spark.sql.shuffle.partitions=5243,好像还没有get到这个资源计算公式的应用,
我结合这几个配置推导方式如下:--executor-cores 4  --executor-memory 6g  --num-executors 80
D&#47;P =&gt; (M&#47;N&#47;2,M&#47;N) =&gt; (6*80&#47;4&#47;2,6*80&#47;4) =&gt; (60G,120G) 这个配置显然不合适,修改一下参数--executor-cores 1500 得到:
D&#47;P =&gt; (M&#47;N&#47;2,M&#47;N) =&gt; (6*80&#47;1500&#47;2,6*80&#47;1500) =&gt; (164M,327M) 看着这个配置算是合理的利用cpu吗
根据这个配置得到的区间在继续推导:
如果数据总量D=1T≈1048576M,那并行度P:spark.sql.shuffle.partitions的大小在这(1048576&#47;164,1048576&#47;327)=&gt;(6394,3207)区间,感觉这个并行度好像太大,也不是很合理,不知道我理解的对不对</div>2021-08-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/a5/9e/19871ffb.jpg" width="30px"><span>Geek</span> 👍（1） 💬（3）<div>吴老师 请教2个问题哈
1、当一个源文件的大小超过了M&#47;N 的内存时，这种情况是不是会报OOM？
2、spark在加载parquet+snappy压缩文件时，它会考虑解压之后的文件大小吗？</div>2021-05-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg" width="30px"><span>西南偏北</span> 👍（1） 💬（1）<div>每个线程申请执行内存的时候都会跟所请求的内存大小进行比较。为一个内存消费者（MemoryConsumer）申请执行内存的具体实现逻辑在TaskMemoryManager#acquireExecutionMemory()方法中，这个方法为内存消费者（consumer）申请指定大小的内存空间，如果没有足够的内存，将会对consumer进行spill()来释放更多内存，具体要对哪些consumer进行spill()，会有一个排序算法（使用了TreeMap）。</div>2021-05-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1f/9b/3b/dc3f819f.jpg" width="30px"><span>小灵芝</span> 👍（1） 💬（1）<div>“并行度可以通过两个参数来设置，分别是 spark.default.parallelism 和 spark.sql.shuffle.partitions。前者用于设置 RDD 的默认并行度，后者在 Spark SQL 开发框架下，指定了 Shuffle Reduce 阶段默认的并行度。”

请问老师，这两个参数是都要设置吗？还是说在用RDD的时候设置前者，Dataframe或者Dataset的时候设置后者即可？</div>2021-04-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg" width="30px"><span>来世愿做友人 A</span> 👍（1） 💬（2）<div>1. excutor 并发度如果过高，考虑极端情况，storageMemery 还没有使用，这时候这部分内存是会被 task 所抢占的，每个任务的上限是 1&#47;N。但是对于 storage 和 execute pool 的池锁不是同一个锁。并发高的情况下，可能会出现 execute 剩余内存假设 1M，task 申请 execute 内存并且 size 申请成功，但是还没 alloc 内存。并行条件下，此时别的 task 线程刚好申请了 storage pool 的内存 0.8M 进行 block 存储，并且申请成功。然后申请 1M 的 task 线程才 allocate，却发现内存不够 OOM 了。不知道这样对不对
2. 其实在 task 每次的 allocatePage，都会动态计算 task 当先的上下限的申请大小。满足申请条件，返回。申请后可用大小加上当前已用不满足下限，则挂起等待其它 task 唤醒抢占。</div>2021-04-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/1d/22/f04cea4c.jpg" width="30px"><span>Fendora范东_</span> 👍（1） 💬（6）<div>1.当executor内所有线程都在运行，实际需要内存比预估申请内存大，这个时候执行内存又不能继续扩展，就会出现oom
2.executor内正在运行的这批任务执行完，下一批任务被执行前，就进行资源调整。根据此时执行内存大小&#47;min(待分配任务数，executor.cores)进行内存分配</div>2021-04-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/29/96/17/200c21f0.jpg" width="30px"><span>狗哭</span> 👍（0） 💬（1）<div>老师请教一个问题，spark中有这么一个配置，spark.locality.wait用来配置task数据本地化级别等待时常，默认是3s，但是发现有时候cpu利用率就很低，如果配成0发现task的数据本地化级别很多是node甚至rack级别的，请问这个该怎么平衡呢？或者有什么好的办法解决呢？</div>2021-10-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1b/4e/29/adcb78e7.jpg" width="30px"><span>静心</span> 👍（0） 💬（1）<div>老师，关于executor每个core分配到的执行内存大小区间是M&#47;N&#47;2到M&#47;N，那若在某个时刻只有一个task调度了该executor，那么这个线程申请的内存大小至少是M&#47;2吗，相当于至少申请一半的执行内存。若实际用不到这么多那不就浪费了。</div>2021-10-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/47/c4/c070afcf.jpg" width="30px"><span>何西东</span> 👍（0） 💬（1）<div>老师,200m的依据是防止任务调度滞后而导致线程挂起吗</div>2021-06-27</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/IcDlyK6DaBrssVGlmosXnahdJ4bwCesjXa98iaapSDozBiagZTqSCok6iaktu2wOibvpNv9Pd6nfwMg7N7KTSTzYRw/132" width="30px"><span>慢慢卢</span> 👍（0） 💬（1）<div>老师能讲一讲spark.executor.core的配置原则吗</div>2021-06-15</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJt9RvLXn5KxqNiccCyxRGy0IDHdqOOiazoH7aqku4GlELB4guibOGibEqPF740iaNwKoe6BjicgmgjR6Vw/132" width="30px"><span>Geek_81beba</span> 👍（0） 💬（1）<div>默认并行度和shuffle partitions哪个更优先呢</div>2021-05-10</li><br/><li><img src="" width="30px"><span>王天雨</span> 👍（0） 💬（1）<div>请问M 和 N 分别对应的是单个Executor的内存和核数，还是针对某个spark任务分配的总Executor数量和核数呢</div>2021-04-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/1d/22/f04cea4c.jpg" width="30px"><span>Fendora范东_</span> 👍（0） 💬（1）<div>磊哥，每个线程分配内存下限  我理解应该是M&#47;2N呢？</div>2021-04-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/4f/5b/2e44385d.jpg" width="30px"><span>马玲玲</span> 👍（0） 💬（0）<div>老师，您好。请问cpu 利用率一般多高属于健康状态？cpu 利用率如果持续90%以上，会有什么影响呢？有什么手段可以降低cpu利用率呢？</div>2025-02-02</li><br/><li><img src="" width="30px"><span>InfoQ_fd2f3311a294</span> 👍（0） 💬（0）<div>老师，讨论的是只有一个executor，实际肯定不止一个，如果是多个该怎么计算？</div>2024-09-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/28/55/5c/643c0e33.jpg" width="30px"><span>ShiPF</span> 👍（0） 💬（0）<div>老师，有一个问题。这里在讨论cpu利用率的时候，都是在executor_num已经确定的情况下。如果运行前没有设置executor_num， 是否会在运行时候根据内存规则，每个task没存在m&#47;n m&#47;n&#47;2这个范围，动态调整executor数量？另外设置动态资源调整，是否在运行过程中也会动态去调整executor数量，避免task被挂起的情况发生？</div>2024-04-04</li><br/>
</ul>