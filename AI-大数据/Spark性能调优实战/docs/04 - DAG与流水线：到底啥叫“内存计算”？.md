你好，我是吴磊。

在日常的开发工作中，我发现有两种现象很普遍。

第一种是缓存的滥用。无论是RDD，还是DataFrame，凡是能产生数据集的地方，开发同学一律用cache进行缓存，结果就是应用的执行性能奇差无比。开发同学也很委屈：“Spark不是内存计算的吗？为什么把数据缓存到内存里去，性能反而更差了？”

第二种现象是关于Shuffle的。我们都知道，Shuffle是Spark中的性能杀手，在开发应用时要尽可能地避免Shuffle操作。不过据我观察，很多初学者都没有足够的动力去重构代码来避免Shuffle，这些同学的想法往往是：“能把业务功能实现就不错了，费了半天劲去重写代码就算真的消除了Shuffle，能有多大的性能收益啊。”

以上这两种现象可能大多数人并不在意，但往往这些细节才决定了应用执行性能的优劣。在我看来，造成这两种现象的根本原因就在于，开发者对Spark内存计算的理解还不够透彻。所以今天，我们就来说说Spark的内存计算都有哪些含义？

## 第一层含义：分布式数据缓存

一提起Spark的“内存计算”的含义，你的第一反应很可能是：Spark允许开发者将分布式数据集缓存到计算节点的内存中，从而对其进行高效的数据访问。没错，这就是内存计算的**第一层含义：众所周知的分布式数据缓存。**
<div><strong>精选留言（24）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/12/07/a6/662bcc6b.jpg" width="30px"><span>来世愿做友人 A</span> 👍（44） 💬（8）<div>问题1: rdd 会有 dep 属性，用来区分是否是 shuffle 生成的 rdd. 而 dep 属性的确定主要是根据子 rdd 是否依赖父 rdd 的某一部分数据，这个就得看他两的分区器(如果 tranf&#47;action 有的话)。如果分区器一致，就不会产生 shuffle。
问题2: 在 task 启动后，会调用 rdd iterator 进行算子链的递归生成，调用 stage 图中最后一个 rdd 的 compute 方法，一般如果是 spark 提供的 rdd，compute 函数大都会继续调用父 rdd 的 iterator 方法，直到到 stage 的根 rdd，一般都是 sourceRdd，比如 hadoopRdd，KakaRdd，就会返回 source iterator。开始返回，如果子rdd 是 map 转换的，就会组成 itr.map(f)。如果再下一个是 filter 转换，就会组成 itr.map(f1).filter(f2)，以此类推。不知道这边理解对不对，有点绕</div>2021-03-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/47/ba/d36340c1.jpg" width="30px"><span>Shockang</span> 👍（41） 💬（6）<div>正如老师在文章里面提到的一样，Hadoop MapReduce使用硬盘来存储中间结果，而 Spark自从诞生以来就一直标榜自己是内存计算，可能有些同学会比较奇怪，为什么内存明显比硬盘快，MR 不去选择内存计算，实际上 MR 也有在使用内存的，比如环形缓冲区的存在就可以说明，之所以这样做，一个很重要的原因是 MR 诞生的年代（04 年）内存比较贵，后来随着科技发展，内存价格在不断下降，大家如果仔细研究就会发现比如 Spark 比如 Redis 等充分利用内存来计算的框架都是 10 年左右出现的，就是在这个时候内存价格开始大幅度下降的。我之所以说这么多，其实就想说明，事物的发展都是有规律的，大数据的背后也潜藏着各种规律，把握好这些规律，个人认为对于理解记忆各种不同的大数据技术都是很有帮助的。</div>2021-03-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg" width="30px"><span>西南偏北</span> 👍（8） 💬（1）<div>1.  DAG以Shuffle划分Stages，Shuffle的产生主要通过宽依赖和窄依赖，而宽窄依赖主要通过不同的算子来产生，比如产生窄依赖的算子：map，flatMap，filter，mapPartitions，union；产生宽依赖的算子：cogroup，join，groupyByKey，reduceByKey，combineByKey，distinct，repartition
2. 官网上看到过：WholeStageCodegen 全阶段代码生成将多个operators编译成一个Java函数来提升性能。</div>2021-05-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/25/0b/aa/09c1215f.jpg" width="30px"><span>Sansi</span> 👍（8） 💬（8）<div>内存计算的第二层含义真的算内存计算吗，mr不是也可以把spark的多个map操作放到一个map任务吗，我认为只是在api层面spark更简单</div>2021-03-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/fe/a2/5252a278.jpg" width="30px"><span>对方正在输入。。。</span> 👍（7） 💬（3）<div>问题一：每个rdd会有个dependencies的属性，deps记录的是该rdd与父rdd之间的依赖关系，deps类型是Seq[dependency], 如果dependency类型是shuffleDenpendency，那么spark就会视其操作为shuffle操作，然后进行stage的切割。

问题二：stage执行时，spark会调用该stage末尾rdd的iterator方法，然后iterator方法实现逻辑是：将该rdd的compute方法作用下父rdd的compute计算结果之上，从而得到该rdd的分区</div>2021-03-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/42/f3/e945e4ac.jpg" width="30px"><span>sparkjoy</span> 👍（6） 💬（1）<div>第一题，主要看父rdd的分区器是否一致，如果一致则生成子rdd的过程中不会产生shuffle</div>2021-07-13</li><br/><li><img src="" width="30px"><span>Geek_18fe90</span> 👍（5） 💬（1）<div>spark shuffle前后的分区数是如何计算的</div>2021-12-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/ea/20/78ab5f92.jpg" width="30px"><span>小学生敬亭山</span> 👍（5） 💬（2）<div>老师您好，我请教个问题。既然是大数据，那么假设数据很大，无论怎么分区或者分布式，单个机器的内存都放不下，那这个时候spark是怎么计算的呢？必然会有一部分在磁盘一部分在内存吧，这种情况spark是如何避免落盘，如何提升效率的呢。</div>2021-03-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg" width="30px"><span>斯盖丸</span> 👍（4） 💬（1）<div>请问下老师，spark里cache的正确姿势是什么？
是直接df.cache()还是val cacheDf = df.cache()呢？另外不管cache还是persist都是lazy的，所以有必要紧接着一句df.count()让它马上执行吗？因为这样会平白无故多一个job，不知道是不是画蛇添足了</div>2021-04-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/cf/14/384258ba.jpg" width="30px"><span>Wiggle Wiggle</span> 👍（4） 💬（4）<div>说个最极端的情况，如果对一个dataframe Read以后做了一堆不会触发shuffle 的操作，最后又调用了一下coalesce(1)，然后write ，那是不是就意味着从读数据开始的所有操作都会在一个executor上完成？ </div>2021-04-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/1d/22/f04cea4c.jpg" width="30px"><span>Fendora范东_</span> 👍（1） 💬（1）<div>1.DAG以shuffle划分stage;  判断shuffle的依据是  rdd的deps属性是narrowDeps还是shuffleDeps;  deps类型怎么得来的，肯定是构造rdd时生成的;构造rdd时依据什么来生成不同类型的deps呢，这块还没深究，猜测是根据算子类型，比如window func或者aggregator。
2.所有算子融合到一起是通过全阶段代码生成。如果不能进行全阶段代码生成就进行基本表达式代码生成，但基本表达式代码生成每个算子处理逻辑还是分开的，所以磊哥能解释下仅仅进行基本表达式代码生成好处在哪嘛</div>2021-04-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/ea/20/78ab5f92.jpg" width="30px"><span>小学生敬亭山</span> 👍（1） 💬（1）<div>问题1:逻辑层面上，如果聚焦在当前节点，看前1个节点和当前节点的关系。存在1对1，1对多，多对1，多对多四种可能。所谓shuffle就是不能链式调用了，需要用到上一步的多个节点。可以理解成上一步的数据要交出来混在一起又重新发出去。因此发生了网络传输或者落盘。多对1和多对多可能会shuffle.代码实现层面就是有 依赖关系可以在stage回溯的时候可以用。
问题2.能融合得益于函数式编程的思想，可以链式调用，然后通过生成类似于树结构的语法分析，然后生成逻辑执行计划，物理执行计划。spark有所谓&quot;钨丝计划&quot;。然后更深入的优化内容，那我就说不清楚了。</div>2021-03-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/54/b7/abb7bfe3.jpg" width="30px"><span>Elon</span> 👍（1） 💬（1）<div>不得不说，这个土豆的例子可是太棒了～</div>2021-03-23</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/t1aR8h117KxusZQHQ9urp6hr3jMA9icnWR3tLlYZ5M1wbgXIqRTKfLHJ9iciaTgliaPhfV5s5fYrARMZySKHltMlUg/132" width="30px"><span>Gti</span> 👍（0） 💬（3）<div>map的结果不是都写到本地磁盘吗？reducer从hdfs去mapper的结果？</div>2021-03-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1e/f5/9d/104bb8ea.jpg" width="30px"><span>Geek2014</span> 👍（0） 💬（1）<div>MR可以开发者自己手动在一个map方法里整合多个算子的功能啊，只是spark做了简化。

问题1主要就是宽窄依赖的问题
</div>2021-03-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/21/63/6e/6b971571.jpg" width="30px"><span>Z宇锤锤</span> 👍（0） 💬（2）<div>是否需要进行shuffle。当RDD和父RDD的依赖关系是宽依赖是，就会进行数据的shuffle.</div>2021-03-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/d8/d5/00f31ac9.jpg" width="30px"><span>觉醒</span> 👍（0） 💬（2）<div>吴老师，请问lead 添加partition 算shuffle操作吗</div>2021-03-22</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/cojb2AA3eM620kb7hj7YoG8k56TKsdCmVletmYKYwibickH5Ced8UyxicpY9icZEM2ZTcqyUaEk2PRmH1FVLtGTggw/132" width="30px"><span>orangelin</span> 👍（0） 💬（1）<div>问题1：是根据算子得依赖关系来判断的，如果是宽依赖则会划分阶段，如果是窄依赖则不会，如何判断宽窄依赖呢？本质是看父RDD分区是否被打散到子RDD中，如果打散则为宽依赖
问题2：不是很清楚具体怎么实现，请老师解答，我的理解是首先同一个阶段的函数必然都是窄依赖关系</div>2021-03-22</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJrZb9pm07aicqH4rErIanMN5owmsBO6rWa6VSkpFMFjVRKOKkNM6JEf2gvQ2g23xUiammg7PUykJFA/132" width="30px"><span>天渡</span> 👍（0） 💬（1）<div>当RDD之间的依赖关系为宽依赖时，就会发生shuffle</div>2021-03-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1b/91/56/f714ad14.jpg" width="30px"><span>老A</span> 👍（0） 💬（4）<div>1.根据一个分区的数据是否分发到下游RDD里的多个分区来判断是否有shuffle 。
2.应该是根据作用在RDD上的计算compute，然后根据函数式编程的调用链来实现的吧，请老师解答吧😀
</div>2021-03-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/25/f1/f7a11901.jpg" width="30px"><span>杜兰特有丶小帅</span> 👍（0） 💬（0）<div>老师，TEZ好像也是将MR任务分解成算子，然后在内存里计算的。我想问一下，spark和TEZ在优化MR上面有什么不同？</div>2023-08-20</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK6Nic6V6iawbbH0UZKtNv9SOMSED5xIQjeq6wgFuia8D4HUNQYHQdn1BOTMOrDuUCddaiaV5Nmmw8RDg/132" width="30px"><span>Geek_853ebe</span> 👍（0） 💬（0）<div>路人转粉，老师怎么加群呀</div>2022-10-21</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTK6Nic6V6iawbbH0UZKtNv9SOMSED5xIQjeq6wgFuia8D4HUNQYHQdn1BOTMOrDuUCddaiaV5Nmmw8RDg/132" width="30px"><span>Geek_853ebe</span> 👍（0） 💬（0）<div>老师，请教下DataSet Api中的union一定会让数据变成一个分区吗？
在线上有个卡死的任务，十几个select union在一起，看卡住的stage中，数据倾斜到了一个task中，其他task空跑，看时间线这个job中其他的action都是并发跑的很快。</div>2022-10-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/b8/a1/019574ad.jpg" width="30px"><span>松花酿酒，春水煎茶</span> 👍（0） 💬（0）<div>问题一：分算子，根据宽窄依赖或者父子RDD partitioner来判断；</div>2022-07-09</li><br/>
</ul>