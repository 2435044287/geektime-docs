你好，我是吴磊。

我们都知道，Spark的优势在于内存计算。一提到“内存计算”，我们的第一反应都是：执行效率高！但如果听到“基于磁盘的计算”，就会觉得性能肯定好不到哪儿去。甚至有的人会想，如果Spark的内存无限大就好了，这样我们就可以把磁盘完全抛弃掉。当然，这个假设大概率不会成真，而且这种一刀切的思维也不正确。

如果内存无限大，我们确实可以通过一些手段，让Spark作业在执行的过程中免去所有的落盘动作。但是，无限大内存引入的大量Full GC停顿（Stop The World），很有可能让应用的执行性能，相比有磁盘操作的时候更差。这就不符合我们一再强调的，**调优的最终目的是在不同的硬件资源之间寻求平衡了**。

所以今天这一讲，我们就来说说磁盘在Spark任务执行的过程中都扮演哪些重要角色，它功能方面的作用，以及性能方面的价值。掌握它们可以帮助我们更合理地利用磁盘，以成本优势平衡不同硬件资源的计算负载。

## 磁盘在功能上的作用

在Spark当中，磁盘都用在哪些地方呢？在Shuffle那一讲我们说过，在Map阶段，Spark根据计算是否需要聚合，分别采用PartitionedPairBuffer和PartitionedAppendOnlyMap两种不同的内存数据结构来缓存分片中的数据记录。分布式计算往往涉及海量数据，因此这些数据结构通常都没办法装满分区中的所有数据。在内存受限的情况下，溢出机制可以保证任务的顺利执行，不会因为内存空间不足就立即报OOM异常。
<div><strong>精选留言（17）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/10/c6/09/7f2bcc6e.jpg" width="30px"><span>sky_sql</span> 👍（18） 💬（1）<div>老师好！咨询个题外话，在调度系统中讲到SchedulerBackend 用 ExecutorData 对 Executor 进行资源画像，ExecutorData中有RPC 地址、主机地址、可用 CPU 核数和满配 CPU 核数等等，但是没有内存相关的，是spark的Executor 内存这块比较复杂？
对比hadoop在调度时会考虑内存是否满足吗？</div>2021-04-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg" width="30px"><span>西南偏北</span> 👍（6） 💬（1）<div>第二题，想到有一种情况：假如原始日志数据中，某些用户的访问日志比较多，而且在调用repartition()哈希分区之后，恰好都落到了一个分区，那后续的计算可能会出现数据倾斜。其他的没想到哈哈</div>2021-05-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/a2/4b/b72f724f.jpg" width="30px"><span>zxk</span> 👍（6） 💬（2）<div>老师，如果我在 df.groupBy(&quot;userId&quot;) 后做磁盘 cache ，后续再做两个 agg 操作，是否跟 ReuseExchange 效果一样？如果一样的话，那么这时候是不是可以不用管 agg 里涉及的字段了？</div>2021-04-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/26/b5/88/477e7812.jpg" width="30px"><span>RespectM</span> 👍（5） 💬（2）<div>老师好，可以讲讲企业级Spark的机器硬件该如何选行吗，以及应该考虑哪些问题。网上都是说根据需求，有什么根据吗，能不能举个例子。</div>2021-04-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/11/20/9f31c4f4.jpg" width="30px"><span>wow_xiaodi</span> 👍（4） 💬（1）<div>老师您好，关于ReuseExchange的例子，由于计算pv uv的聚合算法是不一样的，那么在spill阶段就不可能进行相同的map combine操作。这个ReuseExchange里发生的shuffle是不是单纯只做了按&lt;partitionid, key&gt;来排序的操作，而没有进行combine操作呢？而且对于map端聚合发生在shuffle阶段后面，还紧接着reduce聚合操作，不是很符合常规mr过程，有点难理解，望老师指点迷津。</div>2021-08-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/87/4b/16ea3997.jpg" width="30px"><span>tiankonghewo</span> 👍（3） 💬（1）<div>这种例子很牵强, 正常人都是 agg中直接count和countDistinct了, 还有人分开统计吗</div>2021-11-22</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTIiaeebUYxl7e4jicPshDRKMbiculHUjKgZZ2ygDibn2S7bbsjeqYIdsEUdVyoryKNa43ZGnDQmWjv3ibQ/132" width="30px"><span>Geek_d794f8</span> 👍（3） 💬（2）<div>老师，df.groupBy(&quot;userId&quot;).agg(count(&quot;page&quot;)和df.groupBy(&quot;userId&quot;).agg(countDistinct(&quot;page&quot;)都会按照userId的hashcode值进行Hash Partitioner。而repartition($&quot;userId&quot;)，只不过是在这之前进行了Hash Partitioner的shuffle操作。本质上是一样的呀？为什么最后一个会reuseExchange，一个不会呢？</div>2021-04-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg" width="30px"><span>西南偏北</span> 👍（1） 💬（1）<div>验证了一下第一题，改成count(*)之后再打印Physical Plan发现ReuseExchange失效了，这也印证了老师说的ReuseExchange的第二个触发条件“多个查询所涉及的字段要保持一致”，这个触发条件真的有点严格哈哈</div>2021-05-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/20/d6/b9513db0.jpg" width="30px"><span>kingcall</span> 👍（1） 💬（1）<div>感觉我们只要能让算子复用shuffle 后的数据就可以做到减少shuffle,但是文中提到ReuseExchange条件太苛刻，而评论中说的缓存groupBy后的结果也就是shuffle 后的结果，供后续多个聚合统计又做不到，哈哈，这是不是spark 后续可以优化的地方
我觉得想想为啥spark 没有给RelationalGroupedDataset 添加cache操作，理论上来说shuffle 后的数据和shuffle 前的数据量是一致的，只要shuffle前的数据可以缓存下来，shuffle 后的数据也可以缓存下来,是不是考虑到了 1. shuffle 后的数据可能分布不均，导致个别partition 过大不能cache, 2. 认为shuffle 后的数据使用频率比较低，我们更多关注的是shuffle 聚合后的结果。
希望老师解答一二</div>2021-04-27</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJN8s3YnzyDRCeg73yzglRgQgk581uIY1FRFO01GibMro4Mbxk58rRgulZTKrSGnd8ZD6RHY8uQj2A/132" width="30px"><span>蠟筆小噺</span> 👍（1） 💬（2）<div>内存无限大的情况下，对象不会遇到放不下的问题，为什么会有GC呢。。。。</div>2021-04-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/1d/22/f04cea4c.jpg" width="30px"><span>Fendora范东_</span> 👍（1） 💬（3）<div>有个疑问
有reuseexchange算子的执行计划中，既然reuseexchange阶段是shuffle，shuffle后面直接就应该是reduce聚合了，为啥在这两者之间还存在map端聚合？</div>2021-04-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg" width="30px"><span>斯盖丸</span> 👍（1） 💬（1）<div>ReuseExchange的第二个条件也太苛刻了吧，难道多选级列就不行了，实用性一下子下来了…</div>2021-04-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/a3/f2/ab8c5183.jpg" width="30px"><span>Sampson</span> 👍（0） 💬（1）<div>你好，磊哥，关于ReuseExchange机制，我想问的是，在这个案例中，是否可以将读取文件之后的dataframe cache 下来，后续直接针对这个cache的dataframe操作呢？还是说2者有什么差别？</div>2022-01-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2d/1f/a2/0ac2dc38.jpg" width="30px"><span>组织灵魂 王子健</span> 👍（0） 💬（0）<div>不太理解为什么ReuseExchange没办法做到在已经满足分区的DataFrame上执行不同的操作，真的还不如直接使用persist MEMORY_AND_DISK。不过逻辑其实是一样的。</div>2022-10-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2d/1f/a2/0ac2dc38.jpg" width="30px"><span>组织灵魂 王子健</span> 👍（0） 💬（0）<div>“一旦一个transformation失败就会触发整条 DAG 从头至尾重新计算，这个说法不准确，因为，失败重复的计算源头并不是整条 DAG 的“头”，而是与触发点距离最新的 Shuffle 的中间文件“。也就是说，失败重试是从失败所在的stage的源头重新开始的，因为每个wide transformation (shuffle)和最后的action是一个Job被分为大于等于一个stages的标准</div>2022-10-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/28/01/9a/d2831441.jpg" width="30px"><span>康</span> 👍（0） 💬（0）<div>老师，我想闻一下，ReuseExchange 机制会利用在hive on spark场景嘛？感谢回复</div>2022-07-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2a/c3/4c/82b7df02.jpg" width="30px"><span>罗盖羽</span> 👍（0） 💬（0）<div>老师，关于ReuseExchange的两个例子，最后一个会reuseExchange，第一个例子为什么需要扫描两次数据源？val df: DataFrame = spark.read.parquet(filePath)不是已经扫描过了一次，后面的两次聚合不都是基于df的操作吗？
难道是 spark.read.parquet(filePath)这个操作没有action类的算子触发？</div>2022-04-20</li><br/>
</ul>