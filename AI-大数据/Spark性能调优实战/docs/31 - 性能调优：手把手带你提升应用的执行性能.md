你好，我是吴磊。

在上一讲，我们一起完成了小汽车摇号趋势分析的应用开发，解决了5个案例。今天这一讲，我们逐一对这5个案例做性能调优，一起把专栏中学过的知识和技巧应用到实战中去。

由于趋势分析应用中的案例较多，为了方便对比每一个案例调优前后的性能效果，我们先来对齐并统一性能对比测试的方法论。

首先，我们的性能对比测试是以案例为粒度的，也就是常说的Case By Case。然后，在每一个案例中，我们都有对比基准（Baseline）。**对比基准的含义是，在不采取任何调优方法的情况下，直接把代码交付执行得到的运行时间。**之后，对于每一个案例，我们会采取一种或多种调优方法做性能优化，每一种调优方法都有与之对应的运行时间。最终，我们将不同调优方法的运行时间与对比基准做横向比较，来观察调优前后的性能差异，并分析性能提升/下降的背后原因。

话不多说，我们直接开始今天的课程吧！

## 运行环境

既然调优效果主要由执行时间来体现，那在开始调优之前，我们有必要先来交代一下性能测试采用的硬件资源和配置项设置。硬件资源如下表所示。

![](https://static001.geekbang.org/resource/image/ef/07/efa374437bb4df18b80145133a1cd807.jpeg?wh=1214%2A584 "硬件资源配置")

为了避免因为实验本身而等待太长的时间，我使用了比较强悍的机器资源。实际上，为了跑通应用，完成性能对比测试，你使用笔记本也可以。而且为了给后续调优留出足够空间，除了必需的运行资源设置以外，其他配置项全部保留了默认值，具体的资源配置如下表所示。
<div><strong>精选留言（8）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/1e/f2/f5/b82f410d.jpg" width="30px"><span>Unknown element</span> 👍（9） 💬（1）<div>老师您好，问下 bypass 排序操作为什么要求计算逻辑不涉及聚合呢？reduceByKey 感觉也不需要排序啊。。</div>2022-01-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg" width="30px"><span>斯盖丸</span> 👍（6） 💬（2）<div>老师，我看到很多hdfs源的spark job，它们task建议的大小是128M，也就是一个HDFS block的大小。请问咱们这边说的一个task最佳大小是200多M是怎么来的？</div>2021-05-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/cf/e7/4c2323ba.jpg" width="30px"><span>felicity</span> 👍（1） 💬（1）<div>老师你好，这里的例子是基于spark3.0的版本讲的，已经有了aqe，但是我们公司使用的spark版本还是2系列的，短时间内也不会升级到3，像aqe这种，2系列没有，能否讲解一下针对spark2版本的优化思路，谢谢</div>2021-08-09</li><br/><li><img src="" width="30px"><span>边边爱学习</span> 👍（0） 💬（1）<div>磊哥，shuffle read blocked time 特别长，占据任务的70%以上的时间，应该尝试哪方面的优化呀</div>2021-08-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/d4/78/66b3f2a2.jpg" width="30px"><span>斯盖丸</span> 👍（0） 💬（1）<div>老师，请问数据分片200mb是最佳的，这个结论是如何得出的？因为我自己也试过几次发现确实如此，一旦分片数再大执行时间反而更慢。很好奇老师是怎么知道这个知识点的，我去stackoverflow上查过，没有什么收获</div>2021-05-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/29/25/c7/edd74dfb.jpg" width="30px"><span>淡C</span> 👍（3） 💬（0）<div>作为一个准大四的学生，一开始学习了spark基础之后，很荣幸遇到磊哥的两个spark专栏，这两个专栏让我对spark的理解有了全新的理解，起码让我在实习的时候可以正式员工battle，值得二刷，spark调优甚至值得三刷，争取二刷之后我也可以称自己为spark初学者了。</div>2022-07-15</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/tYopaQ4kGyuG9GCmfUxicicReIZZqhBZNDv5Yp7AQ5B4lLS0mIicgYUsMfkcKAn27CQ1eAM7CQpibF7Rk7yP2uVEpw/132" width="30px"><span>会飞的企鹅</span> 👍（0） 💬（0）<div>老师 我想问了：怎么评估spark所需的资源。需要多少cpu和mem？</div>2022-05-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1b/a5/65/898bc6c5.jpg" width="30px"><span>wayne</span> 👍（0） 💬（1）<div>老师，请问如何注册永久的pyspark
pandas_udf，比如使用sparksql 都可以自动加载</div>2022-03-20</li><br/>
</ul>