你好，我是徐文浩。

前面两讲，我带你体验了OpenAI通过API提供的GPT-3.5系列模型的两个核心接口。一个是获取一段文本的Embedding向量，另一个则是根据提示语，直接生成一段补全的文本内容。我们用这两种方法，都可以实现零样本（zero-shot）或者少样本下的情感分析任务。不过，你可能会提出这样两个疑问。

1. Embedding不就是把文本变成向量吗？我也学过一些自然语言处理，直接用个开源模型，比如Word2Vec、Bert之类的就好了呀，何必要去调用OpenAI的API呢？
2. 我们在这个情感分析里做了很多投机取巧的工作。一方面，我们把3分这种相对中性的评价分数排除掉了；另一方面，我们把1分2分和4分5分分别合并在了一起，把一个原本需要判断5个分类的问题简化了。那如果我们想要准确地预测多个分类，也会这么简单吗？

那么，这一讲我们就先来回答第一个问题。我们还是拿代码和数据来说话，就拿常见的开源模型来试一试，看看能否通过零样本学习的方式来取得比较好的效果。第二个问题，我们下一讲再来探讨，看看能不能利用Embedding进一步通过一些机器学习的算法，来更好地处理情感分析问题。

## 什么是预训练模型？
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/1f/38/48/557629d9.jpg" width="30px"><span>Daniel</span> 👍（63） 💬（1）<div>BERT：BERT 基于 Transformer 的（Encoder）。BERT 使用双向（bidirectional）的自注意力机制，可以同时捕捉文本中的前后上下文信息。
GPT：GPT 基于 Transformer 的（Decoder）。GPT 使用单向（unidirectional）的自注意力机制，只能捕捉文本中的前文（left context）信息。
能否请老师详细讲一下，这两者的差别？</div>2023-03-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/5f/81/1c614f4a.jpg" width="30px"><span>stg609</span> 👍（8） 💬（1）<div>老师能说说
1.  davinci, ada 等模型与gpt3的关系吗？
2. gpt3有1750亿参数，那是不是 ada 也有这么大量的参数</div>2023-03-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/c4/92/338b5609.jpg" width="30px"><span>Roy Liang</span> 👍（6） 💬（4）<div>1. 小数据集验证结果是这样，但是门外汉其实不懂表格里什么意思
              precision    recall  f1-score   support

    negative       0.25      0.99      0.40       136
    positive       1.00      0.48      0.65       789

    accuracy                           0.56       925
   macro avg       0.62      0.74      0.52       925
weighted avg       0.89      0.56      0.61       925

2. 可能适合新闻分类、垃圾邮件分类等不关心词语次序的场景吧</div>2023-03-27</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKPJb1k8zia02BAFmutyQ3jmlNpI9libqhtmNp6bTlJAYEj083ViaM048yuMHKs8na5TvLIkRDFRibiaZA/132" width="30px"><span>Geek_61af67</span> 👍（5） 💬（1）<div>不关心顺序的话，对tags进行分析会不会比较合适？</div>2023-03-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/4f/f2/6ac3bdcf.jpg" width="30px"><span>qingtama</span> 👍（4） 💬（1）<div>请问老师，这里的2.2亿参数，可以理解成向量所在的维度是2.2亿个吗？</div>2023-04-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/28/27/1f/42059b0f.jpg" width="30px"><span>HXL</span> 👍（4） 💬（1）<div>还是没明白什么是 预训练模型</div>2023-04-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/52/4e/c7c5b256.jpg" width="30px"><span>摩西</span> 👍（3） 💬（1）<div>刚接触机器学习，基础比较薄弱，请问老师 Transformer 是指什么？这里的transformer 跟 huggingface 中的transformer是相同的内容吗？</div>2023-04-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/36/85/d6/0221579f.jpg" width="30px"><span>王昊翔Harry</span> 👍（3） 💬（1）<div>这一套流程有没有在Colab友好的。本身没有编程经验该怎么着手？</div>2023-04-04</li><br/><li><img src="" width="30px"><span>yanyu-xin</span> 👍（2） 💬（1）<div>fasttext 无法安装成功，程序没有调试。
T5的程序经过修改，可以安装使用。需要修改了以下句子：
一、将fasttext中的句子：
positive_text = &quot;&quot;&quot;Wanted to save some to bring to ……
negative_text = &quot;&quot;&quot;First, these shoul……
完整黏贴到T5程序开始中。
二、增加：from sklearn.metrics.pairwise import cosine_similarity
三、将调用cosine_similarity 函数参数使用 NumPy 的 reshape()由一维数组转换为二维数组,具体如下:
def get_t5_score(sample_embedding):
    return cosine_similarity(sample_embedding.reshape(1, -1), positive_review_in_t5.reshape(1, -1)) - cosine_similarity(sample_embedding.reshape(1, -1), negative_review_in_t5.reshape(1, -1))

def evaluate_embeddings_approach():
    def label_score(review_embedding):
        return cosine_similarity(review_embedding.reshape(1, -1), positive_review_in_t5.reshape(1, -1)) - cosine_similarity(review_embedding.reshape(1, -1), negative_review_in_t5.reshape(1, -1))

四、将datafile_path = “fine_food_reviews_with_embeddings_1k.csv的地址” #按你电脑实际路径填写
五、减少数量量，将fine_food_reviews_with_embeddings_1k.csv 的数据减少到100个，运行速度快很多</div>2023-04-01</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJcwXucibksEYRSYg6icjibzGa7efcMrCsGec2UwibjTd57icqDz0zzkEEOM2pXVju60dibzcnQKPfRkN9g/132" width="30px"><span>Geek_93970d</span> 👍（1） 💬（1）<div>load_facebook_model 这个函数就得卡好半天，模型文件6.7G，加载很慢。另外 fasttext 既然用的是 gensim 里的，那就不用单独安装了吧？安装也安装不上，PackagesNotFoundError。</div>2023-05-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1d/14/91/794687ef.jpg" width="30px"><span>王石磊</span> 👍（0） 💬（1）<div>参考文中的用例，用T5-base 推理的结果如下，准确度为56%，这大概是什么原因呢？ 
precision    recall  f1-score   support

    negative       0.25      0.99      0.40       136
    positive       1.00      0.48      0.65       789

    accuracy                           0.56       925
   macro avg       0.62      0.74      0.52       925
weighted avg       0.89      0.56      0.61       925
</div>2023-05-13</li><br/><li><img src="" width="30px"><span>Geek_40e894</span> 👍（0） 💬（3）<div>colab Load the FastText pre-trained model 内存不够，崩溃</div>2023-04-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/7a/ff/df4eab46.jpg" width="30px"><span>buffett</span> 👍（0） 💬（1）<div>from transformers import T5Tokenizer, T5Model，这个代码调用T5模型的也是通过通过api请求吗？是maas的方式吗？还是本地load了模型了</div>2023-04-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/d6/39/6b45878d.jpg" width="30px"><span>意无尽</span> 👍（0） 💬（1）<div>老师，有点不是很理解的，就是文中讲的向量，可不可以这么理解：Fasttext 和 GPT3 的一个差别是 Fasttext 使用到的上下文比较少，所以很多场景下缺乏语义，导致判断失败，而 GPT3 包含的很多，所以更精准？</div>2023-04-21</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erms9qcIFYZ4npgLYPu1QgxQyaXcj64ZBicNVeBRWcYUpCZ9p0BGsrEcX8heibMLCV4Gde4P9pf7PjA/132" width="30px"><span>yanger2004</span> 👍（0） 💬（1）<div>如果用完整的110亿参数来测试，准确率应该会更高吧</div>2023-04-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/67/6e/f5ee46e8.jpg" width="30px"><span>海滨</span> 👍（0） 💬（1）<div>除了T5之外，有其他做NLP任务效果更好的大模型推荐吗？</div>2023-04-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/19/e1/a7fbc963.jpg" width="30px"><span>Warren</span> 👍（0） 💬（1）<div>
conda install transformer -c conda-forge 这个命令中的transformer要改成transformers</div>2023-03-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/79/d0/63ac112e.jpg" width="30px"><span>X丶楓</span> 👍（0） 💬（4）<div>现在最新的是gpt4，不应该说gpt4么？</div>2023-03-27</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83epmAicDUiaUdtLhVwSs6fT0yx69ibWy6ia46ZD4vblGtyee8QFz71icKZJkzccAFG3zHnMngSz7WeGBtKw/132" width="30px"><span>小神david</span> 👍（5） 💬（0）<div>鉴黄鉴暴</div>2023-03-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/21/14/423a821f.jpg" width="30px"><span>Steven</span> 👍（2） 💬（0）<div>Fasttext 效果测试时碰到异常：
DLL load failed while importing _imaging

更新 pillow 版本，然后重启 Notebook 即可
pip install --upgrade pillow</div>2023-04-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/36/9d/7e/83524ccd.jpg" width="30px"><span>王尼玛</span> 👍（1） 💬（0）<div>mac 安装pytorch  pip3 install torch torchvision torchaudio  </div>2023-05-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/21/14/423a821f.jpg" width="30px"><span>Steven</span> 👍（1） 💬（1）<div>Win10 下 fasttext 安装（Linux 环境未尝试）：
1，从 https:&#47;&#47;www.lfd.uci.edu&#47;~gohlke&#47;pythonlibs&#47;#fasttext 下载fasttext‑0.9.2‑cp310‑cp310‑win_amd64.whl 文件
2，执行 pip install fasttext‑0.9.2‑cp310‑cp310‑win_amd64.whl 安装</div>2023-04-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/9d/f9/b98d9c40.jpg" width="30px"><span>森林木</span> 👍（0） 💬（0）<div>是不是说明这些预训练模型本身的训练目的也不是情绪?</div>2025-02-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/0a/a4/828a431f.jpg" width="30px"><span>张申傲</span> 👍（0） 💬（0）<div>第4讲打卡</div>2024-06-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2b/f9/7c/75d613e8.jpg" width="30px"><span>Prometheus</span> 👍（0） 💬（0）<div>FileNotFoundError                         Traceback (most recent call last)
&lt;ipython-input-2-75fd75f5cd90&gt; in &lt;cell line: 4&gt;()
      2 import numpy as np
      3 # Load the FastText pre-trained model
----&gt; 4 model = gensim.models.fasttext.load_facebook_model(&#39;cc.en.300.bin&#39;)
      5 
      6 def get_fasttext_vector(line):</div>2024-03-26</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/wr3qiaXYeAaLHyPBArO8ThtgMns5icVkv7SWasTbGeic8QsIkAIC2XCG7ibTTibdicLB6WCibjj5b50kcV7rPKxqVW8Dw/132" width="30px"><span>胡贺鹏</span> 👍（0） 💬（0）<div>AttributeError: &#39;DataFrame&#39; object has no attribute &#39;Text&#39; 出现这个报错怎么处理呢？</div>2023-09-13</li><br/><li><img src="" width="30px"><span>Geek_d54869</span> 👍（0） 💬（0）<div>很想看看bert运行的效果指标 对比一下</div>2023-08-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2d/d3/a6/cb3d35fc.jpg" width="30px"><span>Qweasd</span> 👍（0） 💬（0）<div>也没有GitHub上的代码</div>2023-07-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/33/74/d9d143fa.jpg" width="30px"><span>silentyears</span> 👍（0） 💬（0）<div>请问老师，所谓的大模型就是指参数多吗？
预训练模型学习的数据越多，参数会变得越多吗？文中所说的用来训练的语料文本越丰富，模型中可以放的参数越多是什么意思？ 和采用的模型ada, babbage, curie, davinci 决定的参数有什么关系？
谢谢
</div>2023-07-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/d9/ce/4528cb4b.jpg" width="30px"><span>呼呼</span> 👍（0） 💬（0）<div>我试了下T5-base模型。如果是中文的话，对应国内的好评和差评，得分都是0。是不是T5模型不支持中文？</div>2023-06-19</li><br/>
</ul>