你好，我是Tyler。

今天我们要开启一个全新的章节——技术原理篇。从今天开始，我们正式进入AI大模型的学习。在这节课中，主要学习内容是视觉预训练模型（Pre-Training Model，PTM）。

你可能会问，为什么要先讲预训练模型呢？很好的问题，我们所说的AI大模型，其实就是一种预训练模型。预训练模型会使用海量的数据对模型做长时间的预先训练，这能让它拥有强大的知识储备，更好地解决各个领域的问题。

我会带你深入学习“预训练技术”和“大模型技术”的发展历程，帮助你理解它们的来龙去脉，以便更好地学习大语言模型中的预训练方法。

## 视觉模型的发展之路

在开始预训练技术的学习之前，我们先来了解一下“大模型”的发展历程。

大模型这个概念在视觉领域并不陌生，第一代大模型技术就来自视觉领域。故事的开始还要从1943年说起，那一年神经生物学家 MeCulloch 与青年数学家 Pitts 合作，基于生物神经网络的工作特点，提出了第一个人工神经网络模型。

在此之后，神经科学成为了人工智能最重要的交叉学科之一。随着这两个学科交叉的不断深入，涌现出了各种伟大的仿生神经网络，其中最经典的网络就有我们即将学习的卷积神经网络（Convolutional Neural Networks, CNN）。
<div><strong>精选留言（9）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/38/d2/a8/194d33ed.jpg" width="30px"><span>lw</span> 👍（2） 💬（1）<div>大模型也就是基础模型，也可以通过预训练模型得到，本身也是一种预训练模型
</div>2023-10-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/38/ba/2f/8919c7a9.jpg" width="30px"><span>黎芷维 Vee</span> 👍（2） 💬（1）<div>预训练模型通过在大规模数据上进行预训练来学习通用的数据，然后进行微调。</div>2023-09-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/29/2c/54/e72c8e14.jpg" width="30px"><span>崔馨月</span> 👍（1） 💬（1）<div>大模型，即大语言模型（Large Language Model, LLM），其实本质上也是预训练语言模型，只是相较于最初的GPT , BERT参数更多，模型规模更大，人民俗称为大模型。</div>2023-09-14</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erms9qcIFYZ4npgLYPu1QgxQyaXcj64ZBicNVeBRWcYUpCZ9p0BGsrEcX8heibMLCV4Gde4P9pf7PjA/132" width="30px"><span>yanger2004</span> 👍（1） 💬（2）<div>预训练模型（Pretrained Model）是用大量数据预先训练好的模型，它们通常使用无监督学习算法进行训练，并从大量的文本或图像数据中提取出一组通用的特征，这些特征在许多不同的NLP或计算机视觉任务中都很有用。

大模型（Large Model）是指参数数量很大的深度神经网络模型，例如BERT、大语言模型等在词汇表、层数、隐藏单元等方面具有相当数量的参数。大模型需要使用更多的计算资源和更长的时间进行训练，但相应地也具有更高的模型性能和更好的泛化性能。

通常情况下，预训练模型具有更强的通用性，能够适应更多的语言和任务，而大模型则可以获得更好的性能，因为它们更强大、更丰富的参数设置可以更好地适应数据。因此，许多研究人员将预训练模型与大模型相结合，使用预训练模型初始化大模型的参数，以提高模型的性能和泛化性能。</div>2023-09-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/ff/7b/cbe07b5c.jpg" width="30px"><span>顾琪瑶</span> 👍（1） 💬（1）<div>只知道大模型是通过预训练模型过来的
但具体的关系并不知道, 等一手优质回答</div>2023-09-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/cd/6d/928b0ffd.jpg" width="30px"><span>、轻</span> 👍（2） 💬（0）<div>这节课收货甚多</div>2024-03-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2c/a5/95/a9f55696.jpg" width="30px"><span>榕树</span> 👍（0） 💬（0）<div>预训练模型和大模型之间存在密切的关系
1、预训练模型：
是通过在大规模数据上进行无监督学习得到的模型。这些模型学习到了语言的一般特征和模式，例如语法、语义和语用等方面的知识。
2、大模型：
通常指的是具有大量参数和强大计算能力的模型。这些模型在规模上相对较大，能够处理更复杂的任务和数据。
3、两者关系：
预训练模型可以作为构建大模型的基础。通过使用预训练模型的参数和知识，可以在特定任务上进行微调，从而快速适应新的任务和领域。此外，一些大模型本身也是通过进一步扩展和改进预训练模型得到的。</div>2024-12-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/db/95/daad899f.jpg" width="30px"><span>Seachal</span> 👍（0） 💬（0）<div>预训练模型和大模型啥关系？我觉得，预训练模型就像是给大模型打了个好基础，让大模型能更快更好地学东西。

计算机视觉预训练大模型，商业上人脸识别这些应用火了，投资多了，需求就稳了，模型也就越做越好。技术上，ResNet、ImageNet这些创新，让训练大模型不再是梦。说到预训练，就像高中先上通识课，大学再选专业，模型也是先学基础的，再学专业的。ChatGPT这样的NLP模型也是这个路子。视觉预训练模型的经验，用到NLP上，可不容易。


CV这块儿，从神经科学到CNN，再到ResNet，发展得那叫一个快。预训练技术和大模型，那可是CV的两大功臣。商业化一推，预训练技术就更火了。视觉预训练模型，可解释性强，微调也方便。总的来说，这节课就是讲了讲CV的发展历程，预训练技术的重要性，还有视觉预训练模型的那些事儿。</div>2024-11-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/e7/b6/c9b56731.jpg" width="30px"><span>St.Peter</span> 👍（0） 💬（0）<div>通过本节课的学习了解了预训练+微调的研究范式的起源</div>2024-11-11</li><br/>
</ul>