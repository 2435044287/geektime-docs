你好，我是 Tyler。

在上一节课中，你已经学习了自然语言处理（NLP）的预训练模型技术。经过持续不断的探索，NLP领域迎来了许多重大的突破。其中，Transformer模型及其衍生模型BERT和GPT系列就是最具代表性的例子。这些研究成果为NLP预训练模型的发展带来了曙光。

不过，这只是大语言模型波澜壮阔发展历史的开端。随后，自然语言处理（NLP）的预训练模型技术在短时间内取得了飞跃式进展，迅速迈入了技术爆炸的阶段。其中一个关键因素是Transformer的问世，其出色的性能和训练效率提升为大型模型技术的发展创造了必要的条件。

因此这节课，我就会带你啃下这个 Transformer 这个硬骨头，不过请放心，它的原理其实并不复杂。但想要理解它，请务必确保你已经认真学习了[第12节课](https://time.geekbang.org/column/article/696734)的内容，**深入理解上节课提到的 Seq2Seq 架构以及编码器和解码器的作用**。

因为没有这些基础的话，你学习**Transformer 的过程就会有点像在听天书。**当然了，我也会延续我们课程的风格，尽可能可以通过白话让你理解它。

## 注意力机制

[上节课](https://time.geekbang.org/column/article/698540)我们提到了注意力机制。你可能已经发现，早期的注意力机制是需要附着在其他网络架构上才能发挥作用。
<div><strong>精选留言（7）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/13/a8/22/09c4211e.jpg" width="30px"><span>水木</span> 👍（5） 💬（1）<div>这堂课讲的很有意思啊，用变形金刚 星际舰队寻找目标星球的剧情把抽象的问题方法具体形象化。</div>2023-09-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/51/6e/efb76357.jpg" width="30px"><span>一只豆</span> 👍（3） 💬（1）<div>二刷课程的我，在每一节课中反复感受到 老师自顶向下的教学思想：不仅在每一节课中（本节讲解是我看到所有Transformer讲解中最棒的），也反映在 对整个AI技术发展的探索历史的上帝视角。 纵观这门课程，很多句子都能让学习者恍然大悟，达到“一眼万年”的境界。。。</div>2023-09-23</li><br/><li><img src="" width="30px"><span>Paul Shan</span> 👍（3） 💬（2）<div>RNN是一个单词一个单词处理的，自带顺序，Transformer批量处理，位置信息丢失了，必须加入位置信息才能让模型学到单词之间的位置关系。请问一个问题，为什么位置信息是直接加入到embedding的输出，感觉位置信息和embedding的输出是不同维度的东西，用不同维度表示可能更合理一点，但是我也能理解本来维度已经很高了，直接加不会增加维度，除了不增加维度，直接加还有什么其他理由吗，多谢</div>2023-09-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/c4/9d/0f4ea119.jpg" width="30px"><span>周晓英</span> 👍（4） 💬（0）<div>位置编码在 Transformer 模型中起到了非常重要的作用。

保持顺序信息:

在文本处理任务中，词语之间的顺序关系是非常重要的。但是，由于 Transformer 的自注意力机制是对所有位置的词语同时进行处理，它本身无法区分词语的顺序。位置编码的加入能够提供顺序信息，使得模型能够区分不同位置的词语。
使模型具备顺序感:

当位置编码被加入到输入向量中时，每个位置的向量现在都包含了关于其位置的信息。这使得模型能够根据词语的相对位置来学习和作出预测。
泛化能力:

通过位置编码，模型可以更好地泛化到不同长度的序列，因为它学会了词语之间的相对位置关系。
如果去掉位置编码，Transformer 模型就失去了词语顺序的信息，这会严重影响模型的性能，特别是在需要理解语言顺序的任务中，如机器翻译、文本摘要等。在一些不需要顺序信息的任务中，可能位置编码不是严格必要的，但在大多数自然语言处理任务中，位置编码是非常重要的。</div>2023-10-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2c/a5/95/a9f55696.jpg" width="30px"><span>榕树</span> 👍（0） 💬（0）<div>可以去掉位置编码。
在 Transformer 模型中，位置编码（Positional Encoding）是一个关键组件，用于引入序列中单词的位置信息。由于 Transformer 的自注意力机制本身是无序的，即它对输入序列的位置没有固有的感知能力，因此需要通过位置编码来提供位置信息，使模型能够区分不同位置的单词</div>2024-12-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/db/95/daad899f.jpg" width="30px"><span>Seachal</span> 👍（0） 💬（0）<div>位置编码在Transformer模型中扮演着相当重要的角色。它帮助模型区分输入文本中不同词汇的位置，这是理解文本上下文和语义关系的基础。如果没有位置编码，模型可能会难以判断词汇之间的顺序和相对位置，从而影响对文本的整体理解。因此，我认为在Transformer模型中，位置编码是不可或缺的。  
 Transformer模型先把文本转成高维向量，再通过位置编码给每个词标上位置，这样模型就知道每个词在哪儿了。然后自注意力机制上场，找词与词之间的联系，形成注意力权重，引导模型学习。这比传统注意力机制厉害多了，能学到更多隐藏信息。

Transformer模型在自然语言处理上表现超赞，给大型模型技术发展开了个好头。里面还包括多头注意力、残差归一化、前馈神经网络这些核心部件，都挺关键的。</div>2024-11-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/e7/b6/c9b56731.jpg" width="30px"><span>St.Peter</span> 👍（0） 💬（0）<div>不能去掉位置编码。位置编码保证了一个句子的顺序，去掉位置编码会乱序</div>2024-11-11</li><br/>
</ul>