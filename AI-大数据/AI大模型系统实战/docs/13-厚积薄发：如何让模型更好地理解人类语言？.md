你好，我是Tyler。

在之前的课程中，我们已经了解了语言模型的发展历程，这让我们可以明显看到，NLP领域的发展速度并不亚于计算机视觉（CV）领域，NLP研究人员不断推动着相关方法的不断发展。

然而，在NLP领域，一直存在一个令人尴尬的问题，这也是我们在第11节课时提过的一个问题：既然在计算机视觉领域，预训练模型的表现如此出色，NLP领域是否从他们的成功经验中学到了一些什么呢？

答案是NLP 确实汲取了灵感，但在实践中难以完全复制。NLP 的研究人员只是知道了外面世界的美好，却发现自己没有 CV PTM 的命，为什么这么说呢？这节课我就带你一探究竟。

## 重建巴别塔

首先，视觉能力是人类天生就具备的，而语言能力则要后天学习，所以对人类来说语言能力本来就更难。此外，语言存在不同的语种，每种语种都有不同的词汇、词法和语法。这进一步增加了处理语言的复杂性。

语言的多样性分散了人类的注意力，因此构建一个涵盖多种语言的带标签数据集这件事，变得异常困难。这也使得全球范围内的科学家们很难共同构建“巴别塔”，只能在自己的小圈子里打转。所以 NLP 也被我们称为人工智能皇冠上的宝石。

然而，是不是就没办法了呢？当然不是，只是这个过程非常漫长。既然无法构建一个多语种的、带标签的自然语言数据集，NLP 的预训练模型大军只能早早地离开监督学习的母星，探索无监督学习的深邃宇宙，以光年为单位飞往目标星球。
<div><strong>精选留言（7）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/29/60/a8/d60df0d6.jpg" width="30px"><span>baron</span> 👍（4） 💬（1）<div>bert 训练也是无监督且双向训练出来的，我理解大数据量、无监督都满足，不满足的是不能进行多任务。说得不对的地方请大牛更正哈，我没有算法基础。</div>2023-09-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/91/51/3da9420d.jpg" width="30px"><span>糖糖丸</span> 👍（2） 💬（1）<div>非端到端方法为什么就可能会导致信息丢失呢？应该如何理解？</div>2023-10-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/ff/7b/cbe07b5c.jpg" width="30px"><span>顾琪瑶</span> 👍（1） 💬（2）<div>看到这个思考题去搜了一些关于大语言模型的定义, 其中说到几点
1. 大量的文本数据进行训练
2. 过大规模的无监督训练来学习自然语言的模式和语言结构
3. 表现出一定的逻辑思维和推理能力

搜了下, BERT并不符合第3点, 那就代表不是LLM</div>2023-09-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/24/c4/51/5bca1604.jpg" width="30px"><span>aLong</span> 👍（1） 💬（0）<div>属于大模型，因为我看到那个亚马逊的论文图片中包含了BERT，他是在Encoder-Decoder的路线上面。</div>2023-12-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/2c/a5/95/a9f55696.jpg" width="30px"><span>榕树</span> 👍（0） 💬（0）<div>BERT（Bidirectional Encoder Representations from Transformers）和大语言模型（LLM，Large Language Models）之间有一些关键的差别。以下是它们的主要区别：

1. 架构设计
BERT：BERT 是一种基于 Transformer 的 encoder-only 模型。它的设计目的是通过双向（即同时从左到右和从右到左）理解文本的上下文信息。
大语言模型（如 GPT-3）：大语言模型通常是基于 Transformer 的 decoder-only 模型，或者是 encoder-decoder 结构。GPT 系列模型（如 GPT-3）是典型的 decoder-only 模型，它们主要用于生成文本。
2. 预训练任务
BERT：BERT 使用掩码语言模型（Masked Language Model, MLM）和下一句预测（Next Sentence Prediction, NSP）作为预训练任务。MLM 通过随机掩盖输入文本中的一些单词，然后让模型预测这些被掩盖的单词。NSP 则用于训练模型理解句子之间的关系。
大语言模型（如 GPT-3）：GPT 系列模型使用自回归语言模型（Autoregressive Language Model）进行预训练，即通过预测序列中的下一个单词来训练模型。这种方法使得模型在生成文本时能够逐步生成每个单词。
3. 应用场景
BERT：BERT 更适合于理解和处理文本的任务，如文本分类、命名实体识别、问答系统、文本匹配等。它在需要深入理解文本上下文的任务中表现出色。
大语言模型（如 GPT-3）：大语言模型在生成文本、对话系统、内容创作等任务中表现出色。它们能够生成连贯且上下文相关的长文本段落。
4. 模型大小和计算资源
BERT：BERT 的模型大小相对较小，常见的版本有 BERT-Base 和 BERT-Large。虽然它们也需要大量的计算资源进行预训练，但相比于最新的大语言模型，它们的规模较小。
大语言模型（如 GPT-3）：大语言模型的规模通常非常庞大，例如 GPT-3 具有 1750 亿个参数。这些模型的训练和推理需要极其庞大的计算资源。
5. 训练数据
BERT：BERT 在预训练时使用了大量的文本数据，但这些数据主要用于理解文本的语义和上下文。
大语言模型（如 GPT-3）：大语言模型通常使用更大规模和更多样化的文本数据进行训练，以便在生成文本时能够更好地模拟人类语言。
总结
BERT 和大语言模型（如 GPT-3）在架构设计、预训练任务、应用场景、模型大小和计算资源等方面都有显著的差异。BERT 更侧重于文本理解任务，而大语言模型则在文本生成任务中表现出色。两者在自然语言处理领域都有广泛的应用，但它们的设计目标和优势有所不同。
</div>2024-12-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/db/95/daad899f.jpg" width="30px"><span>Seachal</span> 👍（0） 💬（0）<div>ERT那大数据大参数训练的模型，确实算大语言模型（LLM）里的一员了。

这篇文章啊，讲了NLP领域的挑战，语言多样性、数据集缺乏这些问题。然后Word2Vec无监督学习来了，语义理解、机器翻译都用上了。接着ElMo解决词义多义性问题，GPT-1预训练模型下游任务微调效果也挺好。还说了BERT和GPT-1预训练的不同，BERT的Masked语言模型、Next Sentence Prediction这些创新点也提了下。</div>2024-11-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/e7/b6/c9b56731.jpg" width="30px"><span>St.Peter</span> 👍（0） 💬（0）<div>https:&#47;&#47;arxiv.org&#47;pdf&#47;2304.13712    这是一篇非常好的梳理。其中的图片更是在网络上很流传。</div>2024-11-11</li><br/>
</ul>