你好，我是 Tyler。

上节课，我们学习了如何对特定领域的问题进行训练数据增强。在Alpaca的原文中，虽然已经大幅减少了模型微调所需的计算资源，但仍然需要相当大的算力开销。

Alpaca 论文的作者，使用了 8 块 80GB 的 A100 显卡，进行了 3 小时的微调，才完成训练。不难发现，这样的成本仍然很高，所以业界也一直在探索更具性价比的训练方法，其中最经典的方法莫过于 LoRA。

我们这节课将重点介绍LoRA技术的原理和使用方法，通过理论讲解和实践练习，帮助你掌握LoRA技术，并且在下一节课中用LoRA技术来微调自己的预训练大语言模型。

## LoRA：低秩适应

如果你对文生图的领域有所关注，那么你一定对 LoRA 模型一定不陌生，它似乎已经成为自动定制二次元小姐姐的代名词。

![](https://static001.geekbang.org/resource/image/20/1e/203c6b05088026404df48de8eb34501e.jpg?wh=4000x1930)

然而，实际上LoRA是一种通用的模型训练方法。它最早本就是用来加速大语言模型训练的，这点你从它的全称 “Low-Rank Adaptation of Large Language Models” 就能看出来。

为了进一步降低微调的成本，来自斯坦福大学的研究员Eric J. Wang采用了LoRA（低秩适应）技术复制了Alpaca的结果。

具体来说，Eric J. Wang使用了一块RTX 4090显卡，仅用了5个小时就成功训练出了一个与原版Alpaca相媲美的模型，成功将这类模型对计算资源的需求降低到了消费级显卡的水平。此外，这个模型甚至可以在树莓派上运行，非常适合用于小型的研究团队。
<div><strong>精选留言（2）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/26/1c/40/9fde5f9b.jpg" width="30px"><span>Zachary</span> 👍（2） 💬（0）<div>对于不典型的任务类型，LoRA这么做应该影响很大，因为不知道该换上哪套BA参数了？而GPT3&#47;4的能力看起来是连续的，也就是能胜任很多我们无法明确指名道姓的非标准任务（尽管有强化学习和MoE来增强特定任务的能力），LoRA在这些非标准任务上也许就不太行了，能力比较离散。考虑到这一点后，用一些优化方法也许可以改善。</div>2023-11-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/ff/7b/cbe07b5c.jpg" width="30px"><span>顾琪瑶</span> 👍（2） 💬（0）<div>1. 猪脑过载, 想不到
3. 内在联系, 就像介绍LORA时说的, &quot;较低的内在纬度&quot;, 也就是说在针对特定任务微调时, 只需要关注一个子空间中的参数即可, LORA和ANN都是只关注部分向量即可, 也就是分而治之</div>2023-10-18</li><br/>
</ul>