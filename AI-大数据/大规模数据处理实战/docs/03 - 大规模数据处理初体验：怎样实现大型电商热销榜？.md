你好，我是蔡元楠。

今天我要与你分享的主题是“怎样实现大型电商热销榜”。

我在Google面试过很多优秀的候选人，应对普通的编程问题coding能力很强，算法数据结构也应用得不错。

可是当我追问数据规模变大时该怎么设计系统，他们却说不出所以然来。这说明他们缺乏必备的规模增长的技术思维（mindset of scaling）。这会限制这些候选人的职业成长。

因为产品从1万用户到1亿用户，技术团队从10个人到1000个人，你的技术规模和数据规模都会完全不一样。

今天我们就以大型电商热销榜为例，来谈一谈从1万用户到1亿用户，从GB数据到PB数据系统，技术思维需要怎样的转型升级？

同样的问题举一反三，可以应用在淘宝热卖，App排行榜，抖音热门，甚至是胡润百富榜，因为实际上他们背后都应用了相似的大规模数据处理技术。

![](https://static001.geekbang.org/resource/image/46/ec/469707990cf33d24d8713efab8fe34ec.png?wh=900%2A1600)

真正的排序系统非常复杂，仅仅是用来排序的特征（features）就需要多年的迭代设计。

为了便于这一讲的讨论，我们来构想一个简化的玩具问题，来帮助你理解。

假设你的电商网站销售10亿件商品，已经跟踪了网站的销售记录：商品id和购买时间 {product\_id, timestamp}，整个交易记录是1000亿行数据，TB级。作为技术负责人，你会怎样设计一个系统，根据销售记录统计去年销量前10的商品呢？
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/16/f5/5f/217c6a14.jpg" width="30px"><span>Liu C.</span> 👍（36） 💬（1）<div>有一次处理一个非常高维的feature矩阵，要给它降维，但手头的电脑cpu和内存都不够好。于是我用了非常hack的手段：先使用random projection算法降低一定维度，这是一个纯矩阵乘法，可以分块放入内存计算。之后剩余的维度还是有些大，于是我把feature拆成几组，对每组分别做pca，之后再选出每组最大的主成分拼起来，就完成了降维。
</div>2019-04-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/48/75/02b4366a.jpg" width="30px"><span>乘坐Tornado的线程魔法师</span> 👍（12） 💬（1）<div>作者好！找出前K个集群小节里面的第一个计算集群的第二个节点（机器），是否应该像第一个节点一样计算product_id=1的所有记录。文中图示貌似只有第一个节点计算了。请作者查证。</div>2019-04-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/41/27/3ff1a1d6.jpg" width="30px"><span>hua168</span> 👍（11） 💬（1）<div>分解法…像剁鱼那样，一条一口吃不下就切成块，块一口吃还大，有风险，再就再用筷子分小…
关键问题是怎么切，切多大？怎么不全切碎，让它完整的，让人知道是条鱼😄</div>2019-04-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/ee/16/742956ac.jpg" width="30px"><span>涵</span> 👍（5） 💬（1）<div>做传统数仓时，使用oracle数据库，随着数据量增大会需要使用到分区。分区需要思考使用哪个属性来分，分成多大的区间合适。另外，当视图很大时，有时查询很慢，会使用物化视图的方法。</div>2019-04-22</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLdWHFCr66TzHS2CpCkiaRaDIk3tU5sKPry16Q7ic0mZZdy8LOCYc38wOmyv5RZico7icBVeaPX8X2jcw/132" width="30px"><span>JohnT3e</span> 👍（5） 💬（1）<div>数据倾斜，导致任务运行时间超出预期，这个时候就需要对数据做一些分析和采样，优化shuffle。任务出错后，调试周期变长，这个目前没有很好的解决。不过，之前看flumejava论文，其采用了缓存不变结果来加快调试周期。另外，就是集群规模增大，后期运维的问题了</div>2019-04-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/48/75/02b4366a.jpg" width="30px"><span>乘坐Tornado的线程魔法师</span> 👍（4） 💬（1）<div>顺便复习了王争老师的《数据结构与算法》，看到Top算法的时间复杂度准确来讲应该是是O(nLogK)</div>2019-04-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/dd/b9/ce84d577.jpg" width="30px"><span>Charles.Gast</span> 👍（4） 💬（1）<div>数据不数据什么的无所谓，我就想听听那个力学公式的讲解㊣</div>2019-04-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/1a/f9/180f347a.jpg" width="30px"><span>朱同学</span> 👍（2） 💬（1）<div>实际上传统服务也是这样，业务初期我们一台物理机，后面又是三台物理机，做的反向代理小集群，到现在几个机柜做了虚拟化，数据库也做了读写分离，说到底就是集群化处理</div>2019-04-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/9b/9a/dcb2b713.jpg" width="30px"><span>hufox</span> 👍（2） 💬（1）<div>以前做订单系统的时候，由于数据量没有那么大，没有考虑到大规模数据处理问题，但是一旦数据量上来了，统计查询都很慢，今天阅读了老师这一讲，原来可以这样设计处理大规模数据问题，涨姿势了！继续学习！</div>2019-04-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/48/75/02b4366a.jpg" width="30px"><span>乘坐Tornado的线程魔法师</span> 👍（1） 💬（1）<div>如果K=3的情况下 并且product=2 product_id=3均有多条记录 请问下关于product_id=2 product=3的记录该如何处理 也是 每台机器都要做product_id=1的聚合吗</div>2019-04-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/ae/97/ba512167.jpg" width="30px"><span>darren</span> 👍（0） 💬（1）<div>每台机器计算topK,汇总到master基于每台机器的topk数据，再做top,是不准确的，相当于基于top的数据，来计算top，是不准确的。</div>2019-05-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/8c/2b/3ab96998.jpg" width="30px"><span>青石</span> 👍（44） 💬（1）<div>好多年前还未接触大数据时，写过日志采集统计各接口请求报表及puv的脚本，经历了几个阶段。
1. 最初是汇总所有日志到一台服务器，在处理日志，测试环境没问题，上生产跑起来就几个小时。
2. 后来分到Web服务器各自处理数据，时间缩短了，但是汇总数据偶尔会有问题。
3. 将数据写入到数据库，解决汇总数据问题。但是单表数据量过大，统计又很慢。
4. 按天分表解决数据量问题，最后就这么一直运行下去了。

这段经历其实很普通，但也确实让我更轻松的学习和理解大数据。当我学到mapreduce内容的时候，回忆起这段经历，让我很容易就接受了mapreduce的分治思想。

就像看到hbase的时候，我的理解它就是在实现数据的寻址、不断的拆分&#47;合并表，但是原来的人工操作变成现在自动化操作。</div>2019-04-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/04/5d/259d1768.jpg" width="30px"><span>bwv825</span> 👍（17） 💬（9）<div>Top 1 的情况，只统计每台机器的top 1是不是可能会不准确呢？比如数据按时间段分片，某个商品销量很大很稳定，累计总数第一但很少是top 1, 因为各个时间段都有不同的爆款... </div>2019-04-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/fb/4c/dc35efce.jpg" width="30px"><span>Mr Zhuo</span> 👍（14） 💬（0）<div>老师好，我目前是做NLP落地的，本来是作为补充知识学的这个专栏，但是学了这几节后发现这个方向很有潜力，也很感兴趣。另外由于你们google的BERT横空出世，感觉NLP方向的个人发展有些迷茫，所以想请问老师，对于专栏内容和NLP的结合，在未来发展有没有好的建议呢？</div>2019-04-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/6b/e4/afacba1c.jpg" width="30px"><span>孙稚昊</span> 👍（11） 💬（0）<div>数据量一大，最常见的问题除了各种exception，就是key 值分布不均衡。电商一般都是长尾的，少量的item 占据大多数购买量，很容易发送数据倾斜，需要设计更新的hash-sharding 方法</div>2019-04-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/01/28/56410b04.jpg" width="30px"><span>Kev1n</span> 👍（11） 💬（0）<div>个人经验，拆分，复制，异步，并行，是大规模数据处理和应用架构的常见手段，一致性根据业务场景适当妥协</div>2019-04-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/6b/e4/afacba1c.jpg" width="30px"><span>孙稚昊</span> 👍（9） 💬（2）<div>我们在做商品订单统计的时候，会按itemid + order year + order month 对订单做hash来做group 的 key，分割成更小块，防止popular item 堆积造成的瓶颈</div>2019-04-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/89/4b/56d290f5.jpg" width="30px"><span>leeon</span> 👍（7） 💬（0）<div>大规模的topk在计算过程中很容易引发数据倾斜的问题，在实际业务里，计算的优化是一方面，有时候从数据层面去优化也会有更好的效果，以榜单为例，可以在时间维度和地域为度去拆解数据，先小聚再大聚</div>2019-04-24</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLhMtBwGqqmyhxp5uaDTvvp18iaalQj8qHv6u8rv1FQXGozfl3alPvdPHpEsTWwFPFVOoP6EeKT4bw/132" width="30px"><span>Codelife</span> 👍（6） 💬（0）<div>最初，GPS数据以文件形式存储在盘阵中，数据增长达到TB级别后，考虑到性能和成本以及可扩展性，系统迁移到HDFS中，离线任务用MR，在线查询采用HBSE，现在，数据PB级别后，发现热点数据hbase成本太高，系统迁移到时序数据库，专供线上实时查询，同时，实时分析采用storm，批处理用spark。其实，很多情况下，采用什么技术，成本具有决定性因素</div>2019-04-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/f5/80/baddf03b.jpg" width="30px"><span>zhihai.tu</span> 👍（5） 💬（0）<div>有一个项目，试点的时候由于用户访问量小，传统负载均衡F5下连6台应用服务器访问为啥问题。后续推广后，由于访问量出现了50倍以上的增加，前台响应慢，服务器也出现内存溢出等问题。后续采用了docker容器技术，从应用服务器上抽取出并发访问较高的服务模块，单独部署服务层，支持横向扩展以及在线扩容，较好的解决了问题。</div>2019-04-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/59/fd/128cc75b.jpg" width="30px"><span>Daryl</span> 👍（3） 💬（0）<div>作者其实关于top k没描述清楚，虽然我明白他的意思，因为我了解这边，但是对于没有了解的同学会有点晕乎</div>2019-04-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/ec/38/c7819759.jpg" width="30px"><span>哈哈</span> 👍（2） 💬（0）<div>将大规模数据拆解到多台机器处理，还应该用一定的规则哈希到每台机器吧</div>2019-05-10</li><br/><li><img src="" width="30px"><span>高景洋</span> 👍（1） 💬（0）<div>1、数据量千万级别时，采用的是mysql分库分表的方式，数据大小GB级别，未达到TB级别
2、我们是做电商数据的，当库中商品sku数量，从千万级别，膨胀到亿级的时候，分库分表是一件维护成本极高的事情。因为每个表中的数据量达到一定级别时，总要人为的，去重做散列

3、因此，我们的做法是，将所有数据按5年增长量预算，都导到了hbase中
4、调度程序由普通的程序脚本，换成了spark集群。
5、我们的调度速度，由老版本的1h，提高到12min

———————-
如果我来做，课程里讲的销量统计，我可能会这样做：
1、按订单将数据散列到hbase中
2、spark集群将hbase中的数据拉出来
3、按sku维度做count统计操作
4、以jd为例，sku数量级大概50亿
5、其中 月维度有销量的sku量级 大概30%
6、也就是经过第三步后，统计出来的数据量级在15亿-20亿
7、将这些有销量的数据按t-1的时间维度，入到hive中
8、按老板们的需求从hive中，拉取topN的销量sku</div>2021-09-17</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erdpKbFgRLnicjsr6qkrPVKZcFrG3aS2V51HhjFP6Mh2CYcjWric9ud1Qiclo8A49ia3eZ1NhibDib0AOCg/132" width="30px"><span>西北偏北</span> 👍（1） 💬（0）<div>数据量大后，数据处理思维，除了要利用数据结构，算法在单台机器上做优化之外，还要思考，怎么利用集群的性能来搞定。</div>2019-06-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/48/75/02b4366a.jpg" width="30px"><span>乘坐Tornado的线程魔法师</span> 👍（1） 💬（0）<div>我的思考是：如果K=3，而product_id = 1, product_id = 2, ......product_id = 9(product_id有9种而且每种product_id可能会重复出现，个数是动态的)，那么在统计销量集群，分配计算的方法不变，但是在找出销量前K的集群，每台机器可以按照K负责3个product_id, 如第1台机器负责product_id = 1, 2, 3， 第二台负责product_id = 4, 5, 6以此类推......  这样需要3台机器，每台机器的输出正好就是每台机器本地的最大K。但是可以进一步通用化，如果product_id有11种情况，但是只有3台机器，该怎么办？我想的方案是第1台机器分配product_id = 1~3， 第2台机器分配product_id = 4~7，第3台机器分配product_id = 8~11。然后针对于每台机器的输出，第2第3台机器均需要从4种情况找出排名前3的情况。不确定这样的理解是否在正确的道路上，请指正。</div>2019-04-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/88/d8/fdd2cecd.jpg" width="30px"><span>孤鹜齐飞</span> 👍（1） 💬（0）<div>大道至简，大数据处理本质上还是要化繁为简、以大化小，分布式并行处理并汇总。</div>2019-04-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/66/c8/f598a816.jpg" width="30px"><span>Phantom01</span> 👍（0） 💬（0）<div>主要是分治。考虑怎么拆分合并，有点像外排序。然后想办法提高单次操作的效率，减少落盘或者通信次数。

也可以先分段topK然后合并</div>2020-12-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/75/9b/611e74ab.jpg" width="30px"><span>技术修行者</span> 👍（0） 💬（0）<div>请教一下，大规模数据处理系统的设计对于传统的数据ETL系统设计有什么参考和借鉴价值吗？
我们目前的系统主要针对批量数据，在使用Kettle+Carte来处理，但是对于异常处理和系统监控方面做的不是很好。</div>2020-11-07</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/IVIhn9AycTlGFSPvA4FB4b8GicYgFoqBkBRnRROOAjR6IUDcYxvJzDKGbBOh7YibqaAYLICX450rcWHZhq6ic5jdg/132" width="30px"><span>Geek_639183</span> 👍（0） 💬（0）<div>第一步，统计每个商品的销量。你可以用哈希表（hashtable）数据结构来解决，是一个 O(n) 的算法，这里 n 是 1000 亿。

老师 请问为什么此处用hashtable 而不用hashmap呢？</div>2020-10-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/59/21/d2efde18.jpg" width="30px"><span>布凡</span> 👍（0） 💬（0）<div>老师，&quot;每台机器先把所有 product_id = 1 的销量叠加在了一起，再找出自己机器上销量前 K = 1 的商品。可以看到对于每台机器而言，他们的输出就是最终排名前 K = 1 的商品候选者。&quot;这里有个疑问。
我理解的是因为每台机器上的product_id的值差不多都是均匀分布的，如果在每台机器上销量排前K 那么整体上也会排前K。那会不会存product_id对应的count在机器1上排前K, 机器2上排前K 以后的情况呢？那这样机器2上排前K以后的数是不是就不会被汇总了呢？</div>2020-08-25</li><br/>
</ul>