你好，我是蔡元楠。

今天我要与你分享的主题是“综合案例实战：处理加州房屋信息，构建线性回归模型”。

通过之前的学习，我们对Spark各种API的基本用法有了一定的了解，还通过统计词频的实例掌握了如何从零开始写一个Spark程序。那么现在，让我们从一个真实的数据集出发，看看如何用Spark解决实际问题。

## 数据集介绍

为了完成今天的综合案例实战，我使用的是美国加州1990年房屋普查的数据集。

![](https://static001.geekbang.org/resource/image/a9/5c/a9c1d749f2d1c43261a043aa77056f5c.png?wh=1142%2A593)

数据集中的每一个数据都代表着一块区域内房屋和人口的基本信息，总共包括9项：

1. 该地区中心的纬度（latitude）
2. 该地区中心的经度（longitude）
3. 区域内所有房屋屋龄的中位数（housingMedianAge）
4. 区域内总房间数（totalRooms）
5. 区域内总卧室数（totalBedrooms）
6. 区域内总人口数（population）
7. 区域内总家庭数（households）
8. 区域内人均收入中位数（medianIncome）
9. 该区域房价的中位数（medianHouseValue）

也就是说，我们可以把每一个数据看作一个地区，它含有9项我们关心的信息，也就是上面提到的9个指标。比如下面这个数据：

```
-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000'
```

这个数据代表该地区的经纬度是（-122.230000,37.880000），这个地区房屋历史的中位数是41年，所有房屋总共有880个房间，其中有129个卧室。这个地区内共有126个家庭和322位居民，人均收入中位数是8.3252万，房价中位数是45.26万。
<div><strong>精选留言（19）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg" width="30px"><span>JustDoDT</span> 👍（13） 💬（3）<div>终于跑通了，不容易啊，刚开始数据集没下载正确。有空值，老师给的数据集较干净。
别的数据集，要记得去除空值。
下面是实践代码jupyter
https:&#47;&#47;github.com&#47;LearningChanging&#47;spark-exercise&#47;blob&#47;master&#47;19&#47;CaliforniaHousing.ipynb</div>2019-09-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/9e/50/21e0beca.jpg" width="30px"><span>kylin</span> 👍（12） 💬（2）<div>请问为什么不用dateset进行数据处理而是用dateFrame? </div>2019-06-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/80/be/8350f94d.jpg" width="30px"><span>gotojeff</span> 👍（2） 💬（2）<div>dataset不支持python, 所以在python里只有DF，这算不算python的一大劣势？scala是更好的选择？</div>2019-06-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/54/50/8a76a8cc.jpg" width="30px"><span>Zoe</span> 👍（1） 💬（1）<div>看前两篇文章时还在想，没什么练手的机会啊，今天就推送了实战练习，有一种终于跟上大神思维的幻觉，开心！</div>2019-05-31</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erdpKbFgRLnicjsr6qkrPVKZcFrG3aS2V51HhjFP6Mh2CYcjWric9ud1Qiclo8A49ia3eZ1NhibDib0AOCg/132" width="30px"><span>西北偏北</span> 👍（0） 💬（1）<div>一些实际的大数据处理，确实需要数学啊……怎么才能把数学学好？</div>2019-08-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/15/5b/79/d55044ac.jpg" width="30px"><span>coder</span> 👍（17） 💬（0）<div>老师的代码可以po到Github上，这样大家都可以学习了🌝🌝🌝</div>2019-05-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/9b/9a/dcb2b713.jpg" width="30px"><span>hufox</span> 👍（3） 💬（2）<div>最后一句的代码改成 predictionAndLabel[:2] ，可以了！</div>2019-06-16</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eoRyUPicEMqGsbsMicHPuvwM8nibfgK8Yt0AibAGUmnic7rLF4zUZ4dBj4ialYz54fOD6sURKwuJIWBNjhg/132" width="30px"><span>咸鱼与果汁</span> 👍（2） 💬（1）<div>spark df的数据处理还是略显复杂，感觉大部分的算法人员还是使用pandas进行数据预处理，请问使用pandas是不是就无法发挥spark RDD的威力了？这种情况下spark就相当于是一个异步任务处理框架？</div>2020-02-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/9b/9a/dcb2b713.jpg" width="30px"><span>hufox</span> 👍（2） 💬（3）<div>在执行最后一句代码predictionAndLabel.take(2)时报错：
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-35-0700ca2381fb&gt; in &lt;module&gt;
----&gt; 1 predictionAndLabel.take(2)

AttributeError: &#39;list&#39; object has no attribute &#39;take&#39;</div>2019-06-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/20/bb/43d63c5f.jpg" width="30px"><span>henry</span> 👍（1） 💬（0）<div>最后一步，“模型预测的结果有些偏小”，这一点，从结果上看，不是应该预测的结果要大一些吗？</div>2019-09-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/12/9f/b6eb3471.jpg" width="30px"><span>黄智寿</span> 👍（1） 💬（0）<div>老师，你好，数据集的下载地址能发一下吗？</div>2019-08-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/30/e9/13fb8d51.jpg" width="30px"><span>毛毛</span> 👍（1） 💬（1）<div>老师，建议在案例讲解时对用到的算法大概解释下，比如srandscaler，这不是太清楚什么用处？谢谢</div>2019-06-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/ee/9c/abb7bfe3.jpg" width="30px"><span>abc-web</span> 👍（1） 💬（0）<div>老师代码可以上github吗，这样同学们可以参考下</div>2019-06-03</li><br/><li><img src="" width="30px"><span>Geek5350</span> 👍（0） 💬（0）<div>请问数据集在哪里下载？</div>2021-05-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/ed/e8/8985d6e0.jpg" width="30px"><span>寻水的小鱼</span> 👍（0） 💬（0）<div>&gt;&gt;&gt; 
&gt;&gt;&gt; def convertColumn(df, names, newType)
  File &quot;&lt;stdin&gt;&quot;, line 1
    def convertColumn(df, names, newType)
                                        ^
SyntaxError: invalid syntax
</div>2021-01-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1c/a0/f4/7e122a67.jpg" width="30px"><span>之渊</span> 👍（0） 💬（1）<div>java 版的代码demo : https:&#47;&#47;gitee.com&#47;oumin12345&#47;daimademojihe&#47;tree&#47;master&#47;cloudx&#47;bigdata&#47;src&#47;main&#47;java&#47;test&#47;spark
建议初学者没写过的可以自己敲一下。
机器学习的入门可以看看 ：
https:&#47;&#47;my.oschina.net&#47;ouminzy&#47;blog&#47;4437101</div>2020-08-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/61/61/677e8f92.jpg" width="30px"><span>xianhai</span> 👍（0） 💬（0）<div>最好还是给完整的代码，节省初学者的时间。</div>2020-05-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/55/99/4bdadfd3.jpg" width="30px"><span>Chloe</span> 👍（0） 💬（0）<div>报错：
&quot;
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
NameError: name &#39;FloatType&#39; is not defined
&quot;
Google了一下: https:&#47;&#47;stackoverflow.com&#47;questions&#47;40701122&#47;unexpected-type-class-pyspark-sql-types-datatypesingleton-when-casting-to-i

加了这句就好了：
from pyspark.sql.types import FloatType

大家还有人也遇到这个错误吗？</div>2020-02-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/33/07/8f351609.jpg" width="30px"><span>JustDoDT</span> 👍（0） 💬（1）<div>StandardScaler 归一化之后，两列变成 NaN 了搞不明白
+-----+--------------------+--------------------+
|label|            features|     features_scaled|
+-----+--------------------+--------------------+
|4.526|[129.0,322.0,126....|[NaN,0.2843362208...|
|3.585|[1106.0,2401.0,11...|[NaN,2.1201592122...|
+-----+--------------------+--------------------+

scaled_df.take(2)
[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([nan, 0.2843, 0.3296, 4.3821, 2.8228, 0.2461, nan])),
 Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([nan, 2.1202, 2.9765, 4.3696, 2.5213, 0.2031, nan]))]</div>2019-09-02</li><br/>
</ul>