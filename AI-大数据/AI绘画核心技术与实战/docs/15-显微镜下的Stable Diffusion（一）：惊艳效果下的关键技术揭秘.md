你好，我是南柯。

我们之前已经学习了Stable Diffusion的核心组成模块，比如CLIP、VAE、UNet、注意力机制、采样器等等。在第二个实战项目（可以回顾 [12讲](https://time.geekbang.org/column/article/685751)复习），我们已经动手训练了自己的扩散模型，也基于基础模型微调了我们自己的Stable Diffusion。

为了方便表述，后面我们就用SD指代Stable Diffusion。对于我们来说，SD已经不再是黑盒子了。但其实除了我们已经知道的内容，SD能够生成精美构图的背后，还有很多黑魔法在起作用。

今天，我会带你尝试用“显微镜”解析SD，深入探索其中的技术细节，比如文本引导的原理、注意力机制的实现细节等。学完这一讲，你会对SD的工作原理有更深刻和全面的理解，并将这些知识灵活地应用到你自己的项目中。

## SD模型的演化之路

你也许在社交媒体或者Hugging Face等论坛上，看到过各种各样的SD模型版本，从早期的SD1.4到近期的SDXL 1.0。SD版本号演化的背后，其实是技术路线的改变或者训练数据的优化，搞清楚SD的演化路径能帮助我们理解AI绘画的发展趋势。

当前开源社区流行的SD模型有多个版本，比如SD1.4、SD1.5、SD2.0、SD2.1、SD Reimagine、SDXL等。表面看起来眼花缭乱，但其实这些模型之间存在或多或少的“亲缘关系”。你可以点开下面的图，看一下这些模型的演化历程。
<div><strong>精选留言（1）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/17/4a/e2/d0d7ca81.jpg" width="30px"><span>西柊慧音</span> 👍（1） 💬（1）<div>图像补全是把图切割成若干个像素块，类似拼图，通过对某一个格和周边格子的逻辑关联做填充？</div>2023-09-14</li><br/>
</ul>