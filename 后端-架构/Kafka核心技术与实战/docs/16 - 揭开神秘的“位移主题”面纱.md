你好，我是胡夕。今天我要和你分享的内容是：Kafka中神秘的内部主题（Internal Topic）\_\_consumer\_offsets。

\_\_consumer\_offsets在Kafka源码中有个更为正式的名字，叫**位移主题**，即Offsets Topic。为了方便今天的讨论，我将统一使用位移主题来指代\_\_consumer\_offsets。需要注意的是，它有两个下划线哦。

好了，我们开始今天的内容吧。首先，我们有必要探究一下位移主题被引入的背景及原因，即位移主题的前世今生。

在上一期中，我说过老版本Consumer的位移管理是依托于Apache ZooKeeper的，它会自动或手动地将位移数据提交到ZooKeeper中保存。当Consumer重启后，它能自动从ZooKeeper中读取位移数据，从而在上次消费截止的地方继续消费。这种设计使得Kafka Broker不需要保存位移数据，减少了Broker端需要持有的状态空间，因而有利于实现高伸缩性。

但是，ZooKeeper其实并不适用于这种高频的写操作，因此，Kafka社区自0.8.2.x版本开始，就在酝酿修改这种设计，并最终在新版本Consumer中正式推出了全新的位移管理机制，自然也包括这个新的位移主题。
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/47/ae/0a5f7a56.jpg" width="30px"><span>此方彼方Francis</span> 👍（63） 💬（7）<div>之前遇到过的一个问题跟大家分享一下，原因描述不正确的地方还请大佬指正：
log cleaner线程挂掉还有可能导致消费端出现：Marking Coordinator Dead！

原因大概如下：
log cleaner线程挂掉之后会导致磁盘上位移主题的文件越来越多（当然，大部分是过期数据，只是依旧存在），broker内存中会维护offsetMap，从名字上看这个map就是维护消费进度的，而这个map和位移主题的文件有关联，文件越来越多会导致offsetMap越来越大，甚至导致offsetMap构建失败（为什么会失败没有搞明白），offsetMap构建失败之后broker不会承认自己是coordinator。
消费者组找coordinator的逻辑很简单：abs(consumer_groupName.hashCode) % __consumer_offset.partition.num对应的partition所在的broker就是这个group的coordinate，一旦这个broker的offsetMap构建失败，那么这个broker就不承认自己是这个group的coordinate，这个group的消费就无法继续进行，会出现Marking Coordinator Dead错误。
此时需要删除过期的位移主题的文件（根据文件名很容易确定哪个是最新的），重启broker。重启过程中需要关注log cleaner是否会再次挂掉。

PS：上述问题在broker重启和正常运行中都有可能遇到。</div>2020-02-14</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/x86UN2kFbJGGwiaw7yeVtyaf05y5eZmdOciaAF09WEBRVicbPGsej1b62UAH3icjeJqvibVc6aqB0EuFwDicicKKcF47w/132" width="30px"><span>Eco</span> 👍（30） 💬（7）<div>有个问题想请教一下，这个位移主题，Consumer是像消费其他主题的分区的内容一样去获取数据的话，那么这本身不也得有个位移，那这个位移又保存到哪里的呢？这样下去不就陷入了一个死循环了吗？要么就不是像正常的消费消息那样去从位移主题获取当前消费者对于某个主题的分区的位移？</div>2020-01-15</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/31/7c/14df0dbf.jpg" width="30px"><span>mellow</span> 👍（24） 💬（5）<div>老师能讲一下，同一个group下的consumer启动之后是怎么去offset topic 拿到该group上次消费topic每个partition的最新offset呢？是根据key来定位offset topic的partition吗，然后拿到所有消息得到最新的offset吗</div>2019-07-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/98/5d/8e91b338.jpg" width="30px"><span>王藝明</span> 👍（22） 💬（3）<div>老师好！
为什么位移主题写入消息时，不直接替换掉原来的数据，像 HashMap 一样呢？而是要堆积起来，另起线程来维护位移主题</div>2019-10-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/ae/79/da524393.jpg" width="30px"><span>sharpdeep</span> 👍（16） 💬（2）<div>有个困惑: 位移主题用来记住位移，那么这个位移主题的位移由谁来记住呢?</div>2020-03-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/d0/09/7aaed1d8.jpg" width="30px"><span>🤡</span> 👍（15） 💬（3）<div>对GroupId 还有疑惑，假设一个Group下有 3 个Consumer , 那这三个Consumer 对应的groupid 应该是一样的。这样的话怎么做key做唯一区分呢</div>2019-08-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/ee/d6/0142c3a3.jpg" width="30px"><span>HZ</span> 👍（14） 💬（4）<div>老师 有两点不太清楚 1. 位移主题里面，对于同一个consumer group的位移提交记录，是分布在50个partitions中吗？ 2. 如果把位移主题当作kafka里面的一个普通主题，那么写入这个主题的数据可以保证不丢失吗？ 写入是ack=all？ 同时，broker端的min.insync.replicas的设置有影响吗？</div>2020-02-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/44/7a/a4058eee.jpg" width="30px"><span>帆船出航</span> 👍（13） 💬（1）<div>胡老师，消费者提交的位移消息，保存到位移主题分区是随机的吗？就是某一个消费者的提交第一个位移数据保存在位移主题的A分区里面，第二个位移数据保存在位移主题的B分区里面
还有消费者是怎样获取已消费的位移数据</div>2020-01-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c5/a7/cc8764d1.jpg" width="30px"><span>Geek_cd6rkj</span> 👍（12） 💬（2）<div>consumer端 日常业务发版呢，那每次发版需要重启consumer不是也会导致Rebalance，这个如何规避</div>2019-07-15</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJcvQzf86HsxOkPcRpibBdCxDW0IK9wel9TmkEhicHPUHPRhzKna8wecDcJcVbNHNSrUMt4GHLxY3iaA/132" width="30px"><span>Coder4</span> 👍（10） 💬（4）<div>老师好，前几年一直有个说法，说kafka不适合创建过多topic，请问现在的新版还有这个问题么？</div>2019-07-11</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/wonzX0yIia5NFoB0TZPdIPqVicIuSvzMtowownIkO9VwYpkPWJP2tpEcv5RXMn0ayuEGkAp2GBualL5sFQs0ibQJQ/132" width="30px"><span>gogogo</span> 👍（10） 💬（3）<div>请问offset是以最新的为准，还是值最大的为准？</div>2019-07-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/d3/6e/281b85aa.jpg" width="30px"><span>永光</span> 👍（9） 💬（4）<div>位移主题，适用于高频写的操作，为什么ZooKeeper不适用于这种高频的写操作？zookeeper 也可以按照&lt;Group ID，主题名，分区号 &gt; 来写入呀？</div>2019-07-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/88/26/b8c53cee.jpg" width="30px"><span>南辕北辙</span> 👍（8） 💬（3）<div>老师，请教一下consumer 是如何从这个位移主题中拿到曾经属于自己组的offset呢</div>2019-07-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/ac/f8/a56b7475.jpg" width="30px"><span>燃烧的M豆</span> 👍（6） 💬（1）<div>老师如果超大规模集群超大规模消费者对这一个 50 个 partitions 的 topic 进行消费是不是会引起性能问题？topic 下的 partitions 设置有上线吗 面对超大规模并发除了提升 partitions 数量还有什么办法？谢谢</div>2019-07-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/4c/6d/c20f2d5a.jpg" width="30px"><span>LJK</span> 👍（5） 💬（1）<div>老师好，请教一个小白问题，一个位移主题是在第一个consumer启动时建立的，是说对于一个kafka集群只有一个位移主题么？另外我对kafka框架还是有点迷惑，kafka集群是不是没有NameNode这个概念啊？每一个leader partition就相当于一个NameNode？谢谢老师</div>2019-09-23</li><br/><li><img src="" width="30px"><span>小鱼</span> 👍（5） 💬（1）<div>老师，你好，请问控制Kafka 使用Compact 策略来删除位移主题中的过期消息的参数是哪个？

</div>2019-07-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/e9/0b/1171ac71.jpg" width="30px"><span>WL</span> 👍（5） 💬（2）<div>请教老师三个问题：
1. 在consumer group中的一个consumer消费一条消息后，是往它拉取消息的那个broker写一条offset消息还是往所有它连接的broker都广播一条消息。
2. 一个broker中的位移主题保存的是他自己上面的主题和分区的位移还是整个集群的所有主题所有分区的位移都有保存。
3. 位移主题的50个分区分配在各个broker的方式是啥，轮询，hash，还是随机？</div>2019-07-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/47/1b/64262861.jpg" width="30px"><span>胡小禾</span> 👍（4） 💬（2）<div>那么，何时会写入这类消息呢？一旦某个 Consumer Group 下的所有 Consumer 实例都停止了，而且它们的位移数据都已被删除时，Kafka 会向位移主题的对应分区写入 tombstone 消息，表明要彻底删除这个 Group 的信息。

问题来了：何时kafka会把 Consumer 的位移数据都给删除了？</div>2020-05-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/8b/ec/dc03f5ad.jpg" width="30px"><span>张天屹</span> 👍（4） 💬（4）<div>对于自动提交位移这里有两点疑惑，第一个老师说“能够避免消息丢失”，那如果自动提交之后，业务处理失败呢，不久丢失消息了吗？第二个老师说“最新消息为100，没有继续生产，这个时候消费者会不断自动提交最新位移100”，既然没有消费了，为什么还要提交呢？消费了100就提交100，之后没有消费就意味着位移没变，为啥还要提交呢？这两个问题的根源都在于不是很清楚自动位移提交的触发条件，是消费就触发吗？还是没有发生异常就触发？还是定时触发？</div>2019-11-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/7e/8b/3cc461b3.jpg" width="30px"><span>宋晓明</span> 👍（4） 💬（2）<div>老师 消息从producer到broker里的partition其实都是有序的，这是kafka的机制保证的，那么假如我的consumer是单线程的，也能保证消费是有序的，但是吞吐量就下降了。如果consumer是多线程，如果保证有序性？</div>2019-07-30</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/df/e5/65e37812.jpg" width="30px"><span>快跑</span> 👍（4） 💬（1）<div>老师，你好
Kafka有位移主题，是不是Consumer都是从整个位移主题获取数据应该从哪个offset开始读数据，如果Spark Streaming作为一个Consumer，其offset的控制也是在这个位移主题当中？ 这样的话Spark Streaming用Direct方式读取Kafka其实是不需要额外找其他存储作为位移的保存？</div>2019-07-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/23/c1/54ef6885.jpg" width="30px"><span>MoonGod</span> 👍（4） 💬（1）<div>请教老师一个问题：
在consumer提交位移的时候，通过Coordinator 往所在的broker写消息，那如果当前的broker挂掉了，写入位移主题的消息会丢失吗？还是说位移主题在写入的时候也会把消息同步到其他broker中的副本中，从而保证写入消息不丢失呢</div>2019-07-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/f2/26/8d42ea6f.jpg" width="30px"><span>never leave</span> 👍（4） 💬（1）<div>如果kafka消息没有key的话，怎么Compact？比较value吗？</div>2019-07-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/13/45/16c60da2.jpg" width="30px"><span>蔫巴的小白菜</span> 👍（3） 💬（1）<div>相比zk不适合频繁写入我认为最大的问题，在于一个zk集群中，只有master的节点提供写入的操作，这样写入相当于是单机的tps，而kafka的写入引入分区的概念解决zk单机写入的问题，只有主分区可以写入，kafka集群的主分区可以分布在各个机器上，从而达到避免了单机写入的瓶颈，而zk如果想要解决此问题，可以多搭建几个zk集群，通过上层的proxy机制，将不同的数据写入不同的集群，来提高tps，但是，zk的初衷就是一个分布式协调工具</div>2020-11-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/64/9b/d1ab239e.jpg" width="30px"><span>J.Smile</span> 👍（3） 💬（2）<div>---自动提交位移
当Consumer端参数enable.auto.commit=true：
Consumer 在后台默默地为你定期auto.commit.interval.ms来提交位移
---自动提交位移导致的撑满磁盘场景：
假设 Consumer 当前消费到了某个主题的最新一条消息，位移是 100，之后该主题没有任何新消息产生，故 Consumer 无消息可消费了，所以位移永远保持在 100。
由于是自动提交位移，位移主题中会不停地写入位移 =100 的消息。
显然 Kafka 只需要保留这类消息中的最新一条就可以了，之前的消息都是可以删除的。这就要求 Kafka 必须要有针对位移主题消息特点的消息删除策略，否则这种消息会越来越多，最终撑爆整个磁盘。

---解决办法：显然只需要最新的一次位移数据就可以了，那么删除掉过期的位移数据。Kafka 使用 Compact 策略来删除位移主题中的过期消息，避免该主题无限期膨胀。Kafka 提供了专门的后台线程定期地巡检待 Compact 的主题，看看是否存在满足条件的可删除数据。这个后台线程叫 Log Cleaner。
以上为总结。
-------一个矛盾问题------------
关于自动提交位移，在之前的课程中看到老师说Consumer如果采用这种方式在多线程消费消息时会造成消息丢失的，因为某个线程可能并未消费成功消息而却更新了位移！可这一节老师怎么说“自动提交位移有一个显著的优点，就是省事，你不用操心位移提交的事情，就能保证消息消费不会丢失。”呢？这不是矛盾吗</div>2020-06-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/37/2b/b32f1d66.jpg" width="30px"><span>Ball</span> 👍（3） 💬（1）<div>老师我有问题想请教下，如果我有一个 Consumer 实例且开启了自动提交 offset，Consumer 正在消费一个业务分区的消息，那么：
1、提交给 offset 分区操作肯定是发生在消费业务消息之后发生的，那这个操作是在同一个线程内完成的，还是说后台有个线程在异步写 offset。
2、基于上面那个场景，是不是 Consumer 最少只要和 Broker 建立一个 TCP 连接即可正常消费了？</div>2020-01-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/7a/93/c9302518.jpg" width="30px"><span>高志强</span> 👍（3） 💬（1）<div>enable.auto.commit 这个参数默认值是 true 吗，我看公司使用时并没有特意指定这个参数。如果不是true，怎样手动提交位移呢</div>2019-12-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/04/0d/3dc5683a.jpg" width="30px"><span>柯察金</span> 👍（3） 💬（1）<div>那么消费者怎么获取到自己要消费的那个分区的位移呢？是通过消费位移主题的数据吗？</div>2019-10-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/0e/61/ae68f8eb.jpg" width="30px"><span>dream</span> 👍（3） 💬（1）<div>老师，你最后说“Kafka 提供了专门的后台线程定期地巡检待 Compact 的主题”，那么这个定期是好久呢？这个间隔时间我们可以自己配置吗？</div>2019-07-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1e/1c/3b/210f843f.jpg" width="30px"><span>施亚东</span> 👍（2） 💬（1）<div>从group 的所有consumer实例都停止，到发送tombstone消息到位移主题，这两者之间是不是需要一个延迟？默认的是多少呢？
如果没有延迟，当这个group的消费者实例做版本更新的时候，没有采用滚动升级的方式，一次性停止全部consumer实例，升级完重启的时候不久出现问题了吗（位移主题已经删除这个group下的位移信息）？
</div>2020-04-25</li><br/>
</ul>