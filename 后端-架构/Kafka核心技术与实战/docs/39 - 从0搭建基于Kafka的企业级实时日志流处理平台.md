你好，我是胡夕。今天我要和你分享的主题是：从0搭建基于Kafka的企业级实时日志流处理平台。

简单来说，我们要实现一些大数据组件的组合，就如同玩乐高玩具一样，把它们“插”在一起，“拼”成一个更大一点的玩具。

在任何一个企业中，服务器每天都会产生很多的日志数据。这些数据内容非常丰富，包含了我们的**线上业务数据**、**用户行为数据**以及**后端系统数据**。实时分析这些数据，能够帮助我们更快地洞察潜在的趋势，从而有针对性地做出决策。今天，我们就使用Kafka搭建一个这样的平台。

## 流处理架构

如果在网上搜索实时日志流处理，你应该能够搜到很多教你搭建实时流处理平台做日志分析的教程。这些教程使用的技术栈大多是Flume+Kafka+Storm、Spark Streaming或Flink。特别是Flume+Kafka+Flink的组合，逐渐成为了实时日志流处理的标配。不过，要搭建这样的处理平台，你需要用到3个框架才能实现，这既增加了系统复杂度，也提高了运维成本。

今天，我来演示一下如何使用Apache Kafka这一个框架，实现一套实时日志流处理系统。换句话说，我使用的技术栈是Kafka Connect+Kafka Core+Kafka Streams的组合。
<div><strong>精选留言（29）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/15/f1/55/8ac4f169.jpg" width="30px"><span>陈国林</span> 👍（23） 💬（2）<div>老师好，说下我这边的一些实践。19年一直在做容器日志平台，我们目前的方案是 Fluentd + Kafka + ELK。使用Fluentd做为容器平台的采集器实时采集数据，采集完之后数据写入Kafka通过Kafka进行解耦，使用Logstash消费后写入ES。这套方案目前在容器环境下应该可以说是标配</div>2020-02-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/d5/7b/c512da6a.jpg" width="30px"><span>石栖</span> 👍（8） 💬（2）<div>胡老师，对于Stream的处理和之前的topic-message，我感觉没什么大的区别，感觉流程是类似的。只不过是在以前的consumer中额外添加了producer的逻辑，把处理结果发送到另一个topic中。感觉不用这里说的stream也能实现一样的效果。我不是个明白本质的区别是什么，麻烦能解释一下吗？谢谢</div>2020-05-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/a5/56/d6732e61.jpg" width="30px"><span>小刀</span> 👍（8） 💬（1）<div>老师，上述Kafka Connect+Kafka Core+Kafka Streams例子中，生产者和消费者分别是什么？</div>2019-09-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c7/dc/9408c8c2.jpg" width="30px"><span>ban</span> 👍（5） 💬（1）<div>老师，示例中开启Connect后启动读取的是本机的nginx日志，但如果nginx日志是在其他机器上面，那Connect是不是支持远程读的还是怎么样可以读取到其他机器的日志？</div>2019-09-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/dd/09/feca820a.jpg" width="30px"><span>helloworld</span> 👍（4） 💬（3）<div>写的不太明白啊, 难道每一个nginx服务器上都要部署kafka吗</div>2020-03-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/b2/8e/791d0f5e.jpg" width="30px"><span>w</span> 👍（4） 💬（1）<div>老师我想问一下。Kafka Connect 是一个单独的组件么？类似agent一样，可以在目标采集机器（非kafka集群）上部署？那如果要用，岂不是每个业务机器都要装个kafka？
单机模式跟集群模式有啥区别呢？没太懂。比如kafka集群上启动单机模式的connect ？不行么？
最终操作，都往同一个topic里扔就好了?</div>2020-02-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/37/2b/b32f1d66.jpg" width="30px"><span>Ball</span> 👍（4） 💬（1）<div>老师我有问题要请教下，添加 Connector 步骤里面是用 http REST 接口新建的，那新建的 Connector 是跑在 Broker 里面还是说又启动了一个新的 Java 进程执行 Connector？</div>2020-01-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/ed/4c/8674b6ad.jpg" width="30px"><span>timmy21</span> 👍（3） 💬（1）<div>老师如果有多个分区，并且消息写入是随机的。那么多个kafka streams实例在对os_type进行group_by统计时，需要相互之间传输数据进行shuffle操作吗？</div>2020-10-03</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTJYRZy0TDUsnolSVgRgRNiamraQiby8bpicAXno0CWC3WicftyRDt9QacLQzPhicBTd055yjMKW4Lbuf0w/132" width="30px"><span>Geek_nai6tk</span> 👍（1） 💬（3）<div>老师 想咨询个问题 我这边如果日志不是来自于文件 而是来自于telenet的输出 需要怎么做日志实时分析。 而且需要有上万个telenet实例一起在输出 我需要分别分析每个实例按照某种统一规则</div>2021-03-25</li><br/><li><img src="" width="30px"><span>Geek_6e00ab</span> 👍（1） 💬（1）<div>为什么消费者取出来的值是乱码的</div>2021-01-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/1d/64/52a5863b.jpg" width="30px"><span>大土豆</span> 👍（1） 💬（1）<div>问个大数据相关的问题，一定要读取和写入都在hdfs上才是离线计算吗？</div>2020-10-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/64/9b/d1ab239e.jpg" width="30px"><span>J.Smile</span> 👍（1） 💬（1）<div>“另一方面，它们又都是独立的实体，彼此之间毫无关联，完全依靠 Kafka Streams 帮助它们发现彼此并进行协作。”
————————————————————
完全依靠“kafka cluster”这个吧！？</div>2020-06-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/ee/d6/a9b34bd3.jpg" width="30px"><span>Panda</span> 👍（1） 💬（2）<div>connect与filebeat比单文件收集性能比较来说哪个更高？我们用filebeat单文件收集性能到瓶颈了，我们现在的做法是减小单文件的，大小，来提高并行度，不知道connect性能如何</div>2020-03-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/16/20/ce768739.jpg" width="30px"><span>L.</span> 👍（0） 💬（1）<div>老师，如果要将处理后的结果写到另一个 kafka 集群的 topic，应该怎么做呢？</div>2020-10-15</li><br/><li><img src="" width="30px"><span>InfoQ_ad0a52af1586</span> 👍（0） 💬（1）<div>这个案例运行了下没有两秒钟就输出，老师能怎么排查下是什么问题吗？</div>2020-08-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/cf/81/96f656ef.jpg" width="30px"><span>杨逸林</span> 👍（0） 💬（2）<div>我看了有点多的 SpringCloud + Kafka Stream 整合的，按照 Spring 官方给的教学视频学，结果到头来还是只会改下字符串（从一个 Topic 监听数据，送到另一个 Topic 做数据清洗）。Kafka 官网都是像你写的 demo 一样的，其实还是有点懵的。。。</div>2020-08-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/0c/52/f25c3636.jpg" width="30px"><span>长脖子树</span> 👍（0） 💬（1）<div>用过kafka connector，直接在集群上启动单机的connector连接mqtt，但感觉这种方案的扩展性不高</div>2020-07-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/5b/08/b0b0db05.jpg" width="30px"><span>丁丁历险记</span> 👍（0） 💬（3）<div>说好的从0 开始的，kafka 集群怎么装。</div>2020-06-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/e2/6e/0a300829.jpg" width="30px"><span>李先生</span> 👍（0） 💬（1）<div>胡哥：
开发的一个弹幕系统，分为server和connect，connect是专门维护socket链接和推消息的，可以横向扩展。但是目前遇到了系统瓶颈，单机就支持3000左右连接。在推消息的过程中，系统每次把消息给tcp就不管客户端是否能收到消息，但是操作系统tcp连接在发消息失败的时候会重试，所以在推消息的过程中消息就堆积了起来，导致单机连接数一直上不去。这种能指点一下吗？</div>2020-04-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/47/b0/a9b77a1e.jpg" width="30px"><span>冬风向左吹</span> 👍（0） 💬（1）<div>connect只能支持json格式的数据吗</div>2020-04-05</li><br/><li><img src="" width="30px"><span>leige</span> 👍（0） 💬（1）<div>胡老师，请问对于迟到的数据，os_check主题会生成多条记录吗？此时消费者应用程序应该如何处理？</div>2019-12-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/37/d0/26975fba.jpg" width="30px"><span>西南偏北</span> 👍（0） 💬（4）<div>老师，您在处理json串的时候为什么用Gson，而不用Alibaba的fastjson呢？</div>2019-11-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/b5/3c/967d7291.jpg" width="30px"><span>艺超(鲁鸣)</span> 👍（0） 💬（2）<div>请教一下，集群版的connector是说每个kafka节点都启动一个吗？还有它读取的nginx日志就在本地？谢谢</div>2019-09-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/41/38/4f89095b.jpg" width="30px"><span>写点啥呢</span> 👍（0） 💬（1）<div>请问胡老师，console-consumer输出的message，为什么结束时间是一个很大的整数？从开始时间看，它应该是millisecond epoch，原本以为结束时间应该也是开始时间+2 second，但是文章中的例子看着不像：

bin&#47;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic os-check --from-beginning --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer --property print.key=true --property key.deserializer=org.apache.kafka.streams.kstream.TimeWindowedDeserializer --property key.deserializer.default.windowed.key.serde.inner=org.apache.kafka.common.serialization.Serdes\$StringSerde
[android@1565743788000&#47;9223372036854775807] 1522
[ios@1565743788000&#47;9223372036854775807] 478
[ios@1565743790000&#47;9223372036854775807] 1912
[android@1565743790000&#47;9223372036854775807] 5313
[ios@1565743792000&#47;9223372036854775807] 780
[android@1565743792000&#47;9223372036854775807] 1949
[android@1565743794000&#47;9223372036854775807] 37
……
</div>2019-09-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/dc/08/64f5ab52.jpg" width="30px"><span>陈斌</span> 👍（1） 💬（0）<div>Kafka Stream 能不能将消费者Topic的一个消息分解转换成生产者的多条消息，而且其中还涉及可以把消息数据存入Redis中？</div>2022-02-09</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/m7fLWyJrnwEPoIefiaxusQRh6D1Nq7PCXA8RiaxkmzdNEmFARr5q8L4qouKNaziceXia92an8hzYa5MLic6N6cNMEoQ/132" width="30px"><span>alex_lai</span> 👍（0） 💬（0）<div>Kafka stream 的输出必须要返回到topic么？ 我可以直接用stream 的api输出到我的目标db么？</div>2021-11-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/5b/08/b0b0db05.jpg" width="30px"><span>丁丁历险记</span> 👍（0） 💬（0）<div>终于等到了。</div>2019-12-11</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/b1/92/4272e592.jpg" width="30px"><span>伟</span> 👍（0） 💬（1）<div>老师，请教connect读取mysql数据库中，我的添加connector命令是curl -X POST http:&#47;&#47;l-lzw:8083&#47;connectors -H &quot;Content-Type: application&#47;json&quot; -d &#39;{&quot;name&quot;: &quot;mysql-connector&quot;,&quot;config&quot;: {&quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,&quot;tasks.max&quot;: &quot;1&quot;,&quot;database.hostname&quot;: &quot;lzw-mysql&quot;,&quot;database.port&quot;: &quot;3306&quot;,&quot;database.user&quot;: &quot;root&quot;,&quot;database.password&quot;: &quot;123456&quot;,&quot;database.server.id&quot;: &quot;1&quot;,&quot;database.server.name&quot;: &quot;pydata&quot;,&quot;database.whitelist&quot;: &quot;employee&quot;,&quot;topic.prefix&quot;: &quot;test-mysql-&quot;,&quot;database.history.kafka.bootstrap.servers&quot;: &quot;l-lzw:9092,l-lzw2:9092&quot;,&quot;database.history.kafka.topic&quot;: &quot;db.history.mysql&quot;}}&#39;

消费topic时返回如下：
{&quot;source&quot;:{&quot;version&quot;:&quot;0.9.5.Final&quot;,&quot;connector&quot;:&quot;mysql&quot;,&quot;name&quot;:&quot;pydata&quot;,&quot;server_id&quot;:0,&quot;ts_sec&quot;:0,&quot;gtid&quot;:null,&quot;file&quot;:&quot;mysql-bin.000004&quot;,&quot;pos&quot;:1021,&quot;row&quot;:0,&quot;snapshot&quot;:true,&quot;thread&quot;:null,&quot;db&quot;:null,&quot;table&quot;:null,&quot;query&quot;:null},&quot;databaseName&quot;:&quot;&quot;,&quot;ddl&quot;:&quot;SET character_set_server=latin1, collation_server=latin1_swedish_ci;&quot;}}

没有正常可能原因是什么？

</div>2019-09-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/67/f4/9a1feb59.jpg" width="30px"><span>钱</span> 👍（0） 💬（1）<div>打卡，仅仅使用kafka这一个大数据组件就能实现一个企业级的实时日志流处理平台。
获取——存储——清洗——转存——展示</div>2019-09-24</li><br/>
</ul>