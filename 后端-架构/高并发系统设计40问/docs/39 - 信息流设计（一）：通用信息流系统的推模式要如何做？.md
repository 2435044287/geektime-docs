你好，我是唐扬。

前两节课中，我带你探究了如何设计和实现互联网系统中一个常见模块——计数系统。它的业务逻辑其实非常简单，基本上最多只有三个接口，获取计数、增加计数和重置计数。所以我们在考虑方案的时候考察点也相对较少，基本上使用缓存就可以实现一个兼顾性能、可用性和鲁棒性的方案了。然而大型业务系统的逻辑会非常复杂，在方案设计时通常需要灵活运用多种技术，才能共同承担高并发大流量的冲击。那么接下来，我将带你了解如何设计社区系统中最为复杂、并发量也最高的信息流系统。这样，你可以从中体会怎么应用之前学习的组件了。

最早的信息流系统起源于微博，我们知道，微博是基于关注关系来实现内容分发的，也就是说，如果用户A关注了用户B，那么用户A就需要在自己的信息流中，实时地看到用户B发布的最新内容，**这是微博系统的基本逻辑，也是它能够让信息快速流通、快速传播的关键。** 由于微博的信息流一般是按照时间倒序排列的，所以我们通常把信息流系统称为TimeLine（时间线）。那么当我们设计一套信息流系统时需要考虑哪些点呢？

## 设计信息流系统的关注点有哪些

首先，我们需要关注延迟数据，也就是说，你关注的人发了微博信息之后，信息需要在短时间之内出现在你的信息流中。
<div><strong>精选留言（12）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/11/dd/3c/a595eb2a.jpg" width="30px"><span>台风骆骆</span> 👍（50） 💬（2）<div>信息流的架构演化
1、一开始很简单，两张表，一张存储关注关系 ，一张存储微博消息，用户A发微博就是在相应的微博消息表中写入一条即可，用户B读微博也很简单，就是先得到自己关注的用户列表，然后定时去存储微博消息表中去读取自己关注的微博展示出来即可，优点是只有一份存储，缺点也很明显，对于这张表的读操作太多了，并发过大。
2、改成推模式，即写扩散机制，用户A发送一条消息，除了写入微博消息表以外，还要写入关注它的所有的用户的收件箱中（这个可以用redis来实现），然后用户去收件箱中读取消息即可，优点就是自己读自己的消息，跟别人没有竞争，缺点是多余存储，在大V用户发微博消息中有延迟，同时写入次数太多了，同时取消关注什么的也比较难操作。
3、后面改成了推拉结合的方式，即对于大V用拉模式，对于普通的用户继续用推模式。
4、后面出现了基于时间分区的拉模式，个人觉得可以结合推模式来进行相应的弥补。</div>2019-12-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/bc/25/1c92a90c.jpg" width="30px"><span>tt</span> 👍（9） 💬（2）<div>我觉得推模式最大的问题是没有做到按需传递信息，可能一个粉丝的用户中，只有很少比例才需要较高的时效性，这些用户不应该消耗太多的系统资源。

此外，推模式中的写操作太多了，一个人发送，其他人在本质上都是读取这条消息，却也引发了写入操作。

应该把新的信息写入到若干存储（包括缓存上），然后选择适当的策论，让用户去这些存储上读取数据。这样可以大大降低写入操作的数量。</div>2019-12-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/52/1c/e902de0f.jpg" width="30px"><span>追逐繁星的孩纸~</span> 👍（8） 💬（1）<div>老师，我有个疑问，我们是关注文档，使用推模式，新增和更新文档时，就查询有哪些用户关注了这篇文档，然后新增或者更新这些用户的收件箱。这样导致有个问题，一个是写入和更新操作比较多，另外一个是有重复消息或者消息乱序时，会重复更新用户动态或者把用户的动态更新为更早的。因为我们的关注用户量比较少，所以目前还能使用推模式。我想过使用缓存来缓存文档id和操作时间，每次新增或者更新动态时先比较下操作时间，但就需要加分布式锁。这种情况老师有什么建议么？</div>2020-01-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/c9/c0/730ea586.jpg" width="30px"><span>凡间的雨</span> 👍（5） 💬（2）<div>推的时候只给在线用户推一个未读消息数的提示，然后用户在点击未读消息的时候再去拉消息列表。推拉结合着用。</div>2020-04-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/37/4e/5c3153b2.jpg" width="30px"><span>知行合一</span> 👍（5） 💬（1）<div>推模式中可以给用户分优先级，优先推送优先级高的用户的方式来提升用户体验。</div>2019-12-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/c6/00/683bb4f0.jpg" width="30px"><span>正在减肥的胖籽。</span> 👍（4） 💬（2）<div>1.目前我公司的产品，我的做法没用数据库，全部是用redis来存储，用的就是推的模式。一个用户最多关注也就1万多个用户，当发了一条微博，就扔到消息队里面，消息队列再去拉取用户所有关注的人。一个一个放到对应粉丝的redis中。</div>2020-04-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/17/27/ec30d30a.jpg" width="30px"><span>Jxin</span> 👍（2） 💬（1）<div>1.根据分组又建一张表。应该不存全量数据吧，太浪费空间。存与全局收件箱的id映射就好了吧。

2.这个推模式确实有点恐怖。大量写操作简直就是噩梦。硬刚实现这种大量写是不现实的。所以改成拉模式，在登陆时按需拉取。但全量的按需拉取，登陆时的信息同步延迟也是不能接受，所以推拉结合以及更细力度的操作，满满都是权衡。长见识。

3.拉可以批量可以压缩，其实优化的空间会大点。</div>2019-12-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/87/ac/30a0bd5a.jpg" width="30px"><span>skyeinfo</span> 👍（2） 💬（2）<div>老师，对于信息流的缓存存储有什么比较好的建议呢？因为考虑到分页、过滤等筛选条件。</div>2019-12-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/4b/4b/97926cba.jpg" width="30px"><span>Luciano李鑫</span> 👍（1） 💬（1）<div>不理解为什么基于推模式要给每个用户甚至每个分组存储一份完整的消息，为什么不能用存储关联关系，计算得到推送的消息呢？</div>2019-12-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/c8/34/fb871b2c.jpg" width="30px"><span>海罗沃德</span> 👍（1） 💬（2）<div>跟微博比，我們的信息流弱爆了，目前都是用elasticsearch做信息流拉模式，閱讀之後就給當前頁的數據批量設置狀態，拉到下一頁就給下一頁數據更新狀態</div>2019-12-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/67/f4/9a1feb59.jpg" width="30px"><span>钱</span> 👍（2） 💬（1）<div>目前还没做过这样的系统，不过想一想数据量确实大一亿个粉丝，每发一条消息需要写一亿条消息，确实有些恐怖，这些大v如果是话唠，那分分钟几亿几十亿的消息需要写入，想想就头疼。存储延迟会比较大和存储空间会花费许多。
不知道微博的数据是怎么存储的？所有数据永久保存嘛？压缩存储？而且为了灾备会分区域存储多份？微博的存储系统做的也一定能讲究啦😄</div>2020-05-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/cc/de/e28c01e1.jpg" width="30px"><span>剑八</span> 👍（0） 💬（0）<div>推的模式，需要知道对方地址，当接受对象多则发送方压力大，实时性有影响，对方不怎么登录也要触发及时性的推送没有优先级。
拉的按需消费，登录了就查询消息。实时性如果关系数量不大则推实时性高，否则拉性能高。
拉的模式下保存下目标对象的消费进度就可以。</div>2022-04-16</li><br/>
</ul>