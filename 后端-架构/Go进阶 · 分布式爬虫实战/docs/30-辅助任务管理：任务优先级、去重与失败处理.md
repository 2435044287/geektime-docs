你好，我是郑建勋。

这节课，让我们给系统加入一些辅助功能，把爬虫流程变得更完善一些。这些功能包括：爬虫最大深度、请求不重复、优先队列、以及随机的User-Agent。

## 设置爬虫最大深度

当我们用深度和广度优先搜索爬取一个网站时，为了防止访问陷入到死循环，同时控制爬取的有效链接的数量，一般会给当前任务设置一个最大爬取深度。最大爬取深度是和任务有关的，因此我们要在 Request 中加上MaxDepth 这个字段，它可以标识到爬取的最大深度。Depth 则表示任务的当前深度，最初始的深度为0。

```plain
type Request struct {
	Url       string
	Cookie    string
	WaitTime  time.Duration
    Depth     int
	MaxDepth  int
	ParseFunc func([]byte, *Request) ParseResult
}
```

那在异步爬取的情况下，我们怎么知道当前网站的深度呢？最好的时机是在采集引擎采集并解析爬虫数据，并将下一层的请求放到队列中的时候。以我们之前写好的ParseURL函数为例，在添加下一层的URL时，我们将Depth加1，这样就标识了下一层的深度。

```plain
func ParseURL(contents []byte, req *collect.Request) collect.ParseResult {
	re := regexp.MustCompile(urlListRe)

	matches := re.FindAllSubmatch(contents, -1)
	result := collect.ParseResult{}

	for _, m := range matches {
		u := string(m[1])
		result.Requesrts = append(
			result.Requesrts, &collect.Request{
				Url:      u,
				WaitTime: req.WaitTime,
				Cookie:   req.Cookie,
				Depth:    req.Depth + 1,
				MaxDepth: req.MaxDepth,
				ParseFunc: func(c []byte, request *collect.Request) collect.ParseResult {
					return GetContent(c, u)
				},
			})
	}
	return result
}
```
<div><strong>精选留言（4）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/10/7f/d3/b5896293.jpg" width="30px"><span>Realm</span> 👍（4） 💬（0）<div>https:&#47;&#47;shimo.im&#47;docs&#47;5rk9dVyblnFzZLqx
根据课程讲解以及原代码，自己理解整理的调度过程。
</div>2022-12-18</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/PiajxSqBRaEIqvYMQ1yscgB6xS4nDkoOuP6KiaCiaichQA1OiaQ9rFmNtT9icgrZxeH1WRn5HfiaibDguj8e0lBpo65ricA/132" width="30px"><span>Geek_crazydaddy</span> 👍（2） 💬（1）<div>把worker获取任务的channel换成channel切片，索引值就是优先级，然后用多个select按序监听这些channel，而且要加default，没读到就立即跳过？</div>2022-12-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/49/4e/8798cd01.jpg" width="30px"><span>顷</span> 👍（0） 💬（0）<div>if !req.Task.Reload { ... }  &#47;&#47; 这里我们为任务 Task 引入了一个新的字段 Reload，标识当前任务的网页是否可以重复爬取。如果不可以重复爬取，我们需要在失败重试前删除 Visited 中的历史记录。

这里逻辑是不是反了？如果能重复爬，才需要再重新调度之前删掉记录吧。</div>2023-01-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/18/4f/9e4d5591.jpg" width="30px"><span>翡翠虎</span> 👍（0） 💬（1）<div>用哈希表结构有什么好处？这样的话是不是就显得单机了？如果用类似redis这样的存储，加上布谷鸟算法，能够做到既省空间又支持多机协同，会不会更好？</div>2022-12-20</li><br/>
</ul>