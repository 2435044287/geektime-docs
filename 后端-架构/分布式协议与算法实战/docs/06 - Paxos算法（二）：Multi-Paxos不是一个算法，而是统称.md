你好，我是韩健。

经过上节课的学习，你应该知道，Basic Paxos只能就单个值（Value）达成共识，一旦遇到为一系列的值实现共识的时候，它就不管用了。虽然兰伯特提到可以通过多次执行Basic Paxos实例（比如每接收到一个值时，就执行一次Basic Paxos算法）实现一系列值的共识。但是，很多同学读完论文后，应该还是两眼摸黑，虽然每个英文单词都能读懂，但还是不理解兰伯特提到的Multi-Paxos，为什么Multi-Paxos这么难理解呢？

在我看来，兰伯特并没有把Multi-Paxos讲清楚，只是介绍了大概的思想，缺少算法过程的细节和编程所必须的细节（比如缺少选举领导者的细节）。这也就导致每个人实现的Multi-Paxos都不一样。不过从本质上看，大家都是在兰伯特提到的Multi-Paxos思想上补充细节，设计自己的Multi-Paxos算法，然后实现它（比如Chubby的Multi-Paxos实现、Raft算法等）。

所以在这里，我补充一下：**兰伯特提到的Multi-Paxos是一种思想，不是算法。而Multi-Paxos算法是一个统称，它是指基于Multi-Paxos思想，通过多个Basic Paxos实例实现一系列值的共识的算法（比如Chubby的Multi-Paxos实现、Raft算法等）。** 这一点尤其需要你注意。
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/11/11/18/8cee35f9.jpg" width="30px"><span>HuaMax</span> 👍（40） 💬（8）<div>“当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段”这个优化机制。
请问，什么样是稳定状态？为什么稳定状态可以省掉准备阶段？</div>2020-02-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/54/9a/76c0af70.jpg" width="30px"><span>每天晒白牙</span> 👍（24） 💬（11）<div>只能在主节点进行读操作，效果相当于单机，对吞吐量和性能有所影响
写也是在主节点进行，性能也有问题</div>2020-02-24</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/iajMePfd6rh5RcAUb5hr0RtkjkhQtkssicSgDoRBWZ0h74FiajVVNl1gVwQQKT6GPtjibMW22zKusibX6SK7CT1cH3A/132" width="30px"><span>Geek_MYMSwen</span> 👍（19） 💬（6）<div>Chubby的局限可能在于高读的系统中，如果读请求过大，会导致系统的不可用。另外在系统中如何能够将主节点更替的信息向用户传播也是需要考虑的问题。
还有有一种情况我没有想清楚，请各位指点：
一个分布式系统中有5个节点，3个在一个机房A(机器编号A1，A2，A3)，2个在另一个机房B（机器编号B1，B2）。  1）如果节点A1的机架网络发生故障，导致A1与其他节点通信受阻，那么A1节点将会执行什么操作呢？通讯恢复以后A1节点如何进行数据同步呢？同样在A1无法通讯后出现集群有偶数节点，选举时会出现怎样的情况？ 2）如果主节点为B1，A机房与B机房间通讯产生故障，A机房和B机房的节点将分别执行怎样的操作呢？</div>2020-02-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/cc/87/2a196164.jpg" width="30px"><span>小石头</span> 👍（9） 💬（5）<div>如果多个提议者同时提交提案，可能出现因为提案冲突，在准备阶段没有提议者接收到大多数准备响应，协商失败，需要重新协商。你想象一下，一个 5 节点的集群，如果 3 个节点作为提议者同时提案，就可能发生因为没有提议者接收大多数响应（比如 1 个提议者接收到 1 个准备响应，另外 2 个提议者分别接收到 2 个准备响应）而准备失败，需要重新协商。
这第一个问题理解不了，按上篇讲的，最大提议号的提议者不是会收到所有的准备响应么？</div>2020-03-22</li><br/><li><img src="" width="30px"><span>zjm_tmac</span> 👍（9） 💬（1）<div>leader怎么确定acceptor的总数呢？集群是允许扩容的吗</div>2020-02-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/3f/b7/0d8b5431.jpg" width="30px"><span>snakorse</span> 👍（7） 💬（5）<div>找到一篇微信后台团队对PhxPasox的讲解，感觉是对multi-paxos非常好的补充，推荐一下https:&#47;&#47;mp.weixin.qq.com&#47;s?__biz=MzI4NDMyNTU2Mw==&amp;mid=2247483695&amp;idx=1&amp;sn=91ea422913fc62579e020e941d1d059e&amp;chksm=ebfc62fbdc8bebed551c2a041bb37bcaab836c4b2ca8e575d418f1e24459459c1c16faf70d06</div>2020-03-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/67/0e/c77ad9b1.jpg" width="30px"><span>eason2017</span> 👍（6） 💬（2）<div>集群，分布式的意义是提供更大的吞吐量，和并发，这样的操作无疑是掩耳盗铃</div>2020-04-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/99/27/47aa9dea.jpg" width="30px"><span>阿卡牛</span> 👍（5） 💬（2）<div>主节点只一个？那存在单点故障的问题</div>2020-03-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/77/2a/4f72941e.jpg" width="30px"><span>cpzhao</span> 👍（5） 💬（2）<div>咨询下，对于写请求，主节会发接受请求发给其余节点，只有其余节点过半操作成功才能返回成功给客户端。这里有个问题是，如果操作没有过半成功，比如6台机器只有两个写成功了，那主节点返回失败给客户端后，是不是还得考虑怎么让那两个写成功的回滚操作？大概可以怎么实现</div>2020-03-10</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/5b/70/6411282d.jpg" width="30px"><span>陈</span> 👍（5） 💬（2）<div>
只能在主节点读取，存在单机性能和容错问题。</div>2020-02-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/f8/ba/d28174a9.jpg" width="30px"><span>Geek_zbvt62</span> 👍（5） 💬（1）<div>本来想问个问题，看到思考题提到了。
从raft和zab的实现来看，一致性读操作的处理和写操作是类似的，不从本地读，而是也要发请求到所有节点，得到大多数节点的响应才行。我了解到的有的实现是领导者发送一个空操作给所有节点。
这样做的原因不光是考虑吞吐量的问题，而是读本地是满足不了强一致性的，因为自以为的leader未必是真的leader，此时可能另外的节点已经自己组成一个小团队，选出一个新leader，这个变量也可能都更新好几次了。只有和大多数节点交互一次才能知道自己当前还是不是leader。

有个问题，兰伯特提到的 “当领导者处于稳定状态时...”这个稳定状态是什么意思呢？在领导者是谁这个问题上，达成大多数节点的一致?


</div>2020-02-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/98/a6/41394d63.jpg" width="30px"><span>W.G.Ma</span> 👍（4） 💬（5）<div>如果多个提议者同时提交提案，可能出现因为提案冲突，在准备阶段没有提议者接收到大多数准备响应，协商失败，需要重新协商——————为什么？三个提议者最大编号的的总能收到准备提议的回复</div>2020-03-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/95/d1/7d3834ef.jpg" width="30px"><span>笑若海</span> 👍（4） 💬（2）<div>Chubby是如何解决单机leader的性能的：Chubby的角色定位是提供分布式锁服务，帮助其他应用达成分布式共识，内部存储的都是其他服务如GFS、MapReduce运行所需的元数据信息，数据结构采用类似linux文件系统的结构，每一级目录代表一定级别的锁。按照论文里面测试结果，97%（也可能是99%，记不太清了）以上的服务请求数据大小不超过1k，同时锁是有租约的，租约到期前不需要再发请求可以一直使用，快到期时发请求续租。论文中特别强调不要在Chubby上存储太大数据，有应用因存储1M大小的元数据而性能极差。
建议大家有空看看Chubby的论文。
另外论文提到了状态机，对状态机的有啥用，为了解决什么问题一直没想明白。</div>2020-02-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/46/6e/b748c535.jpg" width="30px"><span>sai</span> 👍（4） 💬（1）<div>您好，假设有3台节点 A, B, C. leader 最开始是A, 依次执行写入操作[set x=1, set y=2, set z=3], 假设B和C都有可能超时，根据paxos只需要大多数写入成功就算执行成功的原则，当前状态可能为A:[x:1, y:2, z:3], B:[x:1, z:3], C: [y2, z3]。如果这个时候主节点A宕机，如何重新选择主节点并恢复数据呢？</div>2020-02-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/9a/db/1472ff3c.jpg" width="30px"><span>Sam</span> 👍（3） 💬（1）<div>老师，能介绍一些关于这方面的书单吗？</div>2020-03-05</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/20/08/bc06bc69.jpg" width="30px"><span>Dovelol</span> 👍（3） 💬（4）<div>老师好，想问下，如果只有领导者可以发起提案，那么这是不是就退化为串行操作了，这样的话性能怎么保证呢，实际应用中是怎么解决的？我觉得最后问题， Chubby只能在主节点上执行读操作，在读请求量非常大的情况下，也是会遇到瓶颈的，还有就是单点问题，主节点挂了，在选出主节点之前就不能提供服务了对吧，该如果解决这类问题呢？</div>2020-02-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/8b/83/d2afc837.jpg" width="30px"><span>路人</span> 👍（2） 💬（1）<div>只允许主节点读，是为了实现强一致吧</div>2020-08-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/14/c2/46ebe3a0.jpg" width="30px"><span>侧耳倾听</span> 👍（2） 💬（2）<div>原文：一个 5 节点的集群，如果 3 个节点作为提议者同时提案，就可能发生因为没有提议者接收大多数响应（比如 1 个提议者接收到 1 个准备响应，另外 2 个提议者分别接收到 2 个准备响应）而准备失败，需要重新协商。
问题：对于接受准备响应的数量不太理解。
前一章老师说过，提议者也是接受者。那么当三个节点中的一个提议者提案时，其他两个提议者是当作当前提议者的接受者来看待吗？这样四个接受者的前提下，对于同一个KV的提案，我考虑了几种情况都能得到最大提案编号达成共识的结果。不知道我理解的有无问题，麻烦老师指点。如果上述方案正确的前提下，不需要引入领导者增加了系统可用性，但是内部请求数依然比较大</div>2020-04-14</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/30/c1/2dde6700.jpg" width="30px"><span>密码123456</span> 👍（2） 💬（1）<div>还以为会比上一节理解起来难。原来就处理2问题。一个是没有提议者没有大多数响应，还有一个是通信过多。我这里理解有一个问题。如果对领导节点选择，使用了base paxos算法。假设有5个机器。3个提议者，没出现大多数怎么办？</div>2020-03-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/8a/02/828938c9.jpg" width="30px"><span>Frank</span> 👍（2） 💬（1）<div>Chubby只能在主节点上执行读操作，牺牲了客户端读操作的并发性能。是不是偏向于CP模型？强调各节点之间的数据一致性。如果各个节点都开放读操作，那偏向于AP模型。考虑到Chubby是作为一个分布式锁的服务，应该选CP。不知道这样理解漏洞多不多。</div>2020-03-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/dd/51/a7e82963.jpg" width="30px"><span>波波</span> 👍（2） 💬（2）<div>老师我有个疑问，如果主节点在发送接收请求给其他节点时，刚好第一个节点接受并返回了，主节点想发送给其他节点时挂掉了，这时会重新选举leader，但是之后怎么保证其他节点数据的一致性啊？</div>2020-03-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/47/31/f35367c8.jpg" width="30px"><span>小晏子</span> 👍（2） 💬（1）<div>Chubby 只能在主节点上执行读写操作, 那么这个主节点就是热点，所有的请求都要经过它，显而易见它就是系统的瓶颈，影响系统的并发度，而且在并发时，写请求会block读请求，影响整个系统的QPS。</div>2020-02-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/fb/7b/2d4b38fb.jpg" width="30px"><span>Jialin</span> 👍（2） 💬（1）<div>Chubby 只能在主节点上执行读操作，这个设计导致高并发情况下读操作的吞吐量受到限制，影响系统的可用性，但是保证了读数据一致性问题。</div>2020-02-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/54/9a/76c0af70.jpg" width="30px"><span>每天晒白牙</span> 👍（2） 💬（1）<div>如果直接通过多次执行 Basic Paxos 实例来达到共识有两个问题
1.如果多个提议者同时提交提案，可能出现因为提案冲突，在准备阶段没有提议者收到大多数准备响应，协商失败，这样就需要重新协商
2.因为准备阶段和接受阶段会进行两轮RPC通讯，往返消息多，耗性能，延迟大，这是需要优化的

那如何解决这两个问题呢？
1.引入领导者
让领导者作为唯一提议者
这里涉及到如何选举领导者，不同的算法可以有不同的实现方法
Chubby 是通过执行 Basic Paxos算法投票选举
Raft通过一个随机倒计时功能，最快得到大多数投票的为领导者

2.优化 Basic Paxos执行
采用当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段</div>2020-02-24</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/a5/ff/6201122c.jpg" width="30px"><span>Geek_89bbab</span> 👍（1） 💬（1）<div>在leader A没有发生变化的情况下，从leader A这里 get x没有啥问题，但是如果发生网络分区，选取出来的新的leader A&#39; 和旧的leader A不在同一分区。那么这个时候，如果 有客户端client1向新的leader A&#39;发出 set x=10, 然后，客户端client2向旧的leader A发出get x,那么旧的leader A还没有意识到自己已经不在是leader了，返回x=2,这样对吗？</div>2020-10-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/fc/25/20ccf994.jpg" width="30px"><span>而立</span> 👍（1） 💬（1）<div>感觉Chubby的实现方案里，非主节点存在的意义就仅仅是等待主节点挂掉之后，成为主节点。
对于系统容量，读写性能都没有什么帮助。</div>2020-08-06</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/49/10/af49fa20.jpg" width="30px"><span>左耳朵狮子</span> 👍（1） 💬（1）<div>“当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段” 这里的稳定状态是如何定义的？根据什么参数</div>2020-05-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/25/00/3afbab43.jpg" width="30px"><span>88591</span> 👍（1） 💬（1）<div>集群中，既然所有节点数据都一致。读可以扩展到其他节点来读</div>2020-03-31</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/19/cc/87/2a196164.jpg" width="30px"><span>小石头</span> 👍（1） 💬（3）<div>如果多个提议者同时提交提案，可能出现因为提案冲突，在准备阶段没有提议者接收到大多数准备响应，协商失败，需要重新协商。你想象一下，一个 5 节点的集群，如果 3 个节点作为提议者同时提案，就可能发生因为没有提议者接收大多数响应（比如 1 个提议者接收到 1 个准备响应，另外 2 个提议者分别接收到 2 个准备响应）而准备失败，需要重新协商。
5个节点的集群就只能发五个准备响应？</div>2020-03-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/19/49/83e9fa4b.jpg" width="30px"><span>Theodore</span> 👍（1） 💬（1）<div>【Chubby 只能在主节点上执行读操作】这就相当于cap中去了cp，性能约等于单机而且当leader挂掉的一瞬间整个集群是不可用的（缺失的那一部分a）。在实际应用当中往往是读写比8:2，这样做一定程度上讲是很不划算的，优化的话可以参考mongoDB分片算法实现：只有Leader能接收&amp;执行写请求，Leader负责接收读请求并把对应的读请求执行分配到其他节点上</div>2020-03-13</li><br/>
</ul>