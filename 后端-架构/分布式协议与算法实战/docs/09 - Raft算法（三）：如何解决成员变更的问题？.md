你好，我是韩健。

在日常工作中，你可能会遇到服务器故障的情况，这时你就需要替换集群中的服务器。如果遇到需要改变数据副本数的情况，则需要增加或移除集群中的服务器。总的来说，在日常工作中，集群中的服务器数量是会发生变化的。

讲到这儿，也许你会问：“老韩，Raft是共识算法，对集群成员进行变更时（比如增加2台服务器），会不会因为集群分裂，出现2个领导者呢？”

在我看来，的确会出现这个问题，因为Raft的领导者选举，建立在“大多数”的基础之上，那么当成员变更时，集群成员发生了变化，就可能同时存在新旧配置的2个“大多数”，出现2个领导者，破坏了Raft集群的领导者唯一性，影响了集群的运行。

而关于成员变更，不仅是Raft算法中比较难理解的一部分，非常重要，也是Raft算法中唯一被优化和改进的部分。比如，最初实现成员变更的是联合共识（Joint Consensus），但这个方法实现起来难，后来Raft的作者就提出了一种改进后的方法，单节点变更（single-server changes）。

为了帮你掌握这块内容，今天我除了带你了解成员变更问题的本质之外，还会讲一下如何通过单节点变更的方法，解决成员变更的问题。学完本讲内容之后，你不仅能理解成员变更的问题和单节点变更的原理，也能更好地理解Raft源码实现，掌握解决成员变更问题的方法。
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/54/9a/76c0af70.jpg" width="30px"><span>每天晒白牙</span> 👍（62） 💬（5）<div>可以参考Kafka的分区和ES的主分片副本分片这种机制，虽然写入只能通过leader写，但每个leader可以负责不同的片区，来提高写入的性能</div>2020-03-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/01/30/cb3a729e.jpg" width="30px"><span>XHH</span> 👍（35） 💬（2）<div>1、leader可以合并请求
2、leader提交日志和日志复制RPC两个步骤可以并行和批量处理 
3、leader可以并发和异步对所有follower 并发日志复制，同时可以利用pipeline的方式提高效率
4、每个节点启动多个raft实例，对请求进行hash或者range后，让每个raft实例负责部分请求
</div>2020-03-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/83/39/f9623363.jpg" width="30px"><span>竹马彦四郎的好朋友影法師</span> 👍（27） 💬（2）<div>韩老师您好~ (A,B,C) 三个旧配置A是leader，然后加入D，然后网络分区为(A,B)和(C,D), 那么A依旧赢得大多数选票而是leader.  C因为分区，所以也开始发起选举，赢得了C、D的选票，注意哦~ ，C也是旧配置哦~ 那么C不也就成为leader了么?  所以不就出现了2个leader了么?  也就是D作为新配置固然无法成为leader，但是C作为旧配置还是可以成为leader的呀~ 希望您能指点一下. 但是我说一下我的看法，我的看法是——您一开始在&quot;成员变更的问题&quot;中举的例子貌似有点问题——应该是C、D、E中的D或者E会成为新配置中的leader而不是C节点会成为新配置中的leader，因为C的（旧）配置中原本就没有D、E，它即便获取到D、E的选票也不能认为自己得票过半，这样就能解释的通了。</div>2020-05-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/4d/54/9c214885.jpg" width="30px"><span>kylexy_0817</span> 👍（18） 💬（1）<div>韩老师，有个问题，其实在实际生产环境中，是不是应该尽量避免网络分区才是重点，例如把某个集群的机器，尽量放在同一个内网中。举个我想到的例子，ABC三个节点，A在网络1，BC在网络2，初始化时，A成为了领导者，后来在网络2的单节点D加入集群，此时正好出现网络分区，BCD重新重新选举，得到B是领导者，后面网络通讯恢复了，这样即使采用单节点变更的方式，不也同样会出现了脑裂了吗？不知道我的理解正不正确，求解答。</div>2020-04-19</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/75/dd/9ead6e69.jpg" width="30px"><span>黄海峰</span> 👍（14） 💬（2）<div>老师，这种共识算法只是用于p2p的分布式系统吧，像hadoop&#47;spark这些大数据分布式系统都是主从模式，部署就决定了谁是master，根本就不用这些共识算法了。。。相对比主从模式更可靠更可控啊，因为没有这些这么复杂的选举逻辑。。除了区块链，其他系统用p2p是不是有什么不可取代的必要性呢？</div>2020-03-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/f4/e2/dbc4a5f2.jpg" width="30px"><span>朱东旭</span> 👍（9） 💬（2）<div>一致性算法与共识算法的区别是啥，raft以领导者的日志为准，不就是保证了数据的最终一致吗。</div>2020-03-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/f8/ba/d28174a9.jpg" width="30px"><span>Geek_zbvt62</span> 👍（9） 💬（7）<div>一般而言，在实际工程中，Consul 的 consistent 就够用了，可以不用线性一致性，

这句话是不是笔误了？</div>2020-03-02</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKYfReHXMbPaxO890ib9GvY9iciclPIUvaAYMYON4scP7ElXCPVzicghF0SH5HN2LqibYOrdrppC7DuSpw/132" width="30px"><span>static</span> 👍（8） 💬（7）<div>老师好，想请教一个困扰我很久的关于Raft算法的一个问题。
在分布式锁场景下（使用Raft算法），A客户端向leader申请获取锁（set lock），此时leader应用lock信息日志，并RPC复制日志信息给follower节点，此时follower节点还没应用到状态机，leader收到大部分follower成功信息，自己应用了状态机并返回客户端说set lock成功，但此时leader宕机了，其中一个follower变为leader，此时客户端B来获取锁，发现leader没有lock信息（因为follower将lock信息应用到状态机靠leader心跳传递，但刚刚leader宕机了没来得及传递），客户端B此时获取锁也成功了，这不就破坏了锁的同步性吗？Raft算法是如何保证这种场景下的强一致性（线性一致性）？</div>2020-03-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/03/1c/c9fe6738.jpg" width="30px"><span>Kvicii.Y</span> 👍（7） 💬（2）<div>NO_OP这个空日志项该怎么理解呢，为了防止出现多个领导者？怎么防止的呢</div>2020-06-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/f9/31/b75cc6d5.jpg" width="30px"><span>ξ！</span> 👍（6） 💬（1）<div>老师，在配置单节点加入的时候，是怎么发现当前集群的呢，难道是在配置的时候就将集群的节点信息写入了么，即便这样的话，当前节点是怎么发现当前集群的领导者呢，在新节点加入的时候他是怎么知道当前集群的领导者的呢，这个发现领导者的过程是新节点主动发起的rpc还是领导者的心跳发现的呢</div>2020-11-17</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/4d/79/803537db.jpg" width="30px"><span>慢动作</span> 👍（5） 💬（3）<div>老师好，有个疑问，集群加入节点都是通过leader处理的，那文章开头3节点到5节点，为什么a还是旧配置，c却是新配置？</div>2020-06-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/fc/49/bd2ff785.jpg" width="30px"><span>刘学</span> 👍（5） 💬（2）<div>韩老师你好，我想到的可以解决因为强领导者导致的写性能瓶颈的办法是多分片，这样多个raft流程并发执行，不同的分片的master落在不同的机器上就可以很好的解决这个问题。在加入新节点后的第一步时，主节点向新加入的节点同步数据，那就意味着主节点需要支持向非本组成员的节点同步数据的功能对么？</div>2020-03-11</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erY7671v8F2joiaBbnCPsbJ6ExFruIMDsd9HRSORsOZgbzBrKibDibU9CRCGghI56hUiampklDWzMFc4A/132" width="30px"><span>华子</span> 👍（3） 💬（2）<div>1. 所谓的＂配置＂就是指集群中的节点要知道新加入节点的IP地址等信息吗？
2. 而之所以不能一次性加入两台或以上的节点，是因为无法保证＂同时＂加入？</div>2020-04-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/81/e6/6cafed37.jpg" width="30px"><span>旅途</span> 👍（3） 💬（1）<div>老师  麻烦解答下 如果旧配置的大多数和新的大多数数量相等 并且有重叠的 这时候为什么不会产生两个领导者呢 是因为只能选重叠的做领导者吗?如果新的大多数数量大于旧配置 ,领导者就会在新的大多数中产生吗?</div>2020-03-09</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLsexaVCEecIhpPsOqHJkZFEnyTfOZ1Ot3lmSMfyIVkR5SerJibatj6mIHackokoIO4ELiaObhQ60rw/132" width="30px"><span>Infinite_gao</span> 👍（3） 💬（2）<div>老师讲的深入浅出，可是有个疑问，为什么配置从老的阶段到中间阶段再到新阶段的这个过程没有进行阐述呢，配置的自动转换过程对于理解细节非常重要。C(old)-&gt;C(old,new)-&gt;C(new)。
还有就是新加入的节点开始是通过什么类型的消息与原leader通信的，通信的信息细节是什么，是选举请求吗？</div>2020-03-07</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/QFE00aXGzaS6ibbfJSJsDrpIkqs0OrIYjzZv6L9vZmMhOlut2j24iaeZb0MCQazToE6FRXN960nNiaTrsmw09YjGw/132" width="30px"><span>岁月如歌</span> 👍（2） 💬（3）<div>我想说的是，在正常情况下，不管旧的集群配置是怎么组成的，旧配置的“大多数”和新配置的“大多数”都会有一个节点是重叠的。 
-----------------------------------------------------------
对于老师上述说法 和 配图有点不理解 为何会发生这样的情况？ 以及非正常情况下出现两个主节点呢 需要怎么处理？</div>2020-03-09</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/QFE00aXGzaS6ibbfJSJsDrpIkqs0OrIYjzZv6L9vZmMhOlut2j24iaeZb0MCQazToE6FRXN960nNiaTrsmw09YjGw/132" width="30px"><span>岁月如歌</span> 👍（2） 💬（1）<div>提升写性能需要单机限制，修改为多主节点形式，分片负载均衡。如redis集群方式 jedis-cluster 使用一致性hash分片负载 提升读写能力。 </div>2020-03-09</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/18/03/ef0efcc4.jpg" width="30px"><span>EidLeung</span> 👍（2） 💬（2）<div>老师，有个地方没懂：
“consistent：客户端访问领导者节点执行读操作，领导者在和大多数节点确认自己仍是领导者之后返回本地数据给客户端，否则返回错误给客户端。在这种情况下，客户端读到的都是最新数据。”和“一般而言，在实际工程中，Consul 的 consistent 就够用了，可以不用线性一致性，只要能保证写操作完成后，每次读都能读到最新值就可以了。”这两句话有点矛盾，没懂。
第一句说读数据时需要向节点确认自己是领导者，应该是客户端读的时候领导者再向其他节点确认自己还是领导者，确认完成后再返回客户端数据。
第二句说，写的时候确认是领导者（写完成），读的时候就是最新的值。
这样第二句话在“写后出现网络分区导致已经选取了新的领导者，新的领导者又写入了数据，而旧的领导者还没退位的情况，此时读的数据应该不是最新的”（这个应该是default模型啊：写的时候保证写入，读的时候直接读）。</div>2020-03-03</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/d6/a9/ffd70fdc.jpg" width="30px"><span>振超</span> 👍（2） 💬（1）<div>既然一个 raft 集群只能有一个 leader 节点，那么可以缩小集群的粒度，即将一个大的 raft 集群分成若干个 group，这样每个 group 范围内都有一个能够支持写操作的 leader 节点，整体上就有多个节点能够响应写操作，从而提升写性能，上层可以采取路由策略对数据进行分片。</div>2020-03-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/01/d7/f835081c.jpg" width="30px"><span>bin的技术小屋</span> 👍（1） 💬（1）<div>老师，这几讲都是再讲raft协议写相关的内容，能否讲下怎么处理读请求的？现在理解了写的过程，但是对于如何保证读的一致性还是不明白。因为客户端写请求，leader节点等到大部分follower复制日志rpc响应成功后，就响应客户端了，但是这时并不是所有的follower节点复制了最新的日志项，也不是所有的follower和leader日志项保持一致。如果在这一瞬间，客户端发起读请求，正好访问到日志落后的follower 节点上，raft是怎么处理的？如何保证强一致性</div>2020-08-12</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/18/3e/f8632713.jpg" width="30px"><span>EveryDayIsNew</span> 👍（1） 💬（1）<div>那个A,B,C , 新增两个D，E的分区没看明白，这种两个节点是怎么一次性加进来的，加入新节点应该是给leader进行报告通信吧，然后leader再把新节点信息同步给集群的其他节点吧，按图貌似主节点A是不知道有新节点加入的；还是新节点加入可以直接给任何一个节点发假如申请，不是强主模型都应该找leader吗？</div>2020-08-07</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/21/d0/aa5422f2.jpg" width="30px"><span>林万伟</span> 👍（1） 💬（1）<div>老师，关于单节点变更，我有个疑问：
 假如一开始是3个节点A、B、C，C为leader，这时候加入新节点D，C节点发送配置变更通知后，挂掉了，此时D是新配置[A,B,C,D]，但是B和C节点都是旧配置[A,B,C]，那么此时A节点发起选举投票，并且当选，那么之前C节点作为leader同步的新配置更新[A,B,C,D],是否还能在A和B上面生效呢？
如果发生这种情况，后续节点是如何添加进去的呢？</div>2020-08-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/25/00/3afbab43.jpg" width="30px"><span>88591</span> 👍（1） 💬（1）<div>不知道可不可行
1、批量写，合并请求
2、优化二阶段提交，不等到大多数成功。用心跳同步数据</div>2020-04-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/1d/e0/5940cabd.jpg" width="30px"><span>分目</span> 👍（1） 💬（1）<div>老师，你好，有个疑问，raft算法的领导者是超过半数的选举同意就可以成为领导者，那剩余节点到达随机超时时间后，也应该是按照规则成为跟随者（故障节点，故障恢复之后，也应该成为跟随者）；那么在节点增加时候，同时增加多个节点有可能导致出现两个领导者（文中提到可能存在旧的节点配置跟新增加节点配置一致的情况，导致新配置也选举出一个领导者），其中旧的那个节点配置跟它之前的领导者配置不一致的原因是什么?是刚好之前节点故障，在增加节点的时候恢复了么?希望老师解答，谢谢！</div>2020-03-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/f6/e3/e4bcd69e.jpg" width="30px"><span>沉淀的梦想</span> 👍（1） 💬（2）<div>&quot;在分区错误、节点故障等情况下，如果我们并发执行单节点变更，那么就可能出现一次单节点变更尚未完成，新的单节点变更又在执行，导致集群出现 2 个领导者的情况。&quot; 
这个是为什么呢？单节点变更第一步不就是请求领导者复制数据嘛，当有两个单节点变成同时发起的时候，领导者应该可以发觉并且制止吧</div>2020-03-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/47/31/f35367c8.jpg" width="30px"><span>小晏子</span> 👍（1） 💬（1）<div>课后思考的问题是个好问题，貌似没有通用可行的办法，必须具体问题具体分析了，有个思路是使用数据分区以分散负载的办法，这个办法目前许多大型系统也在用，但是因为事务不能夸分区，这个办法限制性会比较大。
期望看到老师给出的方案。</div>2020-03-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/57/4f/6fb51ff1.jpg" width="30px"><span>奕</span> 👍（1） 💬（1）<div>Raft 算法的日志项是不是可以分为多种类型，比如：数据日志项，配置日志项，空日志项.......这样的话,日志项里面是不是有个类型字段可以进行区分的？</div>2020-03-02</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1f/05/63/dd59ad18.jpg" width="30px"><span>加油加油</span> 👍（0） 💬（1）<div>老师您好，请教一个问题，如果在集中式架构的分布式集群中如果出现了节点故障该怎么恢复，正常来说可能会触发新一轮的选主，但是这个节点恢复正常之后还有机会加入到集群当中吗？</div>2020-11-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/18/f9/31/b75cc6d5.jpg" width="30px"><span>ξ！</span> 👍（0） 💬（1）<div>raft是共识算法而不是一致性算法是不是因为在日志复制需要伴随下一次心跳发生的时候一起传输，在客户端发送值到应用上值之间数据不是准确的,,,不知道这么说对不对</div>2020-10-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/e7/51/76c53b63.jpg" width="30px"><span>Story</span> 👍（0） 💬（1）<div>我想请教下节点移除时的选举问题，假如开始时有[A、B、C、D]的配置，A为领导者，现在想移除节点D，首先领导者A向其他节点发起配置[A、B、C]变更请求，假设C成功接受到了新配置[A、B、C]，这时领导者A发生了故障，那么A、B的配置应该是[A、B、C、D]，这种情况好像新旧配置都无法选举领导者，是这么回事吗？</div>2020-06-28</li><br/>
</ul>