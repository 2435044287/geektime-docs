你好，我是陈皓，网名左耳朵耗子。

我前面写的《分布式系统架构的本质》系列文章，从分布式系统的业务层、中间件层、数据库层等各个层面介绍了高并发架构、异地多活架构、容器化架构、微服务架构、高可用架构、弹性化架构等，也就是所谓的“纲”。通过这个“纲”，你能够按图索骥，掌握分布式系统中每个部件的用途与总体架构思路。

为了让你更深入地了解分布式系统，在接下来的几期中，我想谈谈分布式系统中一些比较关键的设计模式，其中包括容错、性能、管理等几个方面。

- **容错设计又叫弹力设计**，其中着眼于分布式系统的各种“容忍”能力，包括容错能力（服务隔离、异步调用、请求幂等性）、可伸缩性（有/无状态的服务）、一致性（补偿事务、重试）、应对大流量的能力（熔断、降级）。可以看到，在确保系统正确性的前提下，系统的可用性是弹力设计保障的重点。
- **管理篇**会讲述一些管理分布式系统架构的一些设计模式，比如网关方面的，边车模式，还有一些刚刚开始流行的，如Service Mesh相关的设计模式。
- **性能设计篇**会讲述一些缓存、CQRS、索引表、优先级队列、业务分片等相关的架构模式。

我相信，你在掌握了这些设计模式之后，无论是对于部署一个分布式系统，开发一个分布式的业务模块，还是研发一个新的分布式系统中间件，都会有所裨益。
<div><strong>精选留言（30）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/0f/6b/3d/ae41c2b3.jpg" width="30px"><span>data</span> 👍（1） 💬（1）<div>老师可以提供代码案例来讲解吗这样感觉可以学的更多哈哈</div>2018-04-04</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/63/3c/b4cfbce9.jpg" width="30px"><span>华烬</span> 👍（32） 💬（0）<div>看到挖掘机的时候我笑了，印象中真的经历过光纤被挖断的故障</div>2018-02-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/fa/90/f937f371.jpg" width="30px"><span>qgymje</span> 👍（14） 💬（1）<div>在听这文章的时候，我想到了可能容错这个概念，是统一分布式系统所有知识的核心，几乎所有的设计方案都是围绕着容错进行的，无论是简单的Supervisor启动服务进程，K8S里Pod的重启机制，还是应用层面的限流，熔断，降级，都是为了保证系统的可用性，也就是所谓的弹性；而幂等，补偿，以及数据复制等设计方案，是为了保证系统的正确性，也从另一方案说明了系统的容错能力。</div>2020-06-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/a2/5e/3871ff79.jpg" width="30px"><span>迷途书童</span> 👍（12） 💬（2）<div>弹力这个词是左耳朵耗子翻译的一个词，大家应该慎用，要么直接用Resiliency，要么用容错。 因为在很多公司里，很少直接说，我们这个系统支持弹力设计。从感性的角度来说，容错这个词很难与弹力划上等号</div>2020-06-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/5e/61/985f3eb7.jpg" width="30px"><span>songyy</span> 👍（10） 💬（0）<div>我觉得自己缺少解决 大规模 高可用 分布式 问题的经验，一直希望在这方面进行深挖但无奈工作范围限制，没有相关的问题可以遇到。期待能在这个系列之中看到更多的例子 😁</div>2018-02-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/50/2b/2344cdaa.jpg" width="30px"><span>第一装甲集群司令克莱斯特</span> 👍（7） 💬（2）<div>看到挖掘机挖端光纤的时候，太有意思了。有一次我们的系统不稳定，网络抖动故障，持续几天就出现那个点，上下游系统怎么查也查不出来。最后发现了，那个点下班时间，人员过多走过那条机房外面的路，通过地板砖踩踏，影响李里面的光纤网线了。</div>2020-10-26</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/29/ec/19566a0a.jpg" width="30px"><span>一飞</span> 👍（5） 💬（2）<div>异步调用为啥是容错设计？ 应该是提高性能的一种策略。</div>2019-03-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/13/25/00/3afbab43.jpg" width="30px"><span>88591</span> 👍（4） 💬（0）<div>很多公司应该支撑不到挖掘机这个阶段就倒闭了。</div>2018-12-02</li><br/><li><img src="" width="30px"><span>蓝海</span> 👍（4） 💬（2）<div>耗子哥可否在后面出一篇有关gcc优化带来的相关问题（各种崩溃，优化选项对程序做了哪些假设，哪些&quot;非标准&quot;的代码会导致优化错误），如何判断崩溃是由于优化，二进制不兼容，链接错误导致，而非一般的代码错误。gcc的优化选项看了官网说明很多遍，但说明过于简洁（编译原理只停留在前端印象，优化技术生疏），想了解的感性一些。这些bug问题解决都很费力，想归纳出一条方法经验论，怎样的代码要求才能对各种优化级别不出错（gcc本身bug除外）。以上的问题以及问题本身是否成立，想请耗子哥指导</div>2018-02-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/8d/a6/22c37c91.jpg" width="30px"><span>楊_宵夜</span> 👍（3） 💬（0）<div>耗子叔每篇文章真是干货十足。</div>2018-02-22</li><br/><li><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTLndPicaib35sSOgGib8iafFBq8B0hoBO3Bfp3QViblYQ669lRFjPD1RSX2rDibmElID00l5oWokhuZBJnw/132" width="30px"><span>Geek_7b1383</span> 👍（2） 💬（0）<div>故障是正常的，不可预测突发的，需要弹力（Resiliency）：
在好的情况下，这个事对于我们的用户和内部运维来说是完全透明的，系统自动修复不需要人的干预。
如果修复不了，系统能够做自我保护，而不让事态变糟糕。
我们设定可用率99.99%
实现的难点尽一切努力减少故障恢复时间
按照分类，系统无计划的系统级故障等做好智能监控和自愈能力，自然灾害、人为破坏，以及供电问题等有异地容灾，第三方规避风险。有计划的，流程加全链路监控保证，同时事前检查备份，事中双人复核操作，事后各位措施验证和人工验证，智能监控探测等，异常情况回滚方案的设计原则。
</div>2020-04-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/a0/a3/8da99bb0.jpg" width="30px"><span>业余爱好者</span> 👍（2） 💬（0）<div>服务端是一个多用户系统，从这点就可以知道对后端可用性的高要求。服务端应提供7&#47;24的不间断服务。然而由于分布式系统由多台机器组成，出现故障就不是不可避免的。可以说分布式系统的架构就是为了解决这个矛盾。

服务调度是值一个服务有多个实例。不至于一台宕机整体服务不可用。
全栈监控是整个系统的眼睛，观察整个系统的状况，出现问题做报警。因而需要在业务逻辑中加入监控卖点，向监控系统报告自己的情况，还要做好审计日志以及异常日志，以便故障之后复现原因恢复系统运行。
流量调度是分担每个服务，进程等实例的访问压力。状态与数据调度是在多个副本之间对Ｃ,A做出权衡。</div>2020-03-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/0b/d2/e72e7657.jpg" width="30px"><span>高彬</span> 👍（1） 💬（0）<div>老师的课都是经验啊  赞</div>2020-03-22</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/73/16/595b0342.jpg" width="30px"><span>slark</span> 👍（1） 💬（0）<div>弹力设计，保持系统容错性。错误不可避免，如何在错误发生后恢复或者记录关键信息便于恢复才是我们应该达到的目标。对于故障，同时也要区分可能导致故障的场景，针对性地处理</div>2020-01-16</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/63/77/423345ab.jpg" width="30px"><span>Sdylan</span> 👍（1） 💬（0）<div>对于分布式系统，故障是不避免的。根据公式：我们程序鲁棒性越高越好，故障恢复时长越短越好。如何缩短故障恢复时长，自动化+流程化。</div>2019-12-23</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/f8/a8/e8b8fc68.jpg" width="30px"><span>疾风紫狼</span> 👍（1） 💬（2）<div>能进能退乃真正法器可还行。</div>2019-08-08</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/17/59/37/bd2de0a4.jpg" width="30px"><span>edisonhuang</span> 👍（1） 💬（0）<div>分布式系统出故障是不可避免的，弹力设计的关键是要提高系统的可用性，提高MTTF，提高MTTF，一是拉长系统稳定运行时间，一是减少故障恢复时间。
由于分布式系统故障呢普遍性，因此在分布式系统设计的和开发的过程中，就要把故障当作不可或缺的一环来处理，尽可能让故障恢复过程自动化，从而真正提高系统可用</div>2019-07-01</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/08/f3/4b10028e.jpg" width="30px"><span>卢俊杰 _JAY</span> 👍（1） 💬（0）<div>以前或多或少写过一些数据库, MQ自动重连的代码，不过还没有一个整体的认识，多谢作者把这个事情系统化，条理清晰多了</div>2018-02-20</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/24/df/645f8087.jpg" width="30px"><span>Yayu</span> 👍（0） 💬（0）<div>这个读文章的人的声音听起来真难听</div>2023-09-21</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/10/4c/a0/6cfdefa6.jpg" width="30px"><span>特修斯之船</span> 👍（0） 💬（0）<div>为什么是“平均故障前时间”，而不是“平均故障时间”？</div>2023-07-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/0f/92/6d/becd841a.jpg" width="30px"><span>escray</span> 👍（0） 💬（0）<div>Resiliency 我觉的这个单词表达的意思和”弹力设计“或者”容错设计“略微有一些差别。

resilience n.
1. the ability to become strong, happy, or successful again after a difficult situation or event
2. the ability of a substance such as rubber to return to its original shape after it has been pressed or bent.

可测试和可测量不仅在软件设计中非常重要，其实在其他的地方也一样，不可测则无改进。

从网络、性能、安全、运维、管理和硬件的分类标准，无计划的和有计划的宕机不可避免。

分布式系统是这样，其实人类社会也一样，会宕机，但是要争取无故障时间长，再加上故障恢复时间短。

故障是正常的，而且是常见的；
故障是不可预测突发的，而且相当难缠。

估计很多人都不承认这两条。

Design for Failure，在前几年有个时髦名词，叫做”逆商“。

在网上查了一下 AWS 的 Seven principles，找到一个 Simone Brunozzi 在 2011 AWS Tour Australia 做的演讲 Architecting for the cloud 
https:&#47;&#47;www.slideshare.net&#47;AmazonWebServices&#47;2011-aws-tour-australia-architecting-for-the-cloud-demo-and-best-practices-by-simone-brunozzi

1. Design for failure and nothing will fail
2. Decouple
3. Elasticity
4. Dynamic and Static
5. Think Parallel
6. Don’t fear constraints
7. Security

另一个版本，我觉的更好，也是来自 Amazon 的 Architecting for the Cloud: Best Practices

1. Design for failure and nothing will fail
2. Loose coupling sets you free, the looser they’re coupled, the bigger they scale
3. Implement “Elasticity”, Elasticity is fundamental property of the Cloud
4. Build Security in every layer, Design with Security in mind
5. Don’t feer constraints, Re-think architectural constraints
6. Think Parallel, Serial and Sequential is now history
7. Leverage different storage options, One size DOES NOT fit all

赵云的那个梗有点冷。</div>2023-03-13</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/37/3f/a9127a73.jpg" width="30px"><span>KK</span> 👍（0） 💬（0）<div>MTTF = MTTF&#47;(MTTF+MTTR)</div>2021-08-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/14/37/3f/a9127a73.jpg" width="30px"><span>KK</span> 👍（0） 💬（0）<div>弹力设计！</div>2021-08-18</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/21/51/a6020b74.jpg" width="30px"><span>刘旭</span> 👍（0） 💬（0）<div>韧性设计</div>2021-03-27</li><br/><li><img src="" width="30px"><span>Geek_88604f</span> 👍（0） 💬（0）<div>可用度的计算比较容易理解，关键问题是MTTR和MTTF如何去计算呢？业界有没有相关的实践？</div>2020-05-05</li><br/><li><img src="" width="30px"><span>Geek_130e9e</span> 👍（0） 💬（0）<div>大规模、高可用、分布式，考验实力了。</div>2020-04-27</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/1c/1e/9c/39f5ff2a.jpg" width="30px"><span>常清静</span> 👍（0） 💬（0）<div>挖掘机挖断，非核心业务拖垮核心业务，大量数据量的请求，导致整体核心服务不可用，这些场景，都遇到过，而从这样一次次的事故中，满满的提升自己的服务健壮与容错，并总结出经验，这个的确是不经历，不知道的什么地方会给你埋雷</div>2020-03-29</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/16/7e/24/afb15de3.jpg" width="30px"><span>头发茂密</span> 👍（0） 💬（0）<div>这部分和性能测试有点像，尤其是性能调优部分，会涉及软硬件，网络，数据库等等设施，相当复杂。</div>2020-03-25</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/12/0b/d2/e72e7657.jpg" width="30px"><span>高彬</span> 👍（0） 💬（0）<div>分布式系统可以做类似主从集群的架构吗   避免单点故障</div>2020-03-22</li><br/><li><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eq8qpSvBoZz89u3BhGXWLibs2OibCkZl8bx74aLSJ58f467bR8anNaTiccJklcqjBdhfvvJpvLVmYesA/132" width="30px"><span>Haan</span> 👍（0） 💬（0）<div>get</div>2019-12-29</li><br/>
</ul>