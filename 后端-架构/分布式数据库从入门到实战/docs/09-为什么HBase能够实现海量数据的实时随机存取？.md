你好，我是彭旭。

上节课我们讲到HBase通过自动管理数据分区以及弹性伸缩应对数据的增长。这两个特性解决了我们云服务需求中的运维成本问题。

这节课我们从性能入手，看看HBase为什么能够在高并发场景下也做到秒到毫秒级的实时随机存取，替换掉MySQL。顺便看看HBase是如何实现高可用，解决的云服务高可用与故障恢复的问题。

先来看一下HBase在读写上是怎样优化性能的。

## HBase写入性能优化

[第2讲](http://time.geekbang.org/column/article/783146)的时候我们介绍过LSM数据结构，HBase就是使用LSM的一个典型代表，LSM对写入的性能优化，这里我们直接复习一下。

总体来说，HBase数据写入的时候，无论是新增数据还是更新删除数据，都会先写入MemStore这个内存组件。

MemStore在内存中会使用一个有序的数据结构，比如NavigableSet，所以数据写入就已经排好序。当MemStore的大小达到配置的阈值或者经过一定时间后，MemStore就会将数据刷新输出到磁盘。这里会是一个磁盘的批量顺序写入，所以速度很快。又因为数据已经排好序，所以后续数据的读取也能够基于索引去做二分查找等快速读取。

在我们云服务中，其实大部分时候都是新增写入场景，或者修改了单条数据，比如修改联系人。新增的数据可以顺序写入磁盘。单条或者多条的联系人数据更新，**原本需要的磁盘随机IO，也被转化成为了内存的读写与磁盘的顺序写入。**而内存的写入基本上是纳秒到微秒级别，这样，数据的写入性能就有了一个巨大的提升。
<div><strong>精选留言（2）</strong></div><ul>
<li><img src="https://static001.geekbang.org/account/avatar/00/31/1e/0a/159b2129.jpg" width="30px"><span>lufofire</span> 👍（1） 💬（2）<div>思考题：
1. 读写性能影响：一个client请求，如果查询找到真正负责读取RegionServer，本身就是代理转发过程，有一定网络延迟。
2. 增加master和ZK管理压力：RegionServer状态监控，元数据管理，更多RegionServer意味着更多的元数据读写。

另外一个关于HBase的高并发， 文章简单说了扩缩容， 没有详细说明在扩容后，HBase如何做了哪些事情来保证高可用，比如服务发现和节点故障后如何处理。另外，只是通过依赖CK来着保活和元数据管理, 本身是不够的，毕竟从ZK组件也有缺陷，比如ZooKeeper 集群的性能受到其最慢节点的影响，在大规模的读写操作下，性能可能会成为瓶颈。再比如相比etcd, zk本身没有很好的watcher机制，如何进行服务状态监控呢？  </div>2024-06-28</li><br/><li><img src="https://static001.geekbang.org/account/avatar/00/11/30/c1/2dde6700.jpg" width="30px"><span>密码123456</span> 👍（0） 💬（1）<div>受到的限制应该就是带宽了。文件写入hdfs，本身也是一个服务，不会像硬盘那么快。从hdfs读取也是，也会占用一定的资源。


有个问题，正常服务都是预写日志，memstore，没有提交。预写的日志，没有上传到hdfs，服务故障了，其他服务接替这个分区后，数据是不是就丢失了？</div>2024-06-28</li><br/>
</ul>